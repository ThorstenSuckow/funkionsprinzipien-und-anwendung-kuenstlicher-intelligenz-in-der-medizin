\chapter{Zusammenfassung und Bemerkung}



Ein biologisches neuronales Netz ist ein Kommunikationsverbund, in dem hochkomplexe biochemische Prozesse ineinandergreifen.
Diese Prozesse ermöglichen eine Informationsverarbeitung: Eine einzelne Nervenzelle ist in der Lage, Signale zu empfangen, zu verrechnen und ein Signal auszugeben.
Löst ein Aktionspotenzial aus, diffundieren Neurotransmitter in den synaptischen Spalt, die Folgezelle integriert die daraus entstehenden elektrischen Signale und so setzt sich die Kette fort, oft bis tausende von Neuronen ein Signal an das Ende der Kommunikationskette weitergeleitet haben.

Die Modellierung künstlicher neuronaler Netze folgt in Teilen dem biologischen Vorbild, und es mag zunächst überraschen, dass einige solcher Verschaltungsmuster - wie die \textbf{Divergenz} oder die \textbf{rekurrente} oder \textbf{laterale Hemmung} - zunächst an Logikgatter erinnern (vgl.~\cite[58 f.]{Eil19}). Es liegt nahe, dass die Wissenschaft versucht ist, solche naturgegebenen verrechnenden Einheiten und logischen Verschaltungen zu verstehen und in mathematische Formeln zu gießen, um ein Modell zu erstellen, das menschliche Intelligenz erklärt und nachstellt, und dann als Abstraktionen auf Maschinen übertragen wird: ``if you really understand something, you can usually make a machine do it``~\cite[xiii]{AR88}.\\

Ein früher Versuch davon wurde in Abschnitt~\ref{sec:mcpneuron} mit dem MCP-Modell vorgestellt, ein \textbf{empirisches Modell}, das auf Analyse und einfacher Schwellenwertlogik basiert (vgl.~\cite[16]{AR88}). Dabei wurde ein neurobiologischer Prozess, der heutzutage bei maschinellen Lernverfahren Standard ist, nicht in dem Modell berücksichtigt: Die dynamische Anpassung des Netzes.
Das Rosenblatt-Perzeptron dagegen realisierte eine Art von ``Lernen``.

Der frühe Enthusiasmus wird jedoch in den 1970er Jahren durch die Einsicht gebremst, dass {insb.} das Rosenblatt-Perzeptron nicht auf beliebig komplexe Probleme angewendet werden kann\footnote{
    \textit{Russell und Norvig} fassen zusammen, dass \textit{Minsky und Papert} in \cite{MP88} bewiesen haben, dass ein Perzeptron alles lernen kann, was es auch darstellen kann, aber es könnte halt nur sehr wenig darstellen (vgl.~\cite[45]{RN09})
}.
Der Lighthill Report\footnote{
    ``Workers entered the field around 1950, and even around 1960, with high hopes that are very far from having been realised in 1972. In no part of the field have the discoveries made so far produced the major impact that was then promised.`` in \url{https://www.chilton-computing.org.uk/inf/literature/reports/lighthill\_report/p001.htm}, ``3 Past disappointments``, abgerufen 28.08.2023
} wird 1973 die britischen Regierung dazu bewegen, das Budget für die Forschung an KI zu kürzen (vgl. ~\cite[45]{RN09}): Neuronale Netze werden als Grundlage künstlicher Intelligenz zunächst verworfen, und Forschungsarbeiten an ihnen geht bis zum Anfang der 1980er Jahre zurück.
Diese Periode ist gemeinhin als ``KI-Winter`` bekannt.\\

Die in Kapitel~\ref{ch:knn} vorgestellten Architekturen und Algorithmen sorgten dann zusammen mit dem technologischen Fortschritt für ein erneutes Aufleben der Forschung an neuronalen Netzen und Künstlicher Intelligenz in den 1980er Jahren.
Einige der Ergebnisse dieser Anstrengungen wurden in Kapitel~\ref{ch:gesundheitswesen} in Auszügen vorgestellt.
Dass dabei die Algorithmen nicht immer der Funktionsweise des natürlichen Vorbildes entsprechen, haben wir bei dem Backpropagation-Verfahren gesehen (siehe Abschnitt~\ref{sec:backpropagation}).

Bei allem Fortschritt, der durch Forschung und Wissenschaft in den vergangenen Jahren erreicht wurde, ist bei den hier vorgestellten Ergebnissen aber vor allem eins deutlich: Die hohe Leistungsfähigkeit einiger dieser Netze kann nur durch Lernen erreicht werden (vgl.~\cite[40]{AHR19}), wozu umfangreiche, qualitativ hochwertige Daten benötigt werden.

\textit{Nguyen und Patrick} stellen in~\cite{NP16} fest, dass klinische Daten aufgrund verschiedener Faktoren wie Heterogenität (maschinell / handschriftlich erstellte Daten) und weiterem Rauschen, wie unbekannten Abkürzungen oder Rechtschreibfehlern, als Trainingsdaten für maschinelles Lernen eine besondere Herausforderung darstellen.
Normierte Daten können deshalb helfen, die Auswertung, Anwendung aber auch den Austausch derselben zu erleichtern, wie das Beispiel aus Abschnitt~\ref{sec:therapieprognose} zeigt (vgl. auch ~\cite[42]{AHR19}).

Wir dürfen schließen, dass Künstliche Intelligenz in Form neuronaler Netze nicht nur durch komplexe mathematische Modelle und hohe Rechenleistung ermöglicht wird: Sie benötigt zu der erfolgreichen Bewältigung ihrer Aufgaben auch eine gesunde Datenbasis für ihre Lernverfahren\footnote{
    \textit{Dash et al} fassen in~\cite{SSM+19} einige wesentliche Punkte zum Thema Big Data im Gesundheitswesen zusammen, ausserdem \textit{Rüping und Sander} in~\cite{RS19}, die dort feststellen: ``Bei der datengetriebenen Entwicklung von Systemen ist zu beachten, dass die Qualität des Ergebnisses zentral von der Qualität der Eingabedaten abhängt.``~\cite[19]{RS19}
}.













%
%
%
%

