@book{2012,
  title = {K\"unstliche {{Intelligenz}}: {{Ein}} Moderner {{Ansatz}}},
  author = {Russell, Stuart J. and Norvig, Peter},
  editor = {Langenau, Frank},
  year = {2012},
  edition = {3., aktualisierte Auflage},
  publisher = {{Pearson, Higher Education}},
  address = {{M\"unchen}},
  isbn = {978-3-86894-098-5},
  keywords = {ai ba ki}
}

@book{2022,
  title = {{Theoretische Informatik}},
  author = {Hoffmann, Dirk W.},
  year = {2022},
  edition = {5., aktualisierte Auflage},
  publisher = {{Hanser}},
  address = {{M\"unchen}},
  abstract = {Das Buch f\"uhrt umfassend in das Gebiet der theoretischen Informatik ein und behandelt den Stoffumfang, der f\"ur das Bachelor-Studium an Universit\"aten und Hochschulen in den F\"achern Informatik und Informationstechnik ben\"otigt wird. Die Darstellung und das didaktische Konzept verfolgen das Ziel, einen durchweg praxisnahen Zugang zu den mitunter sehr theoretisch gepr\"agten Themen zu schaffen. Theoretische Informatik muss nicht trocken sein! Sie kann Spass machen und genau dies versucht das Buch zu vermitteln. Die verschiedenen Methoden und Verfahren werden anhand konkreter Beispiele eingef\"uhrt und durch zahlreiche Querverbindungen wird gezeigt, wie die fundamentalen Ergebnisse der theoretischen Informatik die moderne Informationstechnologie pr\"agen. Das Buch behandelt die Themengebiete: Logik und Deduktion, Automatentheorie, formale Sprachen, Entscheidbarkeitstheorie, Berechenbarkeitstheorie und Komplexit\"atstheorie. Die Lehrinhalte aller Kapitel werden durch zahlreiche \"Ubungsaufgaben komplettiert, so dass sich die Lekt\"ure neben der Verwendung als studienbegleitendes Lehrbuch auch bestens zum Selbststudium eignet},
  isbn = {978-3-446-47029-3},
  langid = {german}
}

@article{AA15,
  title = {Neural {{Network Techniques}} for {{Cancer Prediction}}: {{A Survey}}},
  shorttitle = {Neural {{Network Techniques}} for {{Cancer Prediction}}},
  author = {Agrawal, Shikha and Agrawal, Jitendra},
  year = {2015},
  journal = {Procedia Computer Science},
  volume = {60},
  pages = {769--774},
  issn = {18770509},
  doi = {10.1016/j.procs.2015.08.234},
  urldate = {2023-09-07},
  abstract = {Cancer is a dreadful disease. Millions of people died every year because of this disease. It is very essential for medical practitioners to opt a proper treatment for cancer patients. Therefore cancer cells should be identified correctly. Neural networks are currently a burning research area in medical science, especially in the areas of cardiology, radiology, oncology, urology and etc. In this paper, we are surveying various neural network technologies for classification of cancer. The main aim of this survey in medical diagnostics is to guide researchers to develop most cost effective and user friendly systems, processes and approaches for clinicians.},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\WG5FHWAP\Agrawal und Agrawal - 2015 - Neural Network Techniques for Cancer Prediction A.pdf}
}

@article{AAB+23,
  title = {From {{Pavlov Conditioning}} to {{Hebb Learning}}},
  author = {Agliari, Elena and Aquaro, Miriam and Barra, Adriano and Fachechi, Alberto and Marullo, Chiara},
  year = {2023},
  month = apr,
  journal = {Neural Computation},
  volume = {35},
  number = {5},
  pages = {930--957},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01578},
  abstract = {Hebb's learning traces its origin in Pavlov's classical conditioning; however, while the former has been extensively modeled in the past decades (e.g., by the Hopfield model and countless variations on theme), as for the latter, modeling has remained largely unaddressed so far. Furthermore, a mathematical bridge connecting these two pillars is totally lacking. The main difficulty toward this goal lies in the intrinsically different scales of the information involved: Pavlov's theory is about correlations between concepts that are (dynamically) stored in the synaptic matrix as exemplified by the celebrated experiment starring a dog and a ringing bell; conversely, Hebb's theory is about correlations between pairs of neurons as summarized by the famous statement that neurons that fire together wire together. In this letter, we rely on stochastic process theory to prove that as long as we keep neurons' and synapses' timescales largely split, Pavlov's mechanism spontaneously takes place and ultimately gives rise to synaptic weights that recover the Hebbian kernel.}
}

@book{AB16,
  title = {From {{Neuron}} to {{Cognition}} via {{Computational Neuroscience}}},
  author = {Arbib, Michael A. and Bonaiuto, James J.},
  year = {2016},
  publisher = {{The MIT Press}},
  abstract = {This textbook presents a wide range of subjects in neuroscience from a computational perspective. It offers a comprehensive, integrated introduction to core topics, using computational tools to trace a path from neurons and circuits to behavior and cognition. Moreover, the chapters show how computational neuroscience \textendash{} methods for modeling the causal interactions underlying neural systems \textendash{} complements empirical research in advancing the understanding of brain and behavior. The chapters \textendash{} all by leaders in the field, and carefully integrated by the editors \textendash{} cover such subjects as action and motor control; neuroplasticity, neuromodulation, and reinforcement learning; vision; and language \textendash{} the core of human cognition. The book can be used for advanced undergraduate or graduate level courses. It presents all necessary background in neuroscience beyond basic facts about neurons and synapses and general ideas about the structure and function of the human brain. Students should be familiar with differential equations and probability theory, and be able to pick up the basics of programming in MATLAB and/or Python. Slides, exercises, and other ancillary materials are freely available online, and many of the models described in the chapters are documented in the brain operation database, BODB (which is also described in a book chapter). Contributors Michael A. Arbib, Joseph Ayers, James Bednar, Andrej Bicanski, James J. Bonaiuto, Nicolas Brunel, Jean-Marie Cabelguen, Carmen Canavier, Angelo Cangelosi, Richard P. Cooper, Carlos R. Cortes, Nathaniel Daw, Paul Dean, Peter Ford Dominey, Pierre Enel, Jean-Marc Fellous, Stefano Fusi, Wulfram Gerstner, Frank Grasso, Jacqueline A. Griego, Ziad M. Hafed, Michael E. Hasselmo, Auke Ijspeert, Stephanie Jones, Daniel Kersten, Jeremie Knuesel, Owen Lewis, William W. Lytton, Tomaso Poggio, John Porrill, Tony J. Prescott, John Rinzel, Edmund Rolls, Jonathan Rubin, Nicolas Schweighofer, Mohamed A. Sherif, Malle A. Tagamets, Paul F. M. J. Verschure, Nathan Vierling-Claasen, Xiao-Jing Wang, Christopher Williams, Ransom Winder, Alan L. Yuille},
  isbn = {0-262-03496-4},
  file = {C:\Users\thorstensuckow\Zotero\storage\E9KW92I5\From neuron to cognition via computational neuroscience.pdf}
}

@incollection{Abl11,
  title = {Teilgebiete Der {{Biomathematik}} Und Ihre {{Bez\"uge}} Zur {{Menschheitsgeschichte}}},
  booktitle = {Biomathematische {{Modelle}} Im {{Unterricht}}: {{Fachwissenschaftliche}} Und Didaktische {{Grundlagen}} Mit {{Unterrichtsmaterialien}}},
  author = {Ableitinger, Christoph},
  year = {2011},
  pages = {3--28},
  publisher = {{Vieweg+Teubner}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-8348-9770-1_1},
  abstract = {Die Demographie befasst sich ganz allgemein mit der menschlichen Bev\"olkerung, ihrer Struktur und ihren Bewegungen. Kenngr\"o\ss en, die in der Demographie h\"aufig verwendet werden, sind z. B. die Geburten- und die Sterberate. Dabei ist nat\"urlich darauf zu achten, dass diese Gr\"o\ss en in unterschiedlichen Altersgruppen auch unterschiedlicheWerte annehmen. Ebenso wie die Fertilit\"at, also die mittlere Anzahl an Nachkommen pro Zeiteinheit und Frau. Und nicht nur vom Alter, auch von der geographischen Lage h\"angen diese Gr\"o\ss en und damit auch die durchschnittliche Lebenserwartung bzw. die Gesamtzahl an Nachkommen pro Jahrgang ab. So genannte Alterspyramiden, also Darstellungen der Bev\"olkerungsstruktur nach Altersklassen, sehen demnach in unterschiedlichen Teilen der Welt auch qualitativ sehr verschieden aus.},
  isbn = {978-3-8348-9770-1}
}

@article{Abr02,
  title = {({{Physio}})Logical Circuits: {{The}} Intellectual Origins of the {{McCulloch-Pitts}} Neural Networks},
  shorttitle = {({{Physio}})Logical Circuits},
  author = {Abraham, Tara H.},
  year = 2002,
  journal = {Journal of the History of the Behavioral Sciences},
  volume = {38},
  number = {1},
  pages = {3--25},
  issn = {0022-5061, 1520-6696},
  doi = {10.1002/jhbs.1094},
  urldate = {2023-08-08},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\XDQ4VBHM\Abraham - 2002 - (Physio)logical circuits The intellectual origins.pdf}
}

@inproceedings{AGM86,
  title = {Concepts {{In Distributed Systems}}},
  booktitle = {Optical and {{Hybrid Computing}}},
  author = {Anderson, James A. and Golden, Richard M. and Murphy, Gregory L.},
  editor = {Szu, Harold H.},
  year = {1986},
  month = feb,
  pages = {260},
  address = {{Leesburg}},
  doi = {10.1117/12.964018},
  urldate = {2023-09-07},
  abstract = {WWee ddeessccrriibbee aa parraalllleell,, ddiissttrriibbuutteedd,, asssoocciiaattiivvee mmooddeell basseedd on Hebbian modification off connectioonn strengths between ssiimmple eelleemments. Sommee extensionnss tto the ssimple model are descrriibbeedd aanndd interpprreettaattiioonnss ooff thhee mooddeell aass aa cgrrraaddiienntt descenntt algorithm and as maximizinngg a probability density ffuunnccttiioonn aarree ggiivveen. Thhee modell is applied tto ccooncept formmaattiioonn,, siinnccee mmoosstt verrssiioonnss ooff thiiss modelliinngg approach form equivalence classes of inpuuttss thaatt acctt like much llike ppssyycchhoological ccoonncceeppts. Somee computer ssiimulations oof concept--lliikkee bbeehhaavviioorr aarree described. Soommee kinnddss ooff computation can be performed effectiveellyy with these techniquess..},
  file = {C:\Users\thorstensuckow\Zotero\storage\X9YSITYU\Anderson et al. - 1986 - Concepts In Distributed Systems.pdf}
}

@article{AHS85,
  title = {A Learning Algorithm for Boltzmann Machines},
  author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  year = {1985},
  journal = {Cognitive Science},
  volume = {9},
  number = {1},
  pages = {147--169},
  issn = {0364-0213},
  doi = {10.1016/S0364-0213(85)80012-4},
  abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
  file = {C:\Users\thorstensuckow\Zotero\storage\HG55TRIT\[AHS85] A learning algorithm for boltzmann machines.pdf}
}

@article{ALP+13,
  title = {Artificial Neural Networks in Medical Diagnosis},
  author = {Amato, Filippo and L{\'o}pez, Alberto and {Pe{\~n}a-M{\'e}ndez}, Eladia Mar{\'i}a and Va{\v n}hara, Petr and Hampl, Ale{\v s} and Havel, Josef},
  year = {2013},
  month = jul,
  journal = {Journal of Applied Biomedicine},
  volume = {11},
  number = {2},
  pages = {47--58},
  issn = {1214021X, 12140287},
  doi = {10.2478/v10136-012-0031-x},
  urldate = {2023-09-07},
  abstract = {An extensive amount of information is currently available to clinical specialists, ranging from details of clinical symptoms to various types of biochemical data and outputs of imaging devices. Each type of data provides information that must be evaluated and assigned to a particular pathology during the diagnostic process. To streamline the diagnostic process in daily routine and avoid misdiagnosis, artificial intelligence methods (especially computer aided diagnosis and artificial neural networks) can be employed. These adaptive learning algorithms can handle diverse types of medical data and integrate them into categorized outputs. In this paper, we briefly review and discuss the philosophy, capabilities, and limitations of artificial neural networks in medical diagnosis through selected examples.},
  langid = {english},
  file = {C\:\\Users\\thorstensuckow\\Zotero\\storage\\LCJCEQ53\\Amato et al. - 2013 - Artificial neural networks in medical diagnosis.pdf;C\:\\Users\\thorstensuckow\\Zotero\\storage\\UGUU3T6T\\[ALP+13] Artificial neural networks in medical diagnosis.pdf}
}

@book{AR88,
  title = {Neurocomputing: {{Foundations}} of {{Research}}},
  editor = {Anderson, James A. and Rosenfeld, Edward},
  year = {1988},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {0-262-01097-6}
}

@book{AR88a,
  title = {Neurocomputing: {{Foundations}} of {{Research}}},
  editor = {Anderson, James A. and Rosenfeld, Edward},
  year = {1988},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {0-262-01097-6},
  file = {C:\Users\thorstensuckow\Zotero\storage\RMZLKZ9N\[AR88] Neurocomputing - foundations of research.pdf}
}

@article{Arb00,
  title = {Warren {{McCulloch}}'s {{Search}} for the {{Logic}} of the {{Nervous System}}},
  author = {Arbib, Michael},
  year = {2000},
  month = feb,
  journal = {Perspectives in biology and medicine},
  volume = {43},
  pages = {193--216},
  doi = {10.1353/pbm.2000.0001}
}

@book{Arb03,
  title = {The Handbook of Brain Theory and Neural Networks},
  editor = {Arbib, Michael A.},
  year = {2003},
  edition = {2nd ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-01197-6},
  langid = {english},
  lccn = {QP363.3 .H36 2003},
  keywords = {{Handbooks, manuals, etc},Neural networks (Computer science),Neural networks (Neurobiology)},
  file = {C:\Users\thorstensuckow\Zotero\storage\69YVJEN9\Arbib - 2003 - The handbook of brain theory and neural networks.pdf}
}

@incollection{BB,
  title = {The {{History}} of {{Aerospace Research}} at {{Cornell Aeronautical Laboratory}} and {{Calspan}}},
  booktitle = {44th {{AIAA Aerospace Sciences Meeting}} and {{Exhibit}}},
  author = {Burns, Kevin and Bruckner, Adam},
  doi = {10.2514/6.2006-335}
}


@article{BHU+18,
  title = {Skin {{Cancer Classification Using Convolutional Neural Networks}}: {{Systematic Review}}},
  author = {Brinker, Titus Josef and Hekler, Achim and Utikal, Jochen Sven and Grabe, Niels and Schadendorf, Dirk and Klode, Joachim and Berking, Carola and Steeb, Theresa and Enk, Alexander H and {von Kalle}, Christof},
  year = {2018},
  month = oct,
  journal = {J Med Internet Res},
  volume = {20},
  number = {10},
  eprint = {30333097},
  eprinttype = {pubmed},
  pages = {e11936},
  issn = {1438-8871},
  doi = {10.2196/11936},
  abstract = {Background: State-of-the-art classifiers based on convolutional neural networks (CNNs) were shown to classify images of skin cancer on par with dermatologists and could enable lifesaving and fast diagnoses, even outside the hospital via installation of apps on mobile devices. To our knowledge, at present there is no review of the current work in this research area. Objective: This study presents the first systematic review of the state-of-the-art research on classifying skin lesions with CNNs. We limit our review to skin lesion classifiers. In particular, methods that apply a CNN only for segmentation or for the classification of dermoscopic patterns are not considered here. Furthermore, this study discusses why the comparability of the presented procedures is very difficult and which challenges must be addressed in the future. Methods: We searched the Google Scholar, PubMed, Medline, ScienceDirect, and Web of Science databases for systematic reviews and original research articles published in English. Only papers that reported sufficient scientific proceedings are included in this review. Results: We found 13 papers that classified skin lesions using CNNs. In principle, classification methods can be differentiated according to three principles. Approaches that use a CNN already trained by means of another large dataset and then optimize its parameters to the classification of skin lesions are the most common ones used and they display the best performance with the currently available limited datasets. Conclusions: CNNs display a high performance as state-of-the-art skin lesion classifiers. Unfortunately, it is difficult to compare different classification methods because some approaches use nonpublic datasets for training and/or testing, thereby making reproducibility difficult. Future publications should use publicly available benchmarks and fully disclose methods used for training to allow comparability.},
  keywords = {carcinoma classification,convolutional neural networks,deep learning,lesion classification,melanoma classification,skin cancer},
  file = {C:\Users\thorstensuckow\Zotero\storage\MDNWVNAI\Brinker et al. - 2018 - Skin Cancer Classification Using Convolutional Neu.pdf}
}

@incollection{BHWM12,
  title = {Vektorr\"aume Beliebiger {{Dimensionen}}},
  booktitle = {H\"ohere {{Mathematik}} F\"ur {{Ingenieure Band II}}: {{Lineare Algebra}}},
  author = {Burg, Klemens and Haf, Herbert and Wille, Friedrich and Meister, Andreas},
  year = {2012},
  pages = {75--146},
  publisher = {{Vieweg+Teubner Verlag}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-8348-2267-3_2},
  abstract = {In diesem Abschnitt werden zun\"achst die Vektorr\"aume \vphantom\{\}\vphantom\{\} \textbackslash backslashmathbb \{R\}\^n\vphantom\{\}\vphantom\{\}und \vphantom\{\}\vphantom\{\} \textbackslash backslashmathbb \{C\}\^n\vphantom\{\}\vphantom\{\}behandelt, einschlie\ss lich linearer Gleichungssysteme. Anschlie\ss end werden allgemeinere algebraische Strukturen er\"ortert: Gruppen, K\"orper und Vektorr\"aume \"uber beliebigen K\"orpern samt linearen Abbildungen. Diese abstrakteren Teile (ab Abschn. 2.3) k\"onnen vom anwendungsorientierten Leser zun\"achst \"ubersprungen werden.},
  isbn = {978-3-8348-2267-3}
}

@book{SD07,
  doi = {10.1055/b-002-89576},
  url = {https://doi.org/10.1055%2Fb-002-89576},
  year = 2007,
  publisher = {Georg Thieme Verlag},
  editor = {Stefan Silbernagl and Agamemnon Despopoulos},
  title = {Taschenatlas Physiologie}
}

@book{BCP18,
  title = {Neurowissenschaften: {{Ein}} Grundlegendes Lehrbuch F\"ur Biologie, Medizin Und Psychologie},
  author = {Bear, Mark F. and Connors, Barry W. and Paradiso, Michael A.},
  editor = {Engel, Andreas K.},
  year = {2018},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-57263-4},
  isbn = {978-3-662-57263-4}
}

@book{KSJ+13,
  title = {Principles of Neural Science, 5th Edition},
  author = {Kandel, E.R. and Schwartz, James H. and Jessell, Thomas M. and Siegelbaum, Steven A. and Hudspeth, A. J.},
  year = {2013},
  series = {{{McGraw-Hill}}'s {{AccessMedicine}}},
  publisher = {{McGraw-Hill Education}},
  isbn = {978-0-07-139011-8},
  lccn = {2012023071}
}


title = {Principles of Neural Science, 5th Edition},
author = {Kandel, E.R. and Schwartz, James H. and Jessell, Thomas M. and Siegelbaum, Steven A. and Hudspeth, A. J.},
year = {2013},
series = {{{McGraw-Hill}}'s {{AccessMedicine}}},
publisher = {{McGraw-Hill Education}},
isbn = {978-0-07-139011-8},
lccn = {2012023071}
}


@book{BJ17,
  title = {Neural Computing: An Introduction},
  shorttitle = {Neural Computing},
  author = {Beale, Russell and Jackson, Tom},
  year = {2017},
  series = {A {{Taylor}} \& {{Francis}} Book},
  edition = {First issued in hardback},
  publisher = {{CRC Press}},
  address = {{Boca Raton London New York}},
  isbn = {978-0-85274-262-4 978-1-138-41309-2},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\X3VPYYLB\Beale und Jackson - 2017 - Neural computing an introduction.pdf}
}

@article{BL73,
  title = {Long-Lasting Potentiation of Synaptic Transmission in the Dentate Area of the Anaesthetized Rabbit Following Stimulation of the Perforant Path},
  author = {Bliss, T. V. P. and L{\o}mo, T.},
  year = {1973},
  journal = {The Journal of Physiology},
  volume = {232},
  number = {2},
  pages = {331--356},
  doi = {10.1113/jphysiol.1973.sp010273},
  abstract = {1. The after-effects of repetitive stimulation of the perforant path fibres to the dentate area of the hippocampal formation have been examined with extracellular micro-electrodes in rabbits anaesthetized with urethane. 2. In fifteen out of eighteen rabbits the population response recorded from granule cells in the dentate area to single perforant path volleys was potentiated for periods ranging from 30 min to 10 hr after one or more conditioning trains at 10\textendash 20/sec for 10\textendash 15 sec, or 100/sec for 3\textendash 4 sec. 3. The population response was analysed in terms of three parameters: the amplitude of the population excitatory post-synaptic potential (e.p.s.p.), signalling the depolarization of the granule cells, and the amplitude and latency of the population spike, signalling the discharge of the granule cells. 4. All three parameters were potentiated in 29\% of the experiments; in other experiments in which long term changes occurred, potentiation was confined to one or two of the three parameters. A reduction in the latency of the population spike was the commonest sign of potentiation, occurring in 57\% of all experiments. The amplitude of the population e.p.s.p. was increased in 43\%, and of the population spike in 40\%, of all experiments. 5. During conditioning at 10\textendash 20/sec there was massive potentiation of the population spike (`frequency potentiation'). The spike was suppressed during stimulation at 100/sec. Both frequencies produced long-term potentiation. 6. The results suggest that two independent mechanisms are responsible for long-lasting potentiation: (a) an increase in the efficiency of synaptic transmission at the perforant path synapses; (b) an increase in the excitability of the granule cell population.},
  file = {C:\Users\thorstensuckow\Zotero\storage\NMY4769Z\Bliss und Lømo - 1973 - Long-lasting potentiation of synaptic transmission.pdf}
}

@article{BM03,
  title = {The Legacy of {{Donald O}}. {{Hebb}}: More than the {{Hebb Synapse}}},
  author = {Brown, Richard E. and Milner, Peter M.},
  year = {2003},
  month = dec,
  journal = {Nature Reviews Neuroscience},
  volume = {4},
  number = {12},
  pages = {1013--1019},
  issn = {1471-0048},
  doi = {10.1038/nrn1257},
  abstract = {Neuroscientists associate the name of Donald O. Hebb with the Hebbian synapse and the Hebbian learning rule, which underlie connectionist theories and synaptic plasticity, but Hebb's work has also influenced developmental psychology, neuropsychology, perception and the study of emotions, as well as learning and memory. Here, we review the work of Hebb and its lasting influence on neuroscience in honour of the 2004 centenary of his birth.}
}

@incollection{Bre96,
  title = {Die {{Maxwell-Boltzmann-Verteilung}}},
  booktitle = {Statistische {{Theorie}} Der {{W\"arme}}: {{Gleichgewichtsph\"anomene}}},
  author = {Brenig, Wilhelm},
  year = {1996},
  pages = {55--59},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-61038-7_7},
  abstract = {In diesem Kapitel beschreiben wir einige \"Uberlegungen der elementaren klassischen kinetischen Gastheorie, von denen ausgehend man ziemlich direkt zur kanonischen Gesamtheit (6.27) gef\"uhrt wird.},
  isbn = {978-3-642-61038-7}
}



@incollection{Bur,
  title = {The {{History}} of {{Aerospace Research}} at {{Cornell Aeronautical Laboratory}} and {{Calspan}} - 1946 to 1996},
  booktitle = {Space 2004 {{Conference}} and {{Exhibit}}},
  author = {Burns, Kevin},
  doi = {10.2514/6.2004-5884}
}

@article{BY86,
  title = {Spin Glasses: {{Experimental}} Facts, Theoretical Concepts, and Open Questions},
  author = {Binder, K. and Young, A. P.},
  year = {1986},
  month = oct,
  journal = {Rev. Mod. Phys.},
  volume = {58},
  number = {4},
  pages = {801--976},
  publisher = {{American Physical Society}},
  doi = {10.1103/RevModPhys.58.801}
}

@article{Cal20,
  title = {`{{It}} Will Change Everything': {{DeepMind}}'s {{AI}} Makes Gigantic Leap in Solving Protein Structures},
  author = {Callaway, Ewen},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  pages = {203--204},
  doi = {10.1038/d41586-020-03348-4}
}

@inproceedings{CBS+16,
  title = {Doctor {{AI}}: {{Predicting Clinical Events}} via {{Recurrent Neural Networks}}},
  booktitle = {Proceedings of the 1st {{Machine Learning}} for {{Healthcare Conference}}},
  author = {Choi, Edward and Bahadori, Mohammad Taha and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
  editor = {{Doshi-Velez}, Finale and Fackler, Jim and Kale, David and Wallace, Byron and Wiens, Jenna},
  year = {2016},
  month = aug,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {56},
  pages = {301--318},
  publisher = {{PMLR}},
  address = {{Northeastern University, Boston, MA, USA}},
  abstract = {Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients and 2,128 physicians over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79\% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.}
}

@article{CL78,
  title = {Eye-{{Specific Termination Bands}} in {{Tecta}} of {{Three-Eyed Frogs}}},
  author = {{Constantine-Paton}, Martha and Law, Margaret I.},
  year = {1978},
  journal = {Science},
  volume = {202},
  number = {4368},
  pages = {639--641},
  doi = {10.1126/science.309179},
  abstract = {An extra eye primordium was implanted into the forebrain region of embryonic Rana pipiens. During development both normal and supernumerary optic tracts terminated within a single, previously uninnervated tectal lobe. Autoradiographic tracing of either the normal or supernumerary eye's projection revealed distinct, eye-specific bands of radioactivity running rostrocaudally through the dually innervated tectum. Interactions among axons of retinal ganglion cells, possibly mediated through tectal neurons, must be invoked to explain this stereotyped disruption of the normally continuous retinal termination pattern.},
  file = {C:\Users\thorstensuckow\Zotero\storage\4XXZL78Q\[CPL78] Eye-Specific Termination Bands in Tecta of Three-Eyed Frogs.pdf}
}

@article{Coo05,
  title = {Donald {{O}}. {{Hebb}}'s Synapse and Learning Rule: A History and Commentary},
  author = {Cooper, Steven J.},
  year = {2005},
  journal = {Neuroscience \& Biobehavioral Reviews},
  volume = {28},
  number = {8},
  pages = {851--874},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2004.09.009},
  abstract = {This year sees the anniversary of Donald O. Hebb's birth, in July 1904. The impact of his work, especially through his neurophysiological postulate, as described in his magnum opus, The organization of behaviour (1949), has been profound in contemporary neuroscience. Hebb's life, and the scientific milieu in psychology and neurophysiology which preceded and informed Hebb's work are described. His core postulate, which gave rise to such eponymous expressions as the Hebbian synapse and the Hebbian learning rule, is examined in some detail, as well as the part it played in his higher-order theoretical constructs concerned with neocortical structure and function. Early models which made use of the Hebbian synapse are described, and then illustrative examples are given detailing the impact of Hebb's idea in relation to learning and memory, synaptic plasticity and stability, and the question of persistent cortical activity underlying forms of short-term memory.},
  keywords = {Donald Hebb's life,Hebbian learning rule,Hebbian synapse,Karl Lashley,Learning,Long-term potentiation,Lorente de N\'o,Memory,Synaptic plasticity,The organization of behaviour}
}

@article{Cow90,
  title = {Discussion: {{McCulloch-Pitts}} and Related Neural Nets from 1943 to 1989},
  author = {Cowan, Jack D.},
  year = {1990},
  month = jan,
  journal = {Bulletin of Mathematical Biology},
  volume = {52},
  number = {1},
  pages = {73--97},
  issn = {1522-9602},
  doi = {10.1007/BF02459569},
  abstract = {The McCulloch-Pitts paper ``A Logical Calculus of the Ideas Immanent in Nervous Activity'' was published in theBulletin of Mathematical Biophysics in 1943, a decade before the work of Hodgkin, Huxley, Katz and Eccles. The McCulloch-Pitts neuron is an extremely simplified representation of neural properties, based simply on the existence of a threshold for the activation of an action potential.}
}

@article{CS11,
  title = {One {{Cell}} to {{Rule Them All}}, and in the {{Dendrites Bind Them}}},
  author = {Costa, Rui and Sj{\"o}str{\"o}m, P. Jesper},
  year = {2011},
  journal = {Frontiers in Synaptic Neuroscience},
  volume = {3},
  issn = {1663-3563},
  doi = {10.3389/fnsyn.2011.00005},
  file = {C:\Users\thorstensuckow\Zotero\storage\Z9ESFAK6\Costa und Sjöström - 2011 - One Cell to Rule Them All, and in the Dendrites Bi.pdf}
}

@article{DH20,
  title = {Counterfactual Prediction Is Not Only for Causal Inference},
  author = {Dickerman, Barbra A. and Hern{\'a}n, Miguel A.},
  year = {2020},
  month = jul,
  journal = {European Journal of Epidemiology},
  volume = {35},
  number = {7},
  pages = {615--617},
  issn = {1573-7284},
  doi = {10.1007/s10654-020-00659-8}
}

@incollection{Dor91,
  title = {Konnektionismus \textemdash{} Eine {{Einf\"uhrung}}},
  booktitle = {Konnektionismus: {{Von}} Neuronalen {{Netzwerken}} Zu Einer ,,nat\"urlichen`` {{KI}}},
  author = {Dorffner, Georg},
  year = {1991},
  pages = {15--83},
  publisher = {{Vieweg+Teubner Verlag}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-322-94665-2_2},
  abstract = {Die Grundlage des Konnektionismus sind sogenannte neuronale Netzwerke (oder oft auch neurale Netzwerke genannte \textemdash{} K\"ohle 1990). Diese sind eine neuartige Modellvorstellung der Informationsverarbeitung mittels Prozessoren, die sich in mehreren Punkten sowohl von der von Neumann Architektur unterscheidet, als auch vom geschilderten Aufbau der meisten in der AI verwendeten Programme. Diese Modellvorstellung ist zun\"achst unabh\"angig von ihrer Realisierung, legt aber auch eine neue Art von Hardware nahe. Da diese Hardware bestenfalls in Ans\"atzen existiert, werden die meisten Modelle, von denen die Rede sein wird, auf konventionellen Computern simuliert. Die Idee der neuronalen Netzwerke ist die folgende:Die Informationsverarbeitung geschieht durch eine gro\ss e Anzahl von relativ einfachen Prozessoren, die in einem dichten Netzwerk miteinander verbunden sind. Diese Prozessoren (auch Units genannt1) arbeiten lokal, jeder fir sich allein, und kommunizieren mit anderen Units nur via Signale, die sie \"uber die Verbindungen senden.Schematisch ist das in Abb. 2.1 dargestellt. Im Gegensatz zur Architektur in Abb. 1.2 gibt es eine gro\ss e Anzahl voneinander relativ unabh\"angiger Prozessoren, die lokal f\"ur sich arbeiten k\"onnen. Es wird keine zentrale Steuerung angenommen, die gleichzeitig alle Prozessoren (Units) einsehen und danach handeln k\"onnte. Daher ergibt sich auch das Ergebnis der Verarbeitung erst aus der Gesamtheit aller Einzelprozessoren},
  isbn = {978-3-322-94665-2}
}

@article{DOSW08,
  title = {The {{Protein Folding Problem}}},
  author = {Dill, Ken A. and Ozkan, S. Banu and Shell, M. Scott and Weikl, Thomas R.},
  year = {2008},
  journal = {Annual Review of Biophysics},
  volume = {37},
  number = {1},
  pages = {289--316},
  doi = {10.1146/annurev.biophys.37.092707.153558},
  abstract = {The ``protein folding problem'' consists of three closely related puzzles: (a) What is the folding code? (b) What is the folding mechanism? (c) Can we predict the native structure of a protein from its amino acid sequence? Once regarded as a grand challenge, protein folding has seen great progress in recent years. Now, foldable proteins and nonbiological polymers are being designed routinely and moving toward successful applications. The structures of small proteins are now often well predicted by computer methods. And, there is now a testable explanation for how a protein can fold so quickly: A protein solves its large global optimization problem as a series of smaller local optimization problems, growing and assembling the native structure from peptide fragments, local structures first.},
  pmid = {18573083}
}

@article{EKN+17,
  title = {Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
  year = {2017},
  month = feb,
  journal = {Nature},
  volume = {542},
  number = {7639},
  pages = {115--118},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature21056},
  urldate = {2023-09-07},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\NH7WC4HT\Esteva et al. - 2017 - Dermatologist-level classification of skin cancer .pdf}
}

@incollection{Ert21,
  title = {Maschinelles {{Lernen}} Und {{Data Mining}}},
  booktitle = {Grundkurs {{K\"unstliche Intelligenz}}: {{Eine}} Praxisorientierte {{Einf\"uhrung}}},
  author = {Ertel, Wolfgang},
  year = {2021},
  pages = {201--283},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-32075-1_8},
  abstract = {Maschinelles Lernen dominiert heute Forschung und Anwendungen in der KI. Wir stellen in diesem Einf\"uhrungskapitel einige einfache aber wichtige Lernalgorithmen zusammen mit wichtigen Begriffen und Methoden vor.},
  isbn = {978-3-658-32075-1}
}

@incollection{Eil19,
  title = {Nervenzellen},
  booktitle = {Physiologie Des Menschen: Mit Pathophysiologie},
  author = {Eilers, J.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {57--64},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_5},
  abstract = {Aufgabe der Neurone ist die Informationsverarbeitung. Die hierf\"ur notwendige Vernetzung mit anderen Nervenzellen bedingt eine Verzweigung der Zellausl\"aufer, die beachtliche Dimensionen annehmen kann, die Neurone aber auch vor Versorgungsprobleme stellt. Dendriten sind verzweigte Zellausl\"aufer, die der Informationsaufnahme \"uber Synapsen dienen. Dendritische Dornforts\"atze stellen spezialisierte Kontaktstellen und kleinste funktionelle Verrechnungseinheiten dar. Im Soma werden elektrische Signale integriert; ein zur \"uberschwelligen Erregung f\"uhrendes Verrechnungsergebnis wird \"uber das Axon weitergeleitet und an Pr\"asynapsen auf nachgeschaltete Nervenzellen \"ubertragen. Nervenzellen und die von ihnen aufgebauten Vernetzungen unterliegen bedarfsabh\"angigen strukturellen Ver\"anderungen. Diese treten bei Lernvorg\"angen auf; bei einigen degenerativen Erkrankungen des Nervensystems ist die strukturelle Plastizit\"at gest\"ort.},
  isbn = {978-3-662-56468-4}
}


@incollection{Fak19,
  title = {Grundlagen Der Zellul\"aren {{Erregbarkeit}}},
  booktitle = {Physiologie Des {{Menschen}}: Mit {{Pathophysiologie}}},
  author = {Fakler, B.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {38--54},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_4},
  abstract = {Die Aufnahme, Weiterleitung und Verarbeitung von Reizen basiert auf elektrischen Prozessen an der Plasmamembran von Neuronen und Sinneszellen. Grundlage dieser Prozesse ist der Fluss anorganischer Ionen durch eine besondere Klasse von Membranproteinen, die Ionenkan\"ale. Kan\"ale erlauben den Durchtritt entweder nur f\"ur eine Ionenart (selektive Kan\"ale) oder f\"ur verschiedene Ionensorten (nicht-selektive Kan\"ale). F\"ur die Aktivierung von Ionenkan\"alen ist Energie notwendig. In den meisten Kan\"alen wird diese entweder aus der Depolarisierung der Membranspannung (spannungs-gesteuerte Kan\"ale) oder der Bindung eines Liganden (ligand-aktivierte Kan\"ale) bezogen. Die resultierende Bewegung des Spannungssensors bzw. der Ligandbindungsdom\"ane f\"uhrt \"uber Konformations\"anderungen der porenbildenden Transmembransegmente zu einer Aufweitung (= \"Offnung) der Kanalpore.},
  isbn = {978-3-662-56468-4}
}

@book{Fau94,
  title = {Fundamentals of {{Neural Networks}}: {{Architectures}}, {{Algorithms}}, and {{Applications}}},
  author = {Fausett, Laurene},
  year = {1994},
  publisher = {{Prentice-Hall, Inc.}},
  address = {{USA}},
  isbn = {0-13-334186-0}
}

@article{FC54,
  title = {Simulation of Self-Organizing Systems by Digital Computer},
  author = {Farley, B. and Clark, W.},
  year = {1954},
  journal = {Transactions of the IRE Professional Group on Information Theory},
  volume = {4},
  number = {4},
  pages = {76--84},
  doi = {10.1109/TIT.1954.1057468}
}

@incollection{FE19,
  title = {Ruhemembranpotenzial Und {{Aktionspotenzial}}},
  booktitle = {Physiologie Des {{Menschen}}: Mit {{Pathophysiologie}}},
  author = {Fakler, B. and Eilers, J.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {65--71},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_6},
  abstract = {Alle erregbaren Zellen des menschlichen Organismus weisen ein Ruhemembranpotenzial auf, welches ma\ss geblich durch die Diffusionspotenziale von Kalium-, Chlorid- und Natrium-Ionen bestimmt wird. Diffusionspotenziale entstehen als Folge von Ladungstrennung an einer Zellmembran, \"uber die Ionen ungleich verteilt sind (Konzentrationsgradient) und die eine selektive Permeabilit\"at f\"ur die jeweiligen Ionen aufweisen. In Zellen, in denen die selektive Permeabilit\"at durch Kalium-Kan\"ale bestimmt wird, liegt das Ruhemembranpotenzial bei etwa -90 mV. Sind zus\"atzlich Na+-permeable Kan\"ale, auch in geringem Umfang, vorhanden, liegt das Ruhemembranpotenzial positiver, bei Werten zwischen -65 und -90 mV. Starke Reize f\"uhren in erregbaren Zellen zu einer kurzzeitigen und stereotyp ablaufenden \"Anderung des Membranpotenzials, dem Aktionspotenzial. Durch eine reiz-induzierte initiale Depolarisation werden die f\"ur das Ruhemembranpotenzial verantwortlichen Kaliumkan\"ale blockiert und spannungsgesteuerte Natrium (Nav)- und Kalium (Kv)-Kan\"ale aktiviert.},
  isbn = {978-3-662-56468-4}
}

@incollection{Fis19,
  title = {Lineare {{Geometrie}} Im N-Dimensionalen Reellen {{Raum}}},
  booktitle = {Lernbuch {{Lineare Algebra}} Und {{Analytische Geometrie}}: {{Das Wichtigste}} Ausf\"uhrlich F\"ur Das {{Lehramts-}} Und {{Bachelorstudium}}},
  author = {Fischer, Gerd},
  year = {2019},
  pages = {1--82},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-27343-9_1},
  abstract = {Dieses einf\"uhrende Kapitel soll als elementare Grundlage f\"ur das Studium der linearen Algebra dienen. Es werden Fragen behandelt, die auch in den Lehrpl\"anen von Gymnasien enthalten sind. Studierende des Lehramts k\"onnen dadurch diese Themen von einem etwas h\"oheren Standpunkt betrachten und sich \"uberzeugen, dass f\"ur exakte Begr\"undungen ein angemessener theoretischer Rahmen n\"utzlich ist. Aber auch alle anderen Leser k\"onnen sich dadurch f\"ur das weitere Studium der linearen Algebra zus\"atzlich motivieren.},
  isbn = {978-3-658-27343-9}
}

@article{FJOA19,
  title = {Effect of Annealing on the Mechanical Characteristics of Steel Welded Joint},
  author = {Fayomi, O.S.I. and Joshua, T.O. and Olatuja, F.H. and Agboola, O.},
  year = {2019},
  journal = {Procedia Manufacturing},
  volume = {35},
  pages = {1387--1394},
  issn = {23519789},
  doi = {10.1016/j.promfg.2019.09.008},
  urldate = {2023-09-05},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\6LWEQQ3Z\Fayomi et al. - 2019 - Effect of annealing on the mechanical characterist.pdf}
}

@article{FMI83,
  title = {Neocognitron: {{A}} Neural Network Model for a Mechanism of Visual Pattern Recognition},
  author = {Fukushima, Kunihiko and Miyake, Sei and Ito, Takayuki},
  year = {1983},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-13},
  number = {5},
  pages = {826--834},
  doi = {10.1109/TSMC.1983.6313076}
}

@article{FMMS01,
  title = {Influence of Annealing on the Microstructural, Tensile and Fracture Properties of Polypropylene Films},
  author = {{Ferrer-Balas}, D. and Maspoch, M. Ll and Martinez, A. B. and Santana, O. O.},
  year = {2001},
  journal = {Polymer},
  volume = {42},
  number = {4},
  pages = {1697--1705},
  issn = {0032-3861},
  doi = {10.1016/S0032-3861(00)00487-0},
  abstract = {The influence of annealing temperature on the fracture properties of iPP films (one homopolymer and two propylene\textendash ethylene block copolymers) is presented. The fracture behaviour is studied by means of the Essential Work of Fracture (EWF) procedure, and is complemented by the study of the effect of thermal treatment on tensile properties and microstructure, using differential scanning calorimetry (DSC) and wide-angle X-ray scattering (WAXS). It is shown that the initial metastable phase of quenched iPP films, widely known as smectic, transforms gradually into the monoclinic form as the annealing temperature is increased, resulting in an important improvement of the tensile properties, whereas the fracture parameters have different evolutions depending on the ethylene content. The reasons for a decrease in the essential work term and an increase in the plastic term as the crystal perfection grows are discussed on the basis of the microstructural changes of the crystalline phase and the smectic\textendash monoclinic strain-induced phase transformation.},
  keywords = {Annealing,Essential work of fracture,Polypropylene}
}

@incollection{Fro19,
  title = {Transport in {{Membranen}} Und {{Epithelien}}},
  booktitle = {Physiologie Des {{Menschen}}: Mit {{Pathophysiologie}}},
  author = {Fromm, M.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {22--37},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_3},
  abstract = {Zellmembranen und Epithelien halten durch ihre Barrierefunktion und den Transport von Soluten und Wasser ein konstantes inneres Milieu aufrecht. Transport durch Epithelien erfolgt transzellul\"ar durch die Zellmembranen und parazellul\"ar durch die tight junction. Transport durch Zellmembranen wird durch Kan\"ale, Carrier und Pumpen (Transport-ATPasen) vermittelt. Die tight junction besteht aus abdichtenden und kanalbildenden Proteinen. Epithelien werden als ,,leck`` bezeichnet, wenn die tight junction permeabler f\"ur Ionen ist als die apikale Zellmembran (z.B. proximales Nephron, D\"unndarm), w\"ahrend es bei ,,dichten`` Epithelien umgekehrt ist (z.B. distales Nephron, Dickdarm). Man unterscheidet passiven und aktiven Transport. Passiver Transport wird durch Gradienten getrieben (Filtration durch hydrostatischen Druck; Diffusion durch Konzentrations- und Spannungsgradienten) und verl\"auft stets ,,bergab``. Aktiver Transport kann gegen diese Gradienten ,,bergauf`` erfolgen. Prim\"ar aktiver Transport erfolgt durch Pumpen. Sekund\"ar aktiver Transport wird durch Ionengradienten angetrieben, die durch Pumpen aufgebaut wurden. Hierbei wird durch Ausnutzung eines ,,bergab`` f\"uhrenden Ionengradienten eine andere Substanz ,,bergauf`` transportiert. Terti\"ar aktiver Transport wird durch sekund\"ar aktiven Transport angetrieben.},
  isbn = {978-3-662-56468-4}
}

@incollection{FS90,
  title = {Einf\"uhrung},
  booktitle = {Expertensysteme},
  author = {Friedrich, Gerhard and Stumptner, Markus},
  editor = {Gottlob, Georg and Fr{\"u}hwirth, Thomas and Horn, Werner},
  year = {1990},
  pages = {1--19},
  publisher = {{Springer Vienna}},
  address = {{Vienna}},
  doi = {10.1007/978-3-7091-9094-4_1},
  abstract = {Die Konstruktion von Expertensystemen hat sich zu einem Hauptanwendungsgebiet der Artificial Intelligence (AI) entwickelt. Ausgehend von den Zielen der AI werden im folgenden die Methoden dieser Wissenschaft skizziert. Nach einem kurzen \"Uberblick \"uber die Anwendungsgebiete werden Einsatzgebiete und grundlegende Konstruktionsprinzipien von Expertensystemen vorgestellt.},
  isbn = {978-3-7091-9094-4}
}

@article{Fuk75,
  title = {Cognitron: {{A}} Self-Organizing Multilayered Neural Network},
  author = {Fukushima, Kunihiko},
  year = {1975},
  month = sep,
  journal = {Biological Cybernetics},
  volume = {20},
  number = {3},
  pages = {121--136},
  issn = {1432-0770},
  doi = {10.1007/BF00342633},
  abstract = {A new hypothesis for the organization of synapses between neurons is proposed: ``The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y''. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named ``cognitron'', is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a ``teacher'' which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.}
}

@article{Fuk80,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  author = {Fukushima, Kunihiko},
  year = {1980},
  month = apr,
  journal = {Biological Cybernetics},
  volume = {36},
  number = {4},
  pages = {193--202},
  issn = {1432-0770},
  doi = {10.1007/BF00344251},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.}
}

@article{Fuk80a,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  author = {Fukushima, Kunihiko},
  year = {1980},
  month = apr,
  journal = {Biological Cybernetics},
  volume = {36},
  number = {4},
  pages = {193--202},
  issn = {1432-0770},
  doi = {10.1007/BF00344251},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.}
}

@article{Gar19,
  title = {Artificial {{Intelligence}} and {{Japan}}'s {{Fifth Generation}}: {{The Information Society}}, {{Neoliberalism}}, and {{Alternative Modernities}}},
  author = {Garvey, Colin},
  year = {2019},
  month = nov,
  journal = {Pacific Historical Review},
  volume = {88},
  number = {4},
  pages = {619--658},
  issn = {0030-8684},
  doi = {10.1525/phr.2019.88.4.619},
  abstract = {In 1982, Japan launched its Fifth Generation Computer Systems project (FGCS), designed to develop intelligent software that would run on novel computer hardware. As the first national, large-scale artificial intelligence (AI) research and development (R\&amp;D) project to be free from military influence and corporate profit motives, the FGCS was open, international, and oriented around public goods. Although the FGCS did not plan any commercialized technologies, many American computer experts portrayed it as an economic threat to U.S. dominance in computing and the global economy\textemdash and policymakers around the developed world believed them and funded AI projects of their own. Later, however, the FGCS was remembered as a failure. Why? This article recasts the FGCS as an interstice in the shift from a state-funded regime of American science organization to the neoliberal privatized regime of R\&amp;D now ascendant around the world. By exploring how notions of economic competitiveness and national security shaped R\&amp;D, this article reveals AI to be a product of contingent choices by multiple actors\textemdash nation-states, government bureaucracies, corporations, and individuals\textemdash rather than the outcome of deterministic technological forces.}
}

@book{GBC17,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2017},
  publisher = {{MIT Press}}
}

@article{HamalainenParadoxicalheatsensationsmoderatecoolingskin1982,
  title = {Paradoxical Heat Sensations during Moderate Cooling of the Skin},
  author = {H{\"a}m{\"a}l{\"a}inen, H. and Vartiainen, M. and Karvanen, L. and J{\"a}rvilehto, T.},
  year = {1982},
  journal = {Brain Research},
  volume = {251},
  number = {1},
  pages = {77--81},
  issn = {0006-8993},
  doi = {10.1016/0006-8993(82)91275-6},
  abstract = {Paradoxical heat sensations during cooling of the skin were examined in two experiments. In Expt. I the number of occurrences of sensation was studied in 19 naive test subjects (Ss) when cooling from thermal indifference both without and with preceding heating. Without preceding heating 13 Ss reported sensations of paradoxical heat (9.8\% of all stimulations). Preheating markedly facilitated the occurrence of the sensations (35\% of all stimulations). In Expt. II the effects of cooling velocity (velocities 0.4, 0.7 and 2.0 \textdegree C/s) and the type of skin area stimulated (hairy or glabrous skin of the hand) on the thresholds of paradoxical sensations were studied in 4 Ss without and with preheating. Cooling velocity, type of skin area and preheatting had significant effects on the sensation thresholds, the thresholds being the higher (i.e. the sensation appearing at lower stimulation temperatures) the higher the cooling velocity, if the stimuli were applied to the glabrous skin, or if no preheating was used. The results confirm the existence of paradoxical heat sensations during cooling of the skin and suggest that thesensation is mediated by polymodal units supplied by C-fibers.},
  keywords = {man,paradoxical sensation,thermoreception}
}

@article{Heb,
  title = {The {{Hebb Legacy}}},
  author = {Hebb, Donald Olding},
  doi = {10.1037/h0087295},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\DPQSMVIW\Hebb - RAYMOND M. KLEIN, Dalhousie University.pdf}
}

@incollection{Heb1988,
  title = {The {{Organization}} of {{Behavior}}},
  booktitle = {Neurocomputing: {{Foundations}} of {{Research}}},
  author = {Hebb, Donald O.},
  year = {1988},
  pages = {43--54},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {0-262-01097-6}
}

@book{Heb49,
  title = {The Organization of Behavior},
  author = {Hebb, D. O.},
  year = {1949},
  publisher = {{Wiley}},
  address = {{New York}}
}

@incollection{Heb88,
  title = {The {{Organization}} of {{Behavior}}},
  booktitle = {Neurocomputing: {{Foundations}} of {{Research}}},
  author = {Hebb, Donald O.},
  year = {1988},
  pages = {43--54},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {0-262-01097-6}
}

@incollection{Heb88a,
  title = {The {{Organization}} of {{Behavior}}},
  booktitle = {Neurocomputing: {{Foundations}} of {{Research}}},
  author = {Hebb, Donald O.},
  year = {1988},
  pages = {43--54},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {0-262-01097-6}
}

@article{HG03,
  title = {{Anwendungsbeispiele neuronaler Netze in verschiedenen Bereichen der Medizin}},
  author = {Hitzl, W. and Grabner, G.},
  year = {2003},
  month = jun,
  journal = {Spektrum der Augenheilkunde},
  volume = {17},
  number = {3},
  pages = {103--106},
  issn = {0930-4282, 1613-7523},
  doi = {10.1007/BF03163128},
  urldate = {2023-09-07},
  abstract = {Examples of neural networks in different areas in medicine Summary. In daily clinical practice, the ophthalmologist is permanently expected to give statements about the prognosis of a disease as well as the optimal course of therapy. In order to do so, the eye specialist combines his expert knowledge as well as historical information to provide the most accurate prognosis possible. Neural networks can be trained and optimized based on already known courses of the disease that are based on large samples of patients, and to combine in a similar way recent as well as historical data. It is the objective of this manuscript to give a short introduction of this concept, a brief summary of the historical development, an illustration of neural networks as they are applied in different areas in medicine and to discuss important features of neural network models. Examples of published studies within ophthalmology (rates for detection of vessels, identification of patients with diabetic retinopathy with neural nets), internal medicine (prognosis of pulmonary embolism; prognosis whether a patient can successfully be weaned from respiratory support), urology (prostatic carcinoma) and neural nets for early melanoma diagnosis are mentioned. Important advantages and disadvantages of expert systems based on neural networks will be discussed and users are cautioned for possible misuse of such models.},
  langid = {ngerman},
  file = {C:\Users\thorstensuckow\Zotero\storage\9BVDU7KP\Hitzl und Grabner - 2003 - Anwendungsbeispiele neuronaler Netze in verschiede.pdf}
}

@incollection{HI97,
  title = {Introduction (Aus: {{Weakly}} Connected Neural Networks)},
  booktitle = {Weakly {{Connected Neural Networks}}},
  author = {Hoppensteadt, Frank C. and Izhikevich, Eugene M.},
  year = {1997},
  pages = {3--24},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-1828-9_1},
  abstract = {In this chapter we give definitions and explanations of basic neurophysio\-logical terminology that we use in the book. We do not intend to provide a comprehensive background on various topics.},
  isbn = {978-1-4612-1828-9}
}

@inproceedings{HI97a,
  title = {Weakly Connected Neural Networks},
  author = {Hoppensteadt, Frank C. and Izhikevich, Eugene M.},
  year = {1997}
}

@article{HLW16,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},
  year = {2016},
  journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {2261--2269}
}

@article{Hop82,
  title = {Neural {{Networks}} and {{Physical Systems}} with {{Emergent Collective Computational Abilities}}},
  author = {Hopfield, John},
  year = {1982},
  month = may,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {79},
  pages = {2554--8},
  doi = {10.1073/pnas.79.8.2554}
}

@incollection{Hor90,
  title = {Anwendungen von {{Expertensystemen}}},
  booktitle = {Expertensysteme},
  author = {Horn, Werner},
  editor = {Gottlob, Georg and Fr{\"u}hwirth, Thomas and Horn, Werner},
  year = {1990},
  pages = {61--72},
  publisher = {{Springer Vienna}},
  address = {{Vienna}},
  doi = {10.1007/978-3-7091-9094-4_3},
  abstract = {Beginnend mit etwa dem Jahr 1980 kam es zu einer verst\"arkten Anwendung der bis dahin entwickelten Expertensystemtechnologie im Bereich der Wirtschaft. Vier Ursachen sind vordergr\"undig daf\"ur ma\ss geblich:Die zu diesem Zeitpunkt bekanntesten Expertensysteme hatten demonstriert, da\ss{} die Expertensystemtechnologie insbesondere in Bereichen mit vagem Wissen erfolgreich eingesetzt werden kann. Sie waren f\"ahig Leistungen zu erbringen, die der menschlicher Experten vergleichbar waren. Weiters hatten sie eine M\"achtigkeit erreicht, die aufzeigte, da\ss{} komplexe Realprobleme mit derartigen Systemen bew\"altigbar sind.Es wurden in den USA haupts\"achlich von Mitarbeitern in Artificial Intelligence (AI)-Forschungslaboratorien eine Reihe von AI-Firmen gegr\"undet, die sich um die kommerzielle Umsetzung der bisher gewonnenen Forschungsergebnisse bem\"uhten.Diese Firmen entwickelten eine Reihe von Werkzeugen zur Unterst\"utzung des Aufbaues von Expertensystemen (Expertensystem-Shells). Vor allem wurden Werkzeuge, die in Forschungslabors aufgrund des Fehlens umfangreicher Dokumentation nur einer eingeschr\"ankten Personengruppe zug\"anglich waren, erweitert und kommerziell verf\"ugbar gemacht.Die Hardwareentwicklung war soweit gediehen, da\ss{} eine Anwendung wissensintensiver Systeme mit entsprechend gro\ss em Bedarf an Hauptspeicher auch auf kleineren Rechnern m\"oglich wurde.},
  isbn = {978-3-7091-9094-4}
}

@incollection{HS19b,
  title = {Neurotransmitter Und Ihre {{Rezeptoren}}},
  booktitle = {Physiologie Des {{Menschen}}: Mit {{Pathophysiologie}}},
  author = {Hallermann, S. and Schmidt, R. F.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {105--114},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_10},
  abstract = {Die chemische \"Ubertragung an Synapsen beginnt mit der Freisetzung unterschiedlicher \"Ubertr\"agerstoffe, die als Transmitter bezeichnet werden. Klassische Transmitter sind Acetylcholin, die Aminos\"aure Glutamat und die Monoamine Noradrenalin und Gamma-Aminobutters\"aure (GABA). Damit es nach Transmitterfreisetzung nicht zur dauerhaften synaptischen \"Ubertragung kommt, muss der freigesetzte Transmitter wieder aus dem synaptischen Spalt entfernt werden. Dies geschieht, je nach Transmitter, durch aktive und passive Prozesse. So wird Acetylcholin durch die Cholinesterase gespalten, w\"ahrend im ZNS Glutamat aus dem synaptischen Spalt diffundiert und von Gliazellen aufgenommen wird. F\"ur die Wirkung des Transmitters sind die Typen postsynaptischer Rezeptoren entscheidend. So wirkt Acetylcholin erregend an der motorischen Endplatte, aber hemmend an den Schrittmacherzellen des Herzens. Die synaptische \"Ubertragung kann durch Molek\"ule moduliert werden, die die Synapse verst\"arken (Agonisten) oder abschw\"achen (Antagonisten).},
  isbn = {978-3-662-56468-4}
}

@incollection{HS19a,
  title = {Arbeitsweise von {{Synapsen}}},
  booktitle = {Physiologie Des {{Menschen}}: Mit {{Pathophysiologie}}},
  author = {Hallermann, S. and Schmidt, R. F.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {95--104},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_9},
  abstract = {Nervenzellen k\"onnen \"uber chemische oder elektrische Synapsen kommunizieren. Bei der chemischen Synapse wird ein \"Ubertr\"agerstoff (Transmitter) ausgesch\"uttet, der die nachgeschaltete Zelle beeinflusst. Bei der elektrischen Synapse flie\ss en Ionen durch kleine Poren in der Membran direkt von einer zur anderen Zelle. Im ZNS des Menschen spielen die elektrischen Synapsen eine untergeordnete Rolle. An chemischen Synapsen werden Transmitter in Bl\"aschen aus Doppellipidmembranen (synaptischen Vesikeln) angereichert. Durch die Fusion der Vesikel mit der pr\"asynaptischen Plasmamembran (Exozytose) werden dieTransmitter in den synaptischen Spalt freigesetzt. Die Transmitter diffundieren durch den synaptischen Spalt und binden an postsynaptische Rezeptoren, deren Aktivierung Ionenstr\"ome hervorrufen. Ob die postsynaptische Zelle erregt oder gehemmt wird, h\"angt von der Ionenleitf\"ahigkeit der Rezeptoren ab. Nervenzellen k\"onnen synaptische Signale von nur einer bis hin zu Hundertausenden anderen Nervenzellen erhalten. Hierbei kommt es zu einer r\"aumlichen und zeitlichen Summation der erregenden und hemmenden postsynaptischen Str\"ome.},
  isbn = {978-3-662-56468-4}
}

@incollection{HS19c,
  title = {Synaptische {{Plastizit\"at}}},
  booktitle = {Physiologie Des {{Menschen}}: Mit {{Pathophysiologie}}},
  author = {Hallermann, S. and Schmidt, R. F.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {115--120},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_11},
  abstract = {Im Rahmen von Adaptation, Lernen und Entwicklung werden Synapsen moduliert. Neben der Neubildung und Elimination von Synapsen kann sich die St\"arke der synaptischen \"Ubertragung auf unterschiedlichen Zeitskalen ver\"andern (Millisekunden \textendash{} Tage), was als synaptische Plastizit\"at bezeichnet wird. Bei einer Verst\"arkung der synaptischen \"Ubertragung spricht man von synaptischer Potenzierung und bei einer Abschw\"achung von synaptischer Depression. Dauert die Ver\"anderung der \"Ubertragungsst\"arke weniger als etwa eine Minute spricht man von Kurzzeitplastizit\"at, andernfalls von Langzeitplastizit\"at. Eine Vielzahl pr\"a- und postsynaptischer Mechanismen ist an der Entstehung verschiedener Formen der synaptischen Plastizit\"at beteiligt, wobei die intrazellul\"are Ca2+-Konzentration meist eine zentrale Rolle spielt.},
  isbn = {978-3-662-56468-4}
}

@inproceedings{HS83,
  title = {Optimal Perceptual Inference},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hinton, Geoffrey and Sejnowski, Terrence},
  year = {1983},
  month = jan,
  pages = {448--453}
}

@article{HW62,
  title = {Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  year = {1962},
  month = jan,
  journal = {The Journal of Physiology},
  volume = {160},
  number = {1},
  pages = {106--154},
  issn = {00223751},
  doi = {10.1113/jphysiol.1962.sp006837},
  urldate = {2023-09-05},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\ZAADPTMW\Hubel und Wiesel - 1962 - Receptive fields, binocular interaction and functi.pdf}
}

@article{HW62a,
  title = {Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  year = {1962},
  month = jan,
  journal = {The Journal of Physiology},
  volume = {160},
  number = {1},
  pages = {106--154},
  issn = {00223751},
  doi = {10.1113/jphysiol.1962.sp006837},
  urldate = {2023-09-05},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\YSA5ZILM\Hubel und Wiesel - 1962 - Receptive fields, binocular interaction and functi.pdf}
}

@incollection{HWN22,
  title = {Neuronale {{Netze}} Zur {{Effizienzsteigerung}} Der {{Texterkennung}} in Der {{Rezeptabrechnung}}},
  booktitle = {K\"unstliche {{Intelligenz}} Im {{Gesundheitswesen}}: {{Entwicklungen}}, {{Beispiele}} Und {{Perspektiven}}},
  author = {H{\"o}fer, Tobias and Weish{\"a}upl, Frederik and Nischwitz, Alfred},
  editor = {Pfannstiel, Mario A.},
  year = {2022},
  pages = {697--714},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-33597-7_33},
  abstract = {Digitale Bilder sind einer Vielzahl von St\"orungen unterworfen, die eine erfolgreiche Texterkennung erschweren. Der digitale Zwilling enth\"alt unbeabsichtigte Unterschiede zu der realen Bildquelle, die w\"ahrend der Erfassung, Verarbeitung, Komprimierung, Speicherung oder \"Ubertragung entstehen k\"onnen. Als Beispiele f\"ur relevante St\"orfaktoren lassen sich unterbrochene und d\"unne Zeichen, St\"orpixel und Text\"uberlagerungen anf\"uhren. Diese St\"orfaktoren erschweren die automatisierte Erfassung relevanter Daten und machen diese im schlimmsten Fall unbrauchbar. F\"ur die Apothekenrechenzentren bedeutet das einen zus\"atzlichen manuellen Aufwand und damit verbundene Kosten. Dieser Beitrag beschreibt die Entwicklung und Produktivnahme eines neuronalen Netzes bei der NOVENTI, das erfolgreich die St\"orfaktoren des digitalisierten Rezeptbilds reduziert und dadurch eine Effizienzsteigerung der Texterkennung bewirkt.},
  isbn = {978-3-658-33597-7}
}

@misc{IRK+19,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  number = {arXiv:1901.07031},
  eprint = {1901.07031},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-09-09},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C:\Users\thorstensuckow\Zotero\storage\NAVGG5JJ\Irvin et al. - 2019 - CheXpert A Large Chest Radiograph Dataset with Un.pdf}
}

@article{JEP+21,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1\textendash 4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence\textemdash the structure prediction component of the `protein folding problem'8\textemdash has been an important open research problem for more than 50~years9. Despite recent progress10\textendash 14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  file = {C:\Users\thorstensuckow\Zotero\storage\CIPAVWW2\Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf}
}

@book{Jon07,
  title = {Artificial {{Intelligence}}: {{A Systems Approach}}},
  author = {Jones, M. Tim},
  year = {2007},
  edition = {1},
  publisher = {{Infinity Science Press}},
  abstract = {This book offers students and AI programmers a new perspective on the study of artificial intelligence concepts. The essential topics and theory of AI are presented, but it also includes practical information on data input \& reduction as well as data output (i.e., algorithm usage). Because traditional AI concepts such as pattern recognition, numerical optimization and data mining are now simply types of algorithms, a different approach is needed. This sensor / algorithm / effecter approach grounds the algorithms with an environment, helps students and AI practitioners to better understand them, and subsequently, how to apply them. The book has numerous up to date applications in game programming, intelligent agents, neural networks, artificial immune systems, and more. A CD-ROM with simulations, code, and figures accompanies the book. *Features *Covers not only AI theory, but modern applications e.g., game programming, machine learning, swarming, artificial immune systems, genetic algorithms, pattern recognition, numerical optimization, data mining, and more *Discusses the various computer languages of AI from LISP to JAVA and Python *Includes a CD-ROM with 100MB of simulations, code, and fi gures *Table of Contents 1. Introduction. 2. Search. 3. Games. 4. Logic. 5. Planning. 6. Knowledge Representation. 7. Machine Learning. 8. Probabilistic Reasoning. 9. Stochastic Search. 10. Neural Networks. 11. Intelligent Agents. 12. Hybrid Models. 13. Languages of AI.},
  isbn = {0-9778582-3-5}
}

@incollection{Flo19,
  title = {Lernen},
  booktitle = {Physiologie Des Menschen: Mit Pathophysiologie},
  author = {Flor, H.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {827--838},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_66},
  abstract = {Lernen bezeichnet die F\"ahigkeit des Organismus sich durch Erfahrung an die Umwelt anzupassen. Neben nichtassoziativen Lernprozessen spielen das assoziative Lernen und kognitive Prozesse eine wichtige Rolle. Lernen interagiert mit Reifungsprozessen und f\"uhrt zu lebenslanger Plastizit\"at. Neurobiologische Grundlage des Lernens ist die synaptische Plastizit\"at, die bei langanhaltenden Ged\"achtnisprozessen in strukturelle Ver\"anderungen \"ubergeht. Lernprozesse spielen bei psychischen St\"orungen eine wichtige Rolle, modulieren aber auch somatische Erkrankungen und sind besonders wichtig in der Neurorehabilitation.},
  isbn = {978-3-662-56468-4}
}


@incollection{Jon19,
  title = {Aktionspotenzial: {{Fortleitung}} Im {{Axon}}},
  booktitle = {Physiologie Des {{Menschen}}: Mit {{Pathophysiologie}}},
  author = {Jonas, P.},
  editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
  year = {2019},
  pages = {72--82},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-56468-4_7},
  abstract = {Neurone empfangen Eingangssignale, konvertieren diese in Aktionspotenziale und generieren schlie\ss lich Ausgangssignale auf ihren Zielzellen. Dabei sind die zu \"uberwindenden r\"aumlichen Distanzen oft gro\ss. Daher ist entscheidend, dass elektrische Signale in Nervenzellen schnell von einem zum anderen Ort geleitet werden k\"onnen. Diese wichtige Aufgabe erf\"ullt das Axon, der ,,Ausgangsfortsatz`` der Nervenzelle. F\"ur die schnelle Leitung des Aktionspotenzials sind sowohl die passiven Eigenschaften des axonalen Kabels als auch die aktiven Eigenschaften der Zellmembran von entscheidender Bedeutung. Die Evolution bedient sich zweier Tricks, um die Leitungsgeschwindigkeit des Aktionspotenzials zu maximieren. Der eine Trick ist die Zunahme des Axondurchmessers. Der andere Trick ist die Ausbildung von Markscheiden. Dies f\"uhrt bei nahezu gleichem Platzbedarf zu einer Zunahme der Leistungsgeschwindigkeit um fast zwei Gr\"o\ss enordnungen. Die Aktionspotenzialleitung an myelinisierten Axonen erfolgt ,,saltatorisch``.},
  isbn = {978-3-662-56468-4}
}

@article{Jon99,
  title = {Golgi, {{Cajal}} and the {{Neuron Doctrine}}},
  author = {Jones, Edward G.},
  year = {1999},
  journal = {Journal of the History of the Neurosciences},
  volume = {8},
  number = {2},
  pages = {170--178},
  publisher = {{Routledge}},
  doi = {10.1076/jhin.8.2.170.1838},
  pmid = {11624298}
}

@misc{KB17,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-06},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\thorstensuckow\Zotero\storage\7XMPD9G4\Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf}
}

@article{KG14,
  title = {Hebbian Learning and Predictive Mirror Neurons for Actions, Sensations and Emotions},
  author = {Keysers, Christian and Gazzola, Valeria},
  year = {2014},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1644},
  pages = {20130175},
  doi = {10.1098/rstb.2013.0175},
  abstract = {Spike-timing-dependent plasticity is considered the neurophysiological basis of Hebbian learning and has been shown to be sensitive to both contingency and contiguity between pre- and postsynaptic activity. Here, we will examine how applying this Hebbian learning rule to a system of interconnected neurons in the presence of direct or indirect re-afference (e.g. seeing/hearing one's own actions) predicts the emergence of mirror neurons with predictive properties. In this framework, we analyse how mirror neurons become a dynamic system that performs active inferences about the actions of others and allows joint actions despite sensorimotor delays. We explore how this system performs a projection of the self onto others, with egocentric biases to contribute to mind-reading. Finally, we argue that Hebbian learning predicts mirror-like neurons for sensations and emotions and review evidence for the presence of such vicarious activations outside the motor system.}
}

@article{Kle99,
  title = {The {{Hebb}} Legacy.},
  author = {Klein, Raymond M.},
  year = {1999},
  journal = {Canadian Journal of Experimental Psychology / Revue canadienne de psychologie exp\'erimentale},
  volume = {53},
  number = {1},
  pages = {1--3},
  publisher = {{Canadian Psychological Association}},
  address = {{Canada}},
  issn = {1878-7290(Electronic),1196-1961(Print)},
  doi = {10.1037/h0087295},
  abstract = {Discusses the influence of Donald Olding Hebb (1904\textendash 1985) on the discipline of psychology. The author notes that Hebb's principled opposition to radical behaviourism and emphasis on understanding what goes on between stimulus and response (perception, learning, thinking) helped clear the way for the cognitive revolution. His view of psychology as a biological science and his neuropsychological cell-assembly proposal rejuvenated interest in physiological psychology. Since his death, Hebb's seminal ideas exert an ever-growing influence on those interested in mind (cognitive science), brain (neuroscience), and how brains implement mind (cognitive neuroscience). Specific events in Hebb's career are outlined, with particular attention to the influence on psychology of his book The Organization of Behavior: A Neuropsychological Theory (1949). (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {*History of Psychology,*Neuropsychology,Theories}
}

@incollection{Koh90,
  title = {Aus Den {{Anf\"angen Neuraler Netze}}},
  booktitle = {Neurale {{Netze}}},
  author = {K{\"o}hle, Monika},
  year = {1990},
  pages = {16--34},
  publisher = {{Springer Vienna}},
  address = {{Vienna}},
  doi = {10.1007/978-3-7091-9093-7_2},
  abstract = {Theoretische Erkl\"arungen von Gehirn und Denkprozessen wurden schon von den alten Griechen, wie Plato (427\textendash 347 v. Chr.) und Aristoteles (384\textendash 322 v. Chr.), versucht. Die sogenannten kybernetischen Maschinen, zu denen die ``neuralen Computer'' geh\"oren, zeigen eine weit l\"angere Geschichte als allgemein angenommen.},
  isbn = {978-3-7091-9093-7},
  file = {C:\Users\thorstensuckow\Zotero\storage\Z3L4XX96\Köhle - 1990 - Aus den Anfängen Neuraler Netze.pdf}
}

@article{KSEB21,
  title = {Kreislauftherapie Bei {{Sepsis}} \textendash{} Wann, Wie Und Wie Viel?},
  author = {Kochanek, Matthias and {Shimabukuro-Vornhagen}, Alexander and Eichenauer, Dennis A. and B{\"o}ll, Boris},
  year = {2021},
  month = feb,
  journal = {Wiener klinisches Magazin},
  volume = {24},
  number = {1},
  pages = {12--17},
  issn = {1613-7817},
  doi = {10.1007/s00740-020-00376-8},
  abstract = {Das Management der h\"amodynamischen Instabilit\"at im Rahmen einer Sepsis bzw. eines septischen Schocks steht in der Notfallversorgung und auf der Intensivstation ganz im Vordergrund. Kreislaufinstabilit\"at hat einen dramatischen Einfluss auf die Rate an Organkomplikationen und die Mortalit\"at bei Sepsis. Nach der Leitlinie zur Therapie der Sepsis soll ein mittlerer arterieller Druck von 65\,mm\,Hg nicht unterschritten werden. Kristalloide balancierte Fl\"ussigkeit und Katecholamine sind die Eckpfeiler des therapeutischen Managements der septischen Kreislaufinstabilit\"at. In diesem Beitrag sollen die wichtigsten Punkte \textendash{} das Was, Wann und Wieviel \textendash{} der Kreislauftherapie pr\"asentiert und kritisch diskutiert werden.}
}

@inproceedings{KSH12,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}}
}

@article{KUB30,
  title = {A {{THEORETICAL APPLICATION TO SOME NEUROLOGICAL PROBLEMS OF THE PROPERTIES OF EXCITATION WAVES WHICH MOVE IN CLOSED CIRCUITS}}},
  author = {KUBIE, LAWRENCE S.},
  year = {1930},
  month = jul,
  journal = {Brain},
  volume = {53},
  number = {2},
  pages = {166--177},
  issn = {0006-8950},
  doi = {10.1093/brain/53.2.166}
}

@incollection{Kup19,
  title = {Einleitung Und {{Lernziele}}},
  booktitle = {Eine Transdisziplin\"are {{Einf\"uhrung}} in Die {{Welt}} Der {{Kybernetik}}: {{Grundlagen}}, {{Modelle}}, {{Theorien}} Und {{Praxisbeispiele}}},
  author = {K{\"u}ppers, E. W. Udo},
  year = {2019},
  pages = {1--3},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-23725-7_1},
  abstract = {Dieses Lehrbuch \"uber ,,Kybernetische Welten`` k\"onnte auch \"uberschrieben werden mit: ,,Die Macht der negativen R\"uckkopplung``.},
  isbn = {978-3-658-23725-7}
}

@article{Lan09,
  title = {Associative Memory Models: From the Cell-Assembly Theory to Biophysically Detailed Cortex Simulations},
  author = {Lansner, Anders},
  year = {2009},
  journal = {Trends in Neurosciences},
  volume = {32},
  number = {3},
  pages = {178--186},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2008.12.002},
  abstract = {The second half of the past century saw the emergence of a theory of cortical associative memory function originating in Donald Hebb's hypotheses on activity-dependent synaptic plasticity and cell-assembly formation and dynamics. This conceptual framework has today developed into a theory of attractor memory that brings together many experimental observations from different sources and levels of investigation into computational models displaying information-processing capabilities such as efficient associative memory and holistic perception. Here, we outline a development that might eventually lead to a neurobiologically grounded theory of cortical associative memory.}
}

@incollection{Lan22,
  title = {Abrechnung Medizinischer {{Leistungen}} Mit K\"unstlicher {{Intelligenz}}},
  booktitle = {K\"unstliche {{Intelligenz}} Im {{Gesundheitswesen}}: {{Entwicklungen}}, {{Beispiele}} Und {{Perspektiven}}},
  author = {Landgrebe, Jobst},
  editor = {Pfannstiel, Mario A.},
  year = {2022},
  pages = {715--726},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-33597-7_34},
  abstract = {Deutschland verausgabt im Gesundheitssystem hunderte von Milliarden pro Jahr. Die Leistungserbringer m\"ussen diese Leistungen den Leistungstr\"agern und Selbstzahlern gegen\"uber abrechnen, um bezahlt zu werden. Dieser Prozess erfolgt heute manuell durch \"Arzte, durch spezialisierte Berufsgruppen (Medizincontroller) oder wird an Dienstleister vergeben. Dieser Beitrag stellt das Potenzial dar, welches sich aus einer automatisierten Abrechnung sowohl der \"arztlichen Leistungen wie der Unterbringungsleistungen ergeben w\"urde, die zusammen \"uber zwei Drittel der Kosten ausmachen. Das Problem wird geschildert, danach werden verschiedene L\"osungsm\"oglichkeiten und deren Potenziale er\"ortert. Es wird gezeigt, dass die gro\ss e Herausforderung in der korrekten Maschineninterpretation der \"arztlichen Sprache und der Verarbeitung von Daten in schematischer, tabellarischer oder handschriftlicher Form besteht.},
  isbn = {978-3-658-33597-7}
}

@article{LBBH98,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  doi = {10.1109/5.726791}
}

@article{LBBH98a,
  title = {Gradient-{{Based Learning Applied}} to {{Document Recognition}}},
  author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
  year = {1998},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\3GHZL8WX\LeCun et al. - 1998 - Gradient-Based Learning Applied to Document Recogn.pdf}
}

@article{LBD+89,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.}
}

@incollection{LBDA22,
  title = {Erkl\"arbare {{KI}} in Der Medizinischen {{Diagnose}} \textendash{} {{Erfolge}} Und {{Herausforderungen}}},
  booktitle = {K\"unstliche {{Intelligenz}} Im {{Gesundheitswesen}}: {{Entwicklungen}}, {{Beispiele}} Und {{Perspektiven}}},
  author = {Lucieri, Adriano and Bajwa, Muhammad Naseer and Dengel, Andreas and Ahmed, Sheraz},
  editor = {Pfannstiel, Mario A.},
  year = {2022},
  pages = {727--754},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-33597-7_35},
  abstract = {Der gro\ss e Erfolg moderner, bildbasierter KI-Methoden und das damit einhergehende Interesse f\"ur die Anwendung von KI in kritischen Entscheidungsprozessen f\"uhrte zu einem Anstieg der Bem\"uhungen, intelligente Systeme transparent und erkl\"arbar zu gestalten. Besonders im medizinischen Kontext, wo computergest\"utzte Entscheidungen direkten Einfluss auf die Behandlung und das Wohlsein von Patienten haben k\"onnen, ist Transparenz f\"ur den sicheren \"Ubergang von Forschung in die Praxis von h\"ochster Wichtigkeit. Dieser Beitrag besch\"aftigt sich mit dem aktuellen Stand moderner Methoden zur Erkl\"arung und Interpretation von Deep-Learning-basierten KI-Algorithmen in Anwendungen der medizinischen Forschung und Diagnose von Krankheiten. Zun\"achst werden erste bemerkenswerte Erfolge im Einsatz erkl\"arbarer KI zur Validierung bekannter und Exploration potenzieller Biomarker sowie Methoden zur nachtr\"aglichen Korrektur von KI-Modellen aufgezeigt. Im Anschluss werden einige verbleibende Herausforderungen, die der Anwendung von KI als klinische Entscheidungshilfe im Weg stehen, kritisch diskutiert und Empfehlungen f\"ur die Ausrichtung zuk\"unftiger Forschung ausgesprochen.},
  isbn = {978-3-658-33597-7}
}

@article{LBH15,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.}
}

@inproceedings{LeC89,
  title = {Generalization and {{Network Design Strategies}}},
  booktitle = {Connectionism in {{Perspective}}},
  author = {LeCun, Y.},
  editor = {Pfeifer, R. and Schreter, Z. and Fogelman, F. and Steels, L.},
  year = {1989},
  publisher = {{Elsevier}},
  address = {{Zurich, Switzerland}}
}

@article{LHL+,
  title = {G-{{Net}}: A {{Recurrent Network Approach}} to {{G-Computation}} for {{Counterfactual Prediction Under}} a {{Dynamic Treatment Regime}}},
  author = {Li, Rui and Hu, Stephanie and Lu, Mingyu and Utsumi, Yuria and Chakraborty, Prithwish and Sow, Daby M and Madan, Piyush and Ghalwash, Mohamed and Shahn, Zach and Lehman, Li-wei H},
  abstract = {Counterfactual prediction is a fundamental task in decision-making. This paper introduces G-Net, a sequential deep learning framework for counterfactual prediction under dynamic timevarying treatment strategies in complex longitudinal settings. G-Net is based upon gcomputation, a causal inference method for estimating effects of general dynamic treatment strategies. Past g-computation implementations have mostly been built using classical regression models. G-Net instead adopts a recurrent neural network framework to capture complex temporal and nonlinear dependencies in the data. To our knowledge, G-Net is the first g-computation based deep sequential modeling framework that provides estimates of treatment effects under dynamic and time-varying treatment strategies. We evaluate G-Net using simulated longitudinal data from two sources: CVSim, a mechanistic model of the cardiovascular system, and a pharmacokinetic simulation of tumor growth. G-Net outperforms both classical and state-of-the-art counterfactual prediction models in these settings.},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\ZQ6SSRVX\Li et al. - G-Net a Recurrent Network Approach to G-Computati.pdf}
}

@inproceedings{LHL+21,
  title = {G-{{Net}}: A {{Recurrent Network Approach}} to {{G-Computation}} for {{Counterfactual Prediction Under}} a {{Dynamic Treatment Regime}}},
  booktitle = {Proceedings of {{Machine Learning}} for {{Health}}},
  author = {Li, Rui and Hu, Stephanie and Lu, Mingyu and Utsumi, Yuria and Chakraborty, Prithwish and Sow, Daby M. and Madan, Piyush and Li, Jun and Ghalwash, Mohamed and Shahn, Zach and Lehman, Li-wei},
  editor = {Roy, Subhrajit and Pfohl, Stephen and Rocheteau, Emma and Tadesse, Girmaw Abebe and Oala, Luis and Falck, Fabian and Zhou, Yuyin and Shen, Liyue and Zamzmi, Ghada and Mugambi, Purity and Zirikly, Ayah and McDermott, Matthew B. A. and Alsentzer, Emily},
  year = {2021},
  month = dec,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {158},
  pages = {282--299},
  publisher = {{PMLR}},
  abstract = {Counterfactual prediction is a fundamental task in decision-making. This paper introduces G-Net, a sequential deep learning framework for counterfactual prediction under dynamic time-varying treatment strategies in complex longitudinal settings. G-Net is based upon g-computation, a causal inference method for estimating effects of general dynamic treatment strategies. Past g-computation implementations have mostly been built using classical regression models. G-Net instead adopts a recurrent neural network framework to capture complex temporal and nonlinear dependencies in the data. To our knowledge, G-Net is the first g-computation based deep sequential modeling framework that provides estimates of treatment effects under \textbackslash emdynamic and \textbackslash emtime-varying treatment strategies. We evaluate G-Net using simulated longitudinal data from two sources: CVSim, a mechanistic model of the cardiovascular system, and a pharmacokinetic simulation of tumor growth. G-Net outperforms both classical and state-of-the-art counterfactual prediction models in these settings.}
}

@article{Lip87,
  title = {An Introduction to Computing with Neural Nets},
  author = {Lippmann, R.},
  year = {1987},
  journal = {IEEE ASSP Magazine},
  volume = {4},
  number = {2},
  pages = {4--22},
  doi = {10.1109/MASSP.1987.1165576}
}

@article{Liu10,
  title = {The {{Cybernetic Unconscious}}: {{Rethinking Lacan}}, {{Poe}}, and {{French Theory}}},
  author = {Liu, Lydia H.},
  year = {2010},
  journal = {Critical Inquiry},
  volume = {36},
  number = {2},
  pages = {288--320},
  doi = {10.1086/648527}
}

@article{Lon96,
  title = {A {{Systematic Introduction}}},
  author = {London, Hong Kong},
  year = {1996},
  journal = {Neural Networks},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\AB4HFA8J\London - 1996 - A Systematic Introduction.pdf}
}

@article{LS92,
  title = {Selection of {{Intrinsic Horizontal Connections}} in the {{Visual Cortex}} by {{Correlated Neuronal Activity}}},
  author = {L{\"o}wel, Siegrid and Singer, Wolf},
  year = {1992},
  journal = {Science},
  volume = {255},
  number = {5041},
  pages = {209--212},
  doi = {10.1126/science.1372754},
  abstract = {In the visual cortex of the brain, long-ranging tangentially oriented axon collaterals interconnect regularly spaced clusters of cells. These connections develop after birth and attain their specificity by pruning. To test whether there is selective stabilization of connections between those cells that exhibit correlated activity, kittens were raised with artificially induced strabismus (eye deviation) to eliminate the correlation between signals from the two eyes. In area 17, cell clusters were driven almost exclusively from either the right or the left eye and tangential intracortical fibers preferentially connected cell groups activated by the same eye. Thus, circuit selection depends on visual experience, and the selection criterion is the correlation of activity.}
}

@article{LST+16,
  title = {Deep Learning as a Tool for Increased Accuracy and Efficiency of Histopathological Diagnosis},
  author = {Litjens, Geert and S{\'a}nchez, Clara I. and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and {Hulsbergen - van de Kaa}, Christina and Bult, Peter and {van Ginneken}, Bram and {van der Laak}, Jeroen},
  year = {2016},
  month = may,
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {26286},
  issn = {2045-2322},
  doi = {10.1038/srep26286},
  abstract = {Pathologists face a substantial increase in workload and complexity of histopathologic cancer diagnosis due to the advent of personalized medicine. Therefore, diagnostic protocols have to focus equally on efficiency and accuracy. In this paper we introduce `deep learning' as a technique to improve the objectivity and efficiency of histopathologic slide analysis. Through two examples, prostate cancer identification in biopsy specimens and breast cancer metastasis detection in sentinel lymph nodes, we show the potential of this new methodology to reduce the workload for pathologists, while at the same time increasing objectivity of diagnoses. We found that all slides containing prostate cancer and micro- and macro-metastases of breast cancer could be identified automatically while 30\textendash 40\% of the slides containing benign and normal tissue could be excluded without the use of any additional immunohistochemical markers or human intervention. We conclude that `deep learning' holds great promise to improve the efficacy of prostate cancer diagnosis and breast cancer staging.},
  file = {C:\Users\thorstensuckow\Zotero\storage\S7N348Y9\Litjens et al. - 2016 - Deep learning as a tool for increased accuracy and.pdf}
}

@incollection{Mas90,
  title = {{{McCulloch}}, {{Pitts}} and the {{Evolution}} of {{Wiener}}'s {{Neurophysiological Ideas}}},
  booktitle = {Norbert {{Wiener}} 1894\textendash 1964},
  author = {Masani, P. R.},
  year = {1990},
  pages = {218--238},
  publisher = {{Birkh\"auser Basel}},
  address = {{Basel}},
  doi = {10.1007/978-3-0348-9252-0_16},
  abstract = {Wiener first met Dr. Warren McCulloch at the neurophysiological meeting in New York in 1942 at which Dr. Rosenblueth presented their joint work with Bigelow on teleology [43b]. McCulloch was then Professor of Psychiatry in the Medical School of the University of Illinois. In 1917 he had entered Haverford College to honor in mathematics, but in the spring went on active duty with the Naval Reserve for a year, teaching celestial navigation to cadets and learning about submarines. He studied philosophy at Yale and psychology (experimental aesthetics) at Columbia before he entered the Columbia Medical School. He became a serious student of mathematical logic, and investigated the mathematico-logical aspects of schizophrenia and psychopathia while serving at the Rockland Hospital for the Insane. His life's mission is disclosed by his amusing exchange with the Quaker philosopher Rufus Jones at Haverford College in 1917:``Warren'', he said, ``what is thee going to be?'' And I said, ``I don't know.'' ``And what is thee going to do?'' And again I said ``I have no idea; but there is one question I would like to answer: What is a number, that a man may know it, and a man, that he may know a number?'' He smiled and said, ``Friend, thee will be busy as long as thee lives.'' I have been\textbackslash ldots \{M13, p. 2\}.},
  isbn = {978-3-0348-9252-0}
}

@book{McC04,
  title = {Machines {{Who Think}}: {{A Personal Inquiry}} into the {{History}} and {{Prospects}} of {{Artificial Intelligence}}},
  author = {McCorduck, Pamela},
  year = {2004},
  publisher = {{AK Peters Ltd}},
  isbn = {1-56881-205-1}
}

@book{McC1965,
  title = {Embodiments of Mind.},
  author = {McCulloch, Warren},
  year = {1965},
  series = {Embodiments of Mind.},
  pages = {xx, 402},
  publisher = {{The MIT Press}},
  address = {{Cambridge,  MA,  US}},
  abstract = {Lectures, poetry, and essays of the author reflecting the meeting of his several fields of study: psychiatry, neurophysiology, methematics, cybernetics, and experimental epistemology. Contents include: What Is a Number, That a Man May Know It, and a Man, That He May Know a Number?; A Logical Calculus of the Ideas Imminent in Nervous Activity; A Heterarchy of Values Determined by the Topology of Nervous Nets; How We Know Universals: The Perception of Auditory and Visual Forms; Modes of Functional Organization of the Cerebral Cortex; Why the Mind Is in the Head; Through the Den of the Metaphysician; Mysterium Iniquitatis of Sinful Man Aspiring into the Place of God; Effects of Strychnine with Special Reference to Spinal Afferent Fibres; Reflex Inhibition by Dorsal Root Interaction; Toward Some Circuitry of Ethical Robots or an Observational Science of the Genesis of Social Evaluation in the Mind-Like Behavior of Artifacts; Agatha Tyche: Of Nervous Nets\textemdash the Lucky Reckoners; Where Is Fancy Bred?; What the Frog's Eye Tells the Frog's Brain; Finality and Form; The Past of a Delusion; Machines That Think and Want; The Natural Fit; A Historical Introduction to the Postulation Foundations of Experimental Epistemology; Physiological Processes Underlying Psychoneuroses; What's in the Brain That Ink May Character? (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@incollection{McC51,
  title = {Why the Mind Is in the Head.},
  booktitle = {Cerebral Mechanisms in Behavior; the {{Hixon Symposium}}.},
  author = {McCulloch, Warren S.},
  year = {1951},
  pages = {42--111},
  publisher = {{Wiley}},
  address = {{Oxford,  England}},
  abstract = {An attempt to explain the neurological basis of "mind". The nervous system is considered to be a logical machine and some of the author's earlier work in this area is discussed. Ideas are treated in terms of information theory. The origin of ideas and memory are discussed in terms of nerve nets and negative feedback. The perserveration of perceived shape or form from one neural level to another is explained in terms of the sequential excitation of neural sheets. The mind is in the head because the brain contains the greatest possible neural connections (arranged in sheets) and the largest number of functional inverse feedbacks. A panel discussion is included between the pages 57-82. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{McC55,
  title = {Mysterium Iniquitatis of Sinful Man Aspiring into the Place of {{God}}},
  author = {McCulloch, Warren S.},
  year = {1955},
  journal = {The Scientific monthly},
  volume = {80},
  pages = {35--39}
}

@inproceedings{McC79,
  title = {Machines {{Who Think}}: {{A Personal Inquiry}} into the {{History}} and {{Prospects}} of {{Artificial Intelligence}}},
  author = {McCorduck, Pamela},
  year = {1979}
}

@article{McD,
  title = {{{RI}}: An {{Expert}} in the {{Computer Systems Domain}}'},
  author = {McDermott, John},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\74AGGP4B\McDermott - RI an Expert in the Computer Systems Domain’.pdf}
}

@article{MKM+16,
  title = {{{SMART}} on {{FHIR}}: A Standards-Based, Interoperable Apps Platform for Electronic Health Records},
  author = {Mandel, Joshua C and Kreda, David A and Mandl, Kenneth D and Kohane, Isaac S and Ramoni, Rachel B},
  year = {2016},
  month = feb,
  journal = {Journal of the American Medical Informatics Association},
  volume = {23},
  number = {5},
  pages = {899--908},
  issn = {1067-5027},
  doi = {10.1093/jamia/ocv189},
  abstract = {Objective In early 2010, Harvard Medical School and Boston Children's Hospital began an interoperability project with the distinctive goal of developing a platform to enable medical applications to be written once and run unmodified across different healthcare IT systems. The project was called Substitutable Medical Applications and Reusable Technologies (SMART). Methods We adopted contemporary web standards for application programming interface transport, authorization, and user interface, and standard medical terminologies for coded data. In our initial design, we created our own openly licensed clinical data models to enforce consistency and simplicity. During the second half of 2013, we updated SMART to take advantage of the clinical data models and the application-programming interface described in a new, openly licensed Health Level Seven draft standard called Fast Health Interoperability Resources (FHIR). Signaling our adoption of the emerging FHIR standard, we called the new platform SMART on FHIR. Results We introduced the SMART on FHIR platform with a demonstration that included several commercial healthcare IT vendors and app developers showcasing prototypes at the Health Information Management Systems Society conference in February 2014. This established the feasibility of SMART on FHIR, while highlighting the need for commonly accepted pragmatic constraints on the base FHIR specification. Conclusion In this paper, we describe the creation of SMART on FHIR, relate the experience of the vendors and developers who built SMART on FHIR prototypes, and discuss some challenges in going from early industry prototyping to industry-wide production use.}
}

@article{MMA+19,
  title = {Etymology and the Neuron(e)},
  author = {Mehta, Arpan R and Mehta, Puja R and Anderson, Stephen P and MacKinnon, Barbara L H and Compston, Alastair},
  year = {2019},
  month = dec,
  journal = {Brain},
  volume = {143},
  number = {1},
  pages = {374--379},
  issn = {0006-8950},
  doi = {10.1093/brain/awz367}
}

@book{MMM96,
  title = {Brain Processes, Theories, and Models: An International Conference in Honor of {{W}}.{{S}}. {{McCulloch}} 25 Years after His Death},
  shorttitle = {Brain Processes, Theories, and Models},
  editor = {{Moreno-D{\'i}az}, Roberto and Mira, J. and McCulloch, Warren S.},
  year = {1996},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-63170-9},
  langid = {english},
  lccn = {QP376 .B719 1996},
  keywords = {Brain,Congresses,Mathematical models Congresses,{McCulloch, Warren S},Neural networks (Neurobiology),Warren Sturgis},
  file = {C:\Users\thorstensuckow\Zotero\storage\Z9NY4A7Q\Moreno-Díaz et al. - 1996 - Brain processes, theories, and models an internat.pdf}
}

@article{Mur91,
  title = {Multilayer Perceptrons for Classification and Regression},
  author = {Murtagh, Fionn},
  year = {1991},
  journal = {Neurocomputing},
  volume = {2},
  number = {5},
  pages = {183--197},
  issn = {0925-2312},
  doi = {10.1016/0925-2312(91)90023-5},
  abstract = {We review the theory and practice of the multilayer perceptron. We aim at addressing a range of issues which are important from the point of view of applying this approach to practical problems. A number of examples are given, illustrating how the multilayer perceptron compares to alternative, conventional approaches. The application fields of classification and regression are especially considered. Questions of implementation, i.e. of multilayer perceptron architecture, dynamics, and related aspects, are discussed. Recent studies, which are particularly relevant to the areas of discriminant analysis, and function mapping, are cited.},
  keywords = {discriminant analysis,function approximation,Multilayer perceptron,regression,supervised classification}
}

@incollection{Neu56,
  title = {Probabilistic {{Logics}} and the {{Synthesis}} of {{Reliable Organisms From Unreliable Components}}},
  booktitle = {Automata {{Studies}}. ({{AM-34}}), {{Volume}} 34},
  author = {von Neumann, J.},
  editor = {Shannon, C. E. and McCarthy, J.},
  year = {1956},
  pages = {43--98},
  publisher = {{Princeton University Press}},
  address = {{Princeton}},
  doi = {doi:10.1515/9781400882618-003},
  urldate = {2023-08-07},
  isbn = {978-1-4008-8261-8}
}

@book{NKK94,
  title = {Neuronale {{Netze}} Und {{Fuzzy-Systeme}}},
  author = {Nauck, Detlef and Klawonn, Frank and Kruse, Rudolf},
  year = {1994},
  publisher = {{Vieweg+Teubner Verlag}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-322-85993-8_1},
  abstract = {Neuronale Netze, Expertensysteme und Fuzzy-Systeme erscheinen auf den ersten Blick als v\"ollig verschiedene Gebiete, zwischen denen kaum ein Zusammenhang besteht. In dieser Einleitung erl\"autern wir kurz die Grundkonzepte dieser drei Gebiete, aus denen wir ersehen k\"onnen, da\ss{} eine Kopplung Neuronaler Netze mit Expertensystemen und Fuzzy-Systemen vielversprechende M\"oglichkeiten er\"offnet. Au\ss erdem gehen wir auf den inhaltlichen Aufbau des Buches ein, der sich aus der Intention ergibt, die Verbindungen der konnektionistischen Modelle zu den anderen Gebieten aufzuzeigen.},
  isbn = {978-3-322-85993-8}
}

@article{Nwa,
  title = {Neural {{Networks}}, {{Artificial Intelligence}} and the {{Computational Brain}}},
  author = {Nwadiugwu, Martin C},
  abstract = {In recent years, several studies have provided insight on the functioning of the brain which consists of neurons and form networks via interconnection among them by synapses. Neural networks are formed by interconnected systems of neurons, and are of two types, namely, the Artificial Neural Network (ANNs) and Biological Neural Network (interconnected nerve cells). The ANNs are computationally influenced by human neurons and are used in modelling neural systems. The reasoning foundations of ANNs have been useful in anomaly detection, in areas of medicine such as instant physician, electronic noses, pattern recognition, and modelling biological systems. Advancing research in artificial intelligence using the architecture of the human brain seeks to model systems by studying the brain rather than looking to technology for brain models. This study explores the concept of ANNs as a simulator of the biological neuron, and its area of applications. It also explores why brain-like intelligence is needed and how it differs from computational framework by comparing neural networks to contemporary computers and their modern day implementation.},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\TWDZN4DL\Nwadiugwu - Neural Networks, Artificial Intelligence and the C.pdf}
}

@article{OF05,
  title = {How {{Close Are We}} to {{Understanding V1}}?},
  author = {Olshausen, Bruno A. and Field, David J.},
  year = {2005},
  month = aug,
  journal = {Neural Computation},
  volume = {17},
  number = {8},
  pages = {1665--1699},
  issn = {0899-7667},
  doi = {10.1162/0899766054026639},
  abstract = {A wide variety of papers have reviewed what is known about the function of primary visual cortex. In this review, rather than stating what is known, we attempt to estimate how much is still unknown about V1 function. In particular, we identify five problems with the current view of V1 that stem largely from experimental and theoretical biases, in addition to the contributions of nonlinearities in the cortex that are not well understood. Our purpose is to open the door to new theories, a number of which we describe, along with some proposals for testing them.}
}

@article{Ola96,
  title = {A {{Sociological Study}} of the {{Official History}} of the {{Perceptrons Controversy}}},
  author = {Olazaran, Mikel},
  year = {1996},
  journal = {Social Studies of Science},
  volume = {26},
  number = {3},
  pages = {611--659},
  doi = {10.1177/030631296026003005},
  abstract = {In this paper, I analyze the controversy within Artificial Intelligence (AI) which surrounded the `perceptron' project (and neural nets in general) in the late 1950s and early 1960s. I devote particular attention to the proofs and arguments of Minsky and Papert, which were interpreted as showing that further progress in neural nets was not possible, and that this approach to AI had to be abandoned. I maintain that this official interpretation of the debate was a result of the emergence, institutionalization and (importantly) legitimation of the symbolic AI approach (with its resource allocation system and authority structure). At the `research-area' level, there was considerable interpretative flexibility. This interpretative flexibility was further demonstrated by the revival of neural nets in the late 1980s, and subsequent rewriting of the official history of the debate.},
  file = {C:\Users\thorstensuckow\Zotero\storage\ELG2YKBD\Olazaran - 1996 - A Sociological Study of the Official History of th.pdf}
}

@book{OV06,
  title = {Rechneraufbau Und {{Rechnerstrukturen}}},
  author = {Oberschelp, Walter and Vossen, Gottfried},
  year = {2006},
  publisher = {{Oldenbourg Wissenschaftsverlag}},
  urldate = {2023-08-09},
  isbn = {978-3-486-57849-2}
}

@article{Per88,
  title = {Logical Neurons: The Enigmatic Legacy of {{Warren McCulloch}}},
  author = {Perkel, Donald H.},
  year = {1988},
  journal = {Trends in Neurosciences},
  volume = {11},
  number = {1},
  pages = {9--12},
  issn = {0166-2236},
  doi = {10.1016/0166-2236(88)90041-0}
}

@incollection{Pfa22,
  title = {Einleitung ,,{{K\"unstliche Intelligenz}} Im {{Gesundheitswesen}}``},
  booktitle = {K\"unstliche {{Intelligenz}} Im {{Gesundheitswesen}}: {{Entwicklungen}}, {{Beispiele}} Und {{Perspektiven}}},
  author = {Pfannstiel, Mario A.},
  editor = {Pfannstiel, Mario A.},
  year = {2022},
  pages = {1--47},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-33597-7_1},
  abstract = {Der Einsatz von k\"unstlicher Intelligenz im Gesundheitswesen hat in den letzten Jahren stark zugenommen und durchdringt zunehmend alle Lebens- und Arbeitsbereiche. Die Gesundheitsversorgung durchlebt bereits schon heute tiefgreifende Ver\"anderungen durch die DigitalisierungDigitalisierung. K\"unstliche Intelligenz gilt im Gesundheitswesen und in vielen anderen Branchen als Schl\"ussel- und Querschnittstechnologie. Durch sie wird die digitale Transformation in der Patientenversorgung und medizinischen Forschung beschleunigt.},
  isbn = {978-3-658-33597-7}
}

@article{Pic04,
  title = {The {{First Computational Theory}} of {{Mind}} and {{Brain}}: {{A Close Look}} at {{Mcculloch}} and {{Pitts}}'s ``{{Logical Calculus}} of {{Ideas Immanent}} in {{Nervous Activity}}''},
  author = {Piccinini, Gualtiero},
  year = {2004},
  month = aug,
  journal = {Synthese},
  volume = {141},
  number = {2},
  pages = {175--215},
  issn = {1573-0964},
  doi = {10.1023/B:SYNT.0000043018.52445.3e},
  abstract = {Despite its significance in neuroscience and computation, McCulloch and Pitts's celebrated 1943 paper has received little historical and philosophical attention. In 1943 there already existed a lively community of biophysicists doing mathematical work on neural networks. What was novel in McCulloch and Pitts's paper was their use of logic and computation to understand neural, and thus mental, activity. McCulloch and Pitts's contributions included (i) a formalism whose refinement and generalization led to the notion of finite automata (an important formalism in computability theory), (ii) a technique that inspired the notion of logic design (a fundamental part of modern computer design), (iii) the first use of computation to address the mind\textendash body problem, and (iv) the first modern computational theory of mind and brain.}
}

@article{PM47,
  title = {How We Know Universals the Perception of Auditory and Visual Forms},
  author = {Pitts, Walter and McCulloch, Warren S.},
  year = {1947},
  month = sep,
  journal = {The bulletin of mathematical biophysics},
  volume = {9},
  number = {3},
  pages = {127--147},
  issn = {1522-9602},
  doi = {10.1007/BF02478291},
  abstract = {Two neural mechanisms are described which exhibit recognition of forms. Both are independent of small perturbations at synapses of excitation, threshold, and synchrony, and are referred to partiular appropriate regions of the nervous system, thus suggesting experimental verification. The first mechanism averages an apparition over a group, and in the treatment of this mechanism it is suggested that scansion plays a significant part. The second mechanism reduces an apparition to a standard selected from among its many legitimate presentations. The former mechanism is exemplified by the recognition of chords regardless of pitch and shapes regardless of size. The latter is exemplified here only in the reflexive mechanism translating apparitions to the fovea. Both are extensions to contemporaneous functions of the knowing of universals heretofore treated by the authors only with respect to sequence in time.}
}

@article{PO06,
  title = {Context-Sensitive Autoassociative Memories as Expert Systems in Medical Diagnosis},
  author = {Pomi, Andr{\'e}s and Olivera, Fernando},
  year = {2006},
  month = nov,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {6},
  number = {1},
  pages = {39},
  issn = {1472-6947},
  doi = {10.1186/1472-6947-6-39},
  abstract = {The complexity of our contemporary medical practice has impelled the development of different decision-support aids based on artificial intelligence and neural networks. Distributed associative memories are neural network models that fit perfectly well to the vision of cognition emerging from current neurosciences.}
}

@book{Ras16,
  title = {Make {{Your Own Neural Network}}: {{A Gentle Journey Through}} the {{Mathematics}} of {{Neural Networks}}, and {{Making Your Own Using}} the {{Python Computer Language}}},
  author = {Rashid, T.},
  year = {2016},
  publisher = {{CreateSpace Independent Publishing Platform}},
  isbn = {978-1-5308-2660-5}
}

@incollection{RHW86,
  title = {Learning {{Internal Representations}} by {{Error Propagation}}},
  booktitle = {Parallel {{Distributed Processing}}: {{Explorations}} in the {{Microstructure}} of {{Cognition}}, {{Vol}}. 1: {{Foundations}}},
  author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
  year = {1986},
  pages = {318--362},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {0-262-68053-X}
}

@article{RHW86a,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.}
}

@incollection{RM87,
  title = {{{PDP Models}} and {{General Issues}} in {{Cognitive Science}}},
  booktitle = {Parallel {{Distributed Processing}}: {{Explorations}} in the {{Microstructure}} of {{Cognition}}: {{Foundations}}},
  author = {Rumelhart, David E. and McClelland, James L.},
  year = {1987},
  pages = {110--146}
}

@book{RN09,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2009},
  edition = {3rd},
  publisher = {{Prentice Hall Press}},
  address = {{USA}},
  abstract = {The long-anticipated revision of this \#1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.},
  isbn = {0-13-604259-7}
}

@article{ROC+18,
  title = {Scalable and Accurate Deep Learning with Electronic Health Records},
  author = {Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M. and Hajaj, Nissan and Hardt, Michaela and Liu, Peter J. and Liu, Xiaobing and Marcus, Jake and Sun, Mimi and Sundberg, Patrik and Yee, Hector and Zhang, Kun and Zhang, Yi and Flores, Gerardo and Duggan, Gavin E. and Irvine, Jamie and Le, Quoc and Litsch, Kurt and Mossin, Alexander and Tansuwan, Justin and Wang, De and Wexler, James and Wilson, Jimbo and Ludwig, Dana and Volchenboum, Samuel L. and Chou, Katherine and Pearson, Michael and Madabushi, Srinivasan and Shah, Nigam H. and Butte, Atul J. and Howell, Michael D. and Cui, Claire and Corrado, Greg S. and Dean, Jeffrey},
  year = {2018},
  month = may,
  journal = {npj Digital Medicine},
  volume = {1},
  number = {1},
  pages = {18},
  issn = {2398-6352},
  doi = {10.1038/s41746-018-0029-1},
  abstract = {Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient's record. We propose a representation of patients' entire raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two US academic medical centers with 216,221 adult patients hospitalized for at least 24\,h. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting: in-hospital mortality (area under the receiver operator curve [AUROC] across sites 0.93\textendash 0.94), 30-day unplanned readmission (AUROC 0.75\textendash 0.76), prolonged length of stay (AUROC 0.85\textendash 0.86), and all of a patient's final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed traditional, clinically-used predictive models in all cases. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios. In a case study of a particular prediction, we demonstrate that neural networks can be used to identify relevant information from the patient's chart.},
  file = {C:\Users\thorstensuckow\Zotero\storage\5JRNIW8Y\Rajkomar et al. - 2018 - Scalable and accurate deep learning with electroni.pdf}
}

@incollection{Roj93,
  title = {Das {{Modell}} von {{McCulloch}} Und {{Pitts}}},
  booktitle = {Theorie Der Neuronalen {{Netze}}: {{Eine}} Systematische {{Einf\"uhrung}}},
  author = {Rojas, Ra{\'u}l},
  year = {1993},
  pages = {29--50},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-61231-2_2},
  abstract = {Die Betrachtung der Eigenschaften biologischer neuronaler Netze im vorherigen Kapitel hat uns die Grundlagen zur Formulierung eines abstraktes Modells f\"ur k\"unstliche neuronale Netze vermittelt. Wichtig ist bei diesen Modellen das Verhalten des Netzes, und nicht seine genaue Struktur, die erst in einem Lernproze\ss{} endg\"ultig festgelegt wird. K\"unstliche neuronale Netze werden im Prinzip als eine Art black box verwendet, die f\"ur eine gewisse Eingabe eine bestimmte Ausgabe erzeugen soll. In das Netz wird im allgemeinen ein n-dimensionaler reeller Vektor (x1, x2,\textbackslash ldots, x n) eingegeben, dem ein m-dimensionaler Vektor (y l, y2,\textbackslash ldots, ym) als Ausgabe entspricht.},
  isbn = {978-3-642-61231-2}
}

@incollection{Roj93a,
  title = {Das {{Modell}} von {{McCulloch}} Und {{Pitts}}},
  booktitle = {Theorie Der Neuronalen {{Netze}}: {{Eine}} Systematische {{Einf\"uhrung}}},
  author = {Rojas, Ra{\'u}l},
  year = {1993},
  pages = {29--50},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-61231-2_2},
  abstract = {Die Betrachtung der Eigenschaften biologischer neuronaler Netze im vorherigen Kapitel hat uns die Grundlagen zur Formulierung eines abstraktes Modells f\"ur k\"unstliche neuronale Netze vermittelt. Wichtig ist bei diesen Modellen das Verhalten des Netzes, und nicht seine genaue Struktur, die erst in einem Lernproze\ss{} endg\"ultig festgelegt wird. K\"unstliche neuronale Netze werden im Prinzip als eine Art black box verwendet, die f\"ur eine gewisse Eingabe eine bestimmte Ausgabe erzeugen soll. In das Netz wird im allgemeinen ein n-dimensionaler reeller Vektor (x1, x2,\textbackslash ldots, x n) eingegeben, dem ein m-dimensionaler Vektor (y l, y2,\textbackslash ldots, ym) als Ausgabe entspricht.},
  isbn = {978-3-642-61231-2}
}

@techreport{Ros57,
  title = {The Perceptron - {{A}} Perceiving and Recognizing Automaton},
  author = {Rosenblatt, F.},
  year = {1957},
  month = jan,
  number = {85-460-1},
  address = {{Ithaca, New York}},
  institution = {{Cornell Aeronautical Laboratory}},
  file = {C:\Users\thorstensuckow\Zotero\storage\UEQE5ZV2\[Ros57] The Perceptron - A perceiving and recognizing automaton.pdf}
}

@article{Ros58,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {*Brain,*Cognition,*Memory,Nervous System},
  file = {C:\Users\thorstensuckow\Zotero\storage\H8DY4QJ4\Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf}
}

@article{Ros60,
  title = {Perceptron {{Simulation Experiments}}},
  author = {Rosenblatt, Frank},
  year = {1960},
  journal = {Proceedings of the IRE},
  volume = {48},
  number = {3},
  pages = {301--309},
  doi = {10.1109/JRPROC.1960.287598}
}

@inproceedings{Sal90,
  title = {Beschleunigtes {{Lernen}} Durch Adaptive {{Regelung}} Der {{Lernrate}} Bei Back-Propagation in Feed-Forward {{Netzen}}},
  booktitle = {Konnektionismus in {{Artificial Intelligence}} Und {{Kognitionsforschung}}},
  author = {Salomon, R.},
  editor = {Dorffner, Georg},
  year = {1990},
  pages = {173--178},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {Dieser Artikel besch\"aftigt sich mit back-propagation, einem Lernverfahren f\"ur Neuronale Netze. Es wird gezeigt, wie sich durch das Einf\"uhren von Testzyklen erstens eine starke Beschleunigung der Konvergenzgeschwindigkeit ergibt, und zweitens aufwendige Experimente, die zum Einstellen lernrelevanter Parameter dienen, entfallen k\"onnen.},
  isbn = {978-3-642-76070-9}
}

@article{SEJ+20,
  title = {Improved Protein Structure Prediction Using Potentials from Deep Learning},
  author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v Z}{\'i}dek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2020},
  month = jan,
  journal = {Nature},
  volume = {577},
  number = {7792},
  pages = {706--710},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1923-7},
  abstract = {Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)\textemdash a blind assessment of the state of the field\textemdash AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7.},
  file = {C:\Users\thorstensuckow\Zotero\storage\5JS8C7IG\Senior et al. - 2020 - Improved protein structure prediction using potent.pdf}
}

@article{Sha38,
  title = {A Symbolic Analysis of Relay and Switching Circuits},
  author = {Shannon, Claude E.},
  year = {1938},
  journal = {Transactions of the American Institute of Electrical Engineers},
  volume = {57},
  number = {12},
  pages = {713--723},
  doi = {10.1109/T-AIEE.1938.5057767}
}

@article{Sha83,
  title = {The {{Fifth Generation Project}} \textemdash{} a {{Trip Report}}},
  author = {Shapiro, Ehud Y.},
  year = {1983},
  month = sep,
  journal = {Commun. ACM},
  volume = {26},
  number = {9},
  pages = {637--641},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {0001-0782},
  doi = {10.1145/358172.358179},
  abstract = {As part of Japan's effort to become a leader in the computer industry, the Institute for New Generation Computer Technology has launched a revolutionary ten-year plan for the development of large computer systems which will be applicable to knowledge information processing systems. These Fifth Generation computers will be built around the concepts of logic programming. In order to refute the accusation that Japan exploits knowledge from abroad without contributing any of its own, this project will stimulate original research and will make its results available to the international research community.}
}

@inproceedings{Sha86,
  title = {Donald {{Hebb}}: {{The Organization}} of {{Behavior}}},
  booktitle = {Brain {{Theory}}},
  author = {Shaw, G. L.},
  editor = {Palm, G{\"u}nther and Aertsen, Ad},
  year = {1986},
  pages = {231--233},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {I consider this a great privilege to be able to briefly remark on D.O. Hebb's marvellous book ``Organization of Behavior: A Neuropsychological Theory'' which he wrote in 1949. Hebb's ideas have had a profound influence on brain theory, in particular his famous ``A Neurophysiological Postulate'' governing the correlated pre-post synaptic changes which are the basis for the engram or memory trace. Although, there are many different forms of Hebb's postulate, I believe that essentially all ``viable'' mammalian cortical models embody some version of his idea: ``Let us assume then that the persistence or repetition of a reverberatory activity (or ``trace'') tends to induce lasting cellular changes that add to its stability. The assumption can be precisely stated as follows:When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A `s efficiency, as one of the cells firing B, is increased''.},
  isbn = {978-3-642-70911-1}
}

@article{Sha92,
  title = {The {{Developing Brain}}},
  author = {Shatz, Carla J},
  year = {1992},
  journal = {SCIENTIFIC AMERICAN},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\K7ZVS93X\Shatz - 1992 - The Developing Brain.pdf}
}

@article{She13,
  title = {A Set of Five Independent Postulates for {{Boolean}} Algebras, with Application to Logical Constants},
  author = {Sheffer, Henry M.},
  year = {1913},
  journal = {Transactions of the American Mathematical Society},
  volume = {14},
  pages = {481--488}
}

@article{SHM+16,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.}
}

@article{Sil10,
  title = {Neuronal Arithmetic},
  author = {Silver, R. Angus},
  year = {2010},
  month = jul,
  journal = {Nature Reviews Neuroscience},
  volume = {11},
  number = {7},
  pages = {474--489},
  issn = {1471-0048},
  doi = {10.1038/nrn2864},
  abstract = {A neuron can rapidly combine and transform the information it receives through its synaptic inputs before the information is converted into neuronal output. This transformation can be defined by the neuronal input\textendash output (I\textendash O) relationship. Changes in the I\textendash O relationship can correspond to distinct arithmetic operations.During sustained rate-coded signalling, the neuron operates as a signal integrator (over a given time-window) and the neuronal I\textendash O relationship is defined as the dependence of sustained output firing rate on the input rate. During sparse temporally correlated signalling, the neuron acts as a coincidence detector and the I\textendash O relationship can be defined in terms of the dependence of spike probability on the number of coincident inputs or their temporal correlation.Additive and subtractive operations performed on driving inputs by distinct modulatory synaptic input correspond to shifts in the I\textendash O relationship, without a change of shape. Multiplicative and divisive operations correspond to increases or decreases in the slope, or gain, of the I\textendash O relationship. Both of these operations have been observed in vivo during different tasks.Additive operations are essential for linearly combining signals and for controlling the number of inputs required for signalling. In the temporal domain, additive operations control the width of the temporal correlation window that the neuron can respond to and thus the temporal properties of signals that can propagate through the network.Multiplicative operations, or gain changes, are important for signal amplification, normalization and preventing saturation of firing, thereby allowing efficient information transmission. Gain changes are essential for coordinate transforms and have been proposed to control the functional connectivity of networks. In the temporal domain, neural gain also controls the 'roll-off' of the temporal correlation window for synaptic integration.Both morphologically simple neurons and those with extensive dendritic trees possess a number of biophysical mechanisms, including inhibition, short-term synaptic plasticity, synaptic noise and somatic and dendritic conductances that enable them to perform additive and multiplicative operations on their synaptic inputs.Some biophysical mechanisms, such as voltage noise, are general in that they perform the same arithmetic operation (multiplication) on sustained rate-coded and sparse temporally coded signals. However, others seem to be tuned for either sustained rate-coding or sparse coding regimes.Widespread presynaptic and dendritic mechanisms enable spatially segregated inputs to be multiplied together, although other local dendritic conductances seem to be tuned to detect spatio-temporally correlated input onto a specific dendritic branch.An extensive tool kit of nonlinear mechanisms confers considerable computational power on individual neurons, enabling them to perform a range of arithmetic operations on signals encoded in a variety of different ways.}
}

@incollection{Son22,
  title = {Neuronale {{Netze}}},
  booktitle = {Neuronale {{Netze}} Kompakt: {{Vom Perceptron}} Zum {{Deep Learning}}},
  author = {Sonnet, Daniel},
  year = {2022},
  pages = {17--70},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-29081-8_2},
  abstract = {Neuronale Netze sind keine aktuelle Erfindung. Das Thema geht bis in die erste H\"alfte des vergangenen Jahrhunderts zur\"uck. Zu dieser Zeit gab es die Informatik wie in der heutigen Form noch nicht. Darum waren es keine Informatiker, die die Forschung um Neuronale Netze initialisierten, sondern Psychologen. In diesem Kapitel wird mit einem kompakten historischen Abriss gestartet. Alles begann mit dem sogenannten Perceptron, welches bereits erstaunliche approximative F\"ahigkeiten besa\ss{} und eine Disziplin innerhalb der k\"unstlichen Intelligenzforschung bildete. 1970, ausgel\"ost durch den sogenannten Lighthill Report erfuhr die KI-Forschung einen D\"ampfer, welcher den KI-Winter einl\"autete. Jeder Winter wird vom Fr\"uhling verdr\"angt, so auch hier. Mehr- bzw. tiefschichtige Netze (Deep Neural Networks) bilden heute den Status quo. Nach diesem historischen Abriss, der entscheidend zum Verst\"andnis dieser toller Disziplin ist, wird das Kapitel weitergef\"uhrt, indem ein paar unterschiedliche Lern- bzw. Trainingsverfahren Neuronaler Netze vorgestellt werden. Den Abschluss dieses Kapitels bildet ein Unterkapitel \"uber besonders erw\"ahnenswerte Netzwerktypen sowie ihre m\"oglichen Einsatzgebiete. In diesem Kapitel werden noch einmal wichtige Fachbegriffe er\"ortert und den einzelnen Netzwerktypen zugeordnet.},
  isbn = {978-3-658-29081-8}
}

@book{Squ13,
  title = {Fundamental Neuroscience},
  editor = {Squire, Larry R.},
  year = {2013},
  edition = {4th ed},
  publisher = {{Elsevier/Academic Press}},
  address = {{Amsterdam ; Boston}},
  isbn = {978-0-12-385870-2},
  langid = {english},
  lccn = {QP355.2 .F862 2013},
  keywords = {Neurosciences},
  file = {C:\Users\thorstensuckow\Zotero\storage\MMYACL9M\Squire - 2013 - Fundamental neuroscience.pdf}
}

@incollection{SSH95,
  title = {Automaten},
  booktitle = {Automaten {{Sprachen Berechenbarkeit}}: {{Grundkurs Angewandte Informatik IV}}},
  author = {Sander, Peter and Stucky, Wolffried and Herschel, Rudolf},
  editor = {Stucky, W.},
  year = {1995},
  pages = {26--97},
  publisher = {{Vieweg+Teubner Verlag}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-322-84873-4_2},
  abstract = {Automaten umgeben uns im t\"aglichen Leben \"uberall, und jedermann hat eine mehr oder weniger pr\"azise Vorstellung davon, was ein Automat ist. Irgendwie sind damit Begriffe wie ,,definierter Ablauf von Handlungen``, ,,selbstt\"atige Aneinanderreihung von Aktionen``, ,,determinierte Folge von Zust\"anden`` oder dergleichen verbunden. Die Beispiele f\"ur Automaten sind ungeheuer mannigfaltig, sie reichen vom Zigaretten-, Fahrkarten-, Getr\"anke- und Geldwechselautomaten \"uber den M\"unz- und Kartenfernsprecher bis zu Computern, den Rechenautomaten. Die Kompliziertheit der aufgef\"uhrten Beispiele ist sehr verschiedenartig. Es soll die Aufgabe des folgenden einf\"uhrenden Abschnitts sein, die prinzipiellen Gemeinsamkeiten herauszuarbeiten. Dabei geht es insbesondere um die Frage, wie man die Arbeitsweise, das Verhalten eines Automaten losgel\"ost von seinen Bauelementen und seiner physikalischen Realisierung darstellen kann. Die sich dabei ergebenden Prinzipien und Begriffe bilden dann die Grundlage f\"ur eine mathematische Beschreibung in den folgenden Abschnitten.},
  isbn = {978-3-322-84873-4}
}

@article{SST+20,
  title = {Fluid-Limiting Treatment Strategies among Sepsis Patients in the {{ICU}}: A Retrospective Causal Analysis},
  author = {Shahn, Zach and Shapiro, Nathan I. and Tyler, Patrick D. and Talmor, Daniel and Lehman, Li-wei H.},
  year = {2020},
  month = feb,
  journal = {Critical Care},
  volume = {24},
  number = {1},
  pages = {62},
  issn = {1364-8535},
  doi = {10.1186/s13054-020-2767-0},
  abstract = {In septic patients, multiple retrospective studies show an association between large volumes of fluids administered in the first 24\,h and mortality, suggesting a benefit to fluid restrictive strategies. However, these studies do not directly estimate the causal effects of fluid-restrictive strategies, nor do their analyses properly adjust for time-varying confounding by indication. In this study, we used causal inference techniques to estimate mortality outcomes that would result from imposing a range of arbitrary limits (``caps'') on fluid volume administration during the first 24\,h of intensive care unit (ICU) care.}
}

@incollection{Ste22,
  title = {Explainable {{AI}} Im {{Gesundheitswesen}}},
  booktitle = {K\"unstliche {{Intelligenz}} Im {{Gesundheitswesen}}: {{Entwicklungen}}, {{Beispiele}} Und {{Perspektiven}}},
  author = {Steinwendner, Joachim},
  editor = {Pfannstiel, Mario A.},
  year = {2022},
  pages = {755--767},
  publisher = {{Springer Fachmedien Wiesbaden}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-33597-7_36},
  abstract = {K\"unstliche Intelligenz (KI) erlaubt das automatische L\"osen von Aufgaben ohne menschliche Interaktion. Im Bereich der medizinischen Diagnostik sind besonders tiefe neuronale Netze (Deep Learning) sehr erfolgreich. Je geringer die Bench-to-Bedside-Distanz, desto klarer werden die Probleme, die mit diesen Methoden einhergehen. Eine gro\ss e Herausforderung ist die Notwendigkeit der Erkl\"arbarkeit einer Aussage eines KI-Modells, die vor allem auch bei Blackbox-Modell genannten neuronalen Netze besteht. F\"ur den behandelnden Gesundheitsdienstleister wird diese Frage jedoch immer wichtiger, je mehr diese Algorithmen in die Klinik dr\"angen. Mit der Frage der Interpretierbarkeit oder Erkl\"arbarkeit in diesem Zusammenhang besch\"aftigt sich das Feld der Explainable Artificial Intelligence (XAI).},
  isbn = {978-3-658-33597-7}
}

@book{Str01,
  title = {The {{Gale}} Encyclopedia of Psychology},
  editor = {Strickland, Bonnie B.},
  year = {2001},
  edition = {2nd ed},
  publisher = {{Gale Group}},
  address = {{Detroit, MI}},
  isbn = {978-0-7876-4786-5},
  langid = {english},
  lccn = {BF31 .G35 2001},
  keywords = {Encyclopedias,Psychology},
  file = {C:\Users\thorstensuckow\Zotero\storage\5AKNT6WJ\Strickland - 2001 - The Gale encyclopedia of psychology.pdf}
}

@misc{SVI+15,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year = {2015}
}

@article{SZJ+19,
  title = {Artificial {{Intelligence Versus Clinicians}} in {{Disease Diagnosis}}: {{Systematic Review}}},
  author = {Shen, Jiayi and Zhang, Casper J P and Jiang, Bangsheng and Chen, Jiebin and Song, Jian and Liu, Zherui and He, Zonglin and Wong, Sum Yi and Fang, Po-Han and Ming, Wai-Kit},
  year = {2019},
  month = aug,
  journal = {JMIR Med Inform},
  volume = {7},
  number = {3},
  eprint = {31420959},
  eprinttype = {pubmed},
  pages = {e10010},
  issn = {2291-9694},
  doi = {10.2196/10010},
  abstract = {Background: Artificial intelligence (AI) has been extensively used in a range of medical fields to promote therapeutic development. The development of diverse AI techniques has also contributed to early detections, disease diagnoses, and referral management. However, concerns about the value of advanced AI in disease diagnosis have been raised by health care professionals, medical service providers, and health policy decision makers. Objective: This review aimed to systematically examine the literature, in particular, focusing on the performance comparison between advanced AI and human clinicians to provide an up-to-date summary regarding the extent of the application of AI to disease diagnoses. By doing so, this review discussed the relationship between the current advanced AI development and clinicians with respect to disease diagnosis and thus therapeutic development in the long run. Methods: We systematically searched articles published between January 2000 and March 2019 following the Preferred Reporting Items for Systematic reviews and Meta-Analysis in the following databases: Scopus, PubMed, CINAHL, Web of Science, and the Cochrane Library. According to the preset inclusion and exclusion criteria, only articles comparing the medical performance between advanced AI and human experts were considered. Results: A total of 9 articles were identified. A convolutional neural network was the commonly applied advanced AI technology. Owing to the variation in medical fields, there is a distinction between individual studies in terms of classification, labeling, training process, dataset size, and algorithm validation of AI. Performance indices reported in articles included diagnostic accuracy, weighted errors, false-positive rate, sensitivity, specificity, and the area under the receiver operating characteristic curve. The results showed that the performance of AI was at par with that of clinicians and exceeded that of clinicians with less experience. Conclusions: Current AI development has a diagnostic performance that is comparable with medical experts, especially in image recognition-related fields. Further studies can be extended to other types of medical imaging such as magnetic resonance imaging and other medical practices unrelated to images. With the continued development of AI-assisted technologies, the clinical implications underpinned by clinicians' experience and guided by patient-centered health care principle should be constantly considered in future AI-related and other technology-based medical research.},
  keywords = {artificial intelligence,computer-assisted,deep learning,diagnosis,diagnostic imaging,image interpretation,patient-centered care},
  file = {C:\Users\thorstensuckow\Zotero\storage\FXALHZBJ\Shen et al. - 2019 - Artificial Intelligence Versus Clinicians in Disea.pdf}
}

@article{Szo88,
  title = {Artificial {{Intelligence}} in {{Medical Diagnosis}}},
  author = {Szolovits, Peter},
  year = {1988},
  journal = {Artificial Intelligence},
  volume = {108},
  number = {1},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\82FT4RL9\Szolovits - 1988 - Artificial Intelligence in Medical Diagnosis.pdf}
}

@inproceedings{Tap19,
  title = {Who {{Is}} the {{Father}} of {{Deep Learning}}?},
  booktitle = {2019 {{International Conference}} on {{Computational Science}} and {{Computational Intelligence}} ({{CSCI}})},
  author = {Tappert, Charles C.},
  year = {2019},
  pages = {343--348},
  doi = {10.1109/CSCI49370.2019.00067},
  file = {C:\Users\thorstensuckow\Zotero\storage\Y6GVMKS7\Tappert - 2019 - Who Is the Father of Deep Learning.pdf}
}

@article{TS81,
  title = {An Analysis of Physician Attitudes Regarding Computer-Based Clinical Consultation Systems},
  author = {Teach, Randy L. and Shortliffe, Edward H.},
  year = {1981},
  journal = {Computers and Biomedical Research},
  volume = {14},
  number = {6},
  pages = {542--558},
  issn = {0010-4809},
  doi = {10.1016/0010-4809(81)90012-4},
  abstract = {Physician attitudes regarding computer-based clinical decision aids and the effect of a 2-day tutorial on medical computing are studied. The results indicate that physicians are accepting of applications that enhance their patient management capabilities, but tend to oppose applications in which they perceive an infringement on their management role. Expectations about the effect of computing on current medical practices are found to be generally favorable, although considerable individual differences exist among subgroups. The study participants place substantial demands on the performance capabilities of acceptable consultation systems, and emphasize the need for humanlike interactive capabilities. The tutorial had no effect on attitudes regarding appropriate clinical uses of computers nor on expectations about the effect of the technology on medical practice. However, it did increase the participants' knowledge of medical computing and led to more informed demands on system performance. We discuss the implications of the study and offer suggestions for developing and implementing computer-based clinical decision aids.}
}

@article{Tuc86,
  title = {On Shunting Inhibition},
  author = {Tuckwell, H. C.},
  year = {1986},
  month = nov,
  journal = {Biological Cybernetics},
  volume = {55},
  number = {2},
  pages = {83--90},
  issn = {1432-0770},
  doi = {10.1007/BF00341923},
  urldate = {2023-08-02},
  abstract = {The interaction between excitation and inhibition is analyzed for nerve cylinders when reversal potentials for synaptic action are included. Both impulsive and sustained conductance changes are employed to model synaptic action.},
  langid = {english},
  keywords = {Amplification Factor,Exact Result,Reversal Potential,Spatial Separation,Synaptic Action}
}

@article{TuringComputableNumbersApplicationEntscheidungsproblem1937,
  title = {On {{Computable Numbers}}, with an {{Application}} to the {{Entscheidungsproblem}}},
  author = {Turing, A. M.},
  year = {1937},
  journal = {Proceedings of the London Mathematical Society},
  volume = {s2-42},
  number = {1},
  pages = {230--265},
  doi = {10.1112/plms/s2-42.1.230}
}

@article{Ugu12,
  title = {A {{Biomedical System Based}} on {{Artificial Neural Network}} and {{Principal Component Analysis}} for {{Diagnosis}} of the {{Heart Valve Diseases}}},
  author = {U{\u g}uz, Harun},
  year = {2012},
  month = feb,
  journal = {Journal of Medical Systems},
  volume = {36},
  number = {1},
  pages = {61--72},
  issn = {1573-689X},
  doi = {10.1007/s10916-010-9446-7},
  abstract = {Listening via stethoscope is a primary method, being used by physicians for distinguishing normally and abnormal cardiac systems. Listening to the voices, coming from the cardiac valves via stethoscope, upon the flow of the blood running in the heart, physicians examine whether there is any abnormality with regard to the heart. However, listening via stethoscope has got a number of limitations, for interpreting different heart sounds depends on hearing ability, experience, and respective skill of the physician. Such limitations may be reduced by developing biomedical based decision support systems. In this study, a biomedical-based decision support system was developed for the classification of heart sound signals, obtained from 120 subjects with normal, pulmonary and mitral stenosis heart valve diseases via stethoscope. Developed system was mainly comprised of three stages, namely as being feature extraction, dimension reduction, and classification. At feature extraction stage, applying Discrete Fourier Transform (DFT) and Burg autoregressive (AR) spectrum analysis method, features, representing heart sounds in frequency domain, were obtained. Obtained features were reduced in lower dimensions via Principal Component Analysis (PCA), being used as a dimension reduction technique. Heart sounds were classified by having the features applied as input to Artificial Neural Network (ANN). Classification results have shown that, dimension reduction, being conducted via PCA, has got positive effects on the classification of the heart sounds.}
}

@inproceedings{Van86,
  title = {Frank {{Rosenblatt}}: {{Principles}} of {{Neurodynamics}}: {{Perceptrons}} and the {{Theory}} of {{Brain Mechanisms}}},
  booktitle = {Brain {{Theory}}},
  author = {Van Der Malsburg, C.},
  editor = {Palm, G{\"u}nther and Aertsen, Ad},
  year = {1986},
  pages = {245--248},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {Frank Rosenblatt's intention with his book, according to his own introduction, is not just to describe a machine, the perceptron, but rather to put forward a theory. He formulates a series of machines. Each machine serves to introduce a new concept.},
  isbn = {978-3-642-70911-1},
  file = {C:\Users\thorstensuckow\Zotero\storage\ANRUL7H2\Van Der Malsburg - 1986 - Frank Rosenblatt Principles of Neurodynamics Per.pdf}
}

@incollection{Von11,
  title = {Einf\"uhrung},
  booktitle = {Glastechnische {{Fabrikationsfehler}}: ``{{Pathologische}}'' {{Ausnahmezust\"ande}} Des {{Werkstoffes Glas}} Und Ihre {{Behebung}}; {{Eine Br\"ucke}} Zwischen {{Wissenschaft}}, {{Technologie}} Und {{Praxis}}},
  author = {{Von Jebsen-Marwedel}, H.},
  editor = {{Jebsen-Marwedel}, Hans and Br{\"u}ckner, Rolf},
  year = {2011},
  pages = {3--15},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-16433-0_1},
  abstract = {Glas ist ein aus der Schmelze hervorgegangenes Produkt, das bei hohen Temperaturen eine stark ausgepr\"agte Z\"ahigkeit besitzt. Diese erlaubt seinen Strukturelementen (Tetraeder, Oktaeder) nicht, mit einsetzender Abk\"uhlung bei der Verarbeitungstemperatur den geordneten Zustand eines Kristallgitters anzunehmen, wie es bei anderen Stoffen, z.B. den Ionenkristallen oder Metallen der Fall ist, die beim Schmelzpunkt spontan kristallisieren. Glasschmelzen unterschreiten diese thermodynamisch kritische Zone und erstarren metastabil als unterk\"uhlte Fl\"ussigkeiten, die beim weiteren Abk\"uhlen ihre fl\"ussigkeits\"ahnliche Struktur einfrieren [1]. Sie bieten auch den Anblick einer als Fl\"ussigkeit von unendlich hoher Viskosit\"at verfestigten Substanz und stellen damit einen Sonderzustand der Materie dar, der auch schon \textendash{} etwas anspruchsvoll \textendash{} von Berger [2] als IV. Aggregatzustand bezeichnet wurde. Er befindet sich sozusagen zwischen der fluiden und der festen Beschaffenheit in der Schwebe. Das Glas hat gewisserma\ss en vers\"aumt, die allenfalls f\"allige Wandlung zur Kristallisation an sich zu vollziehen. Bild 1.1 gibt den Zusammenhang schematisch wieder. Die Glasschmelze durchl\"auft beim Abk\"uhlen den metastabilen Zustand der unterk\"uhlten Schmelze (Tammann [la]), um die Struktur im Einfrier bereich zu fixieren (Jenckel [1 c]) und anschlie\ss end in den instabilen Zustand des Glases \"uberzugehen (e \textendash{} f \textendash{} h in Bild 1.1).},
  isbn = {978-3-642-16433-0}
}

@incollection{von51,
  title = {The General and Logical Theory of Automata.},
  booktitle = {Cerebral Mechanisms in Behavior; the {{Hixon Symposium}}.},
  author = {{von Neumann}, John},
  year = {1951},
  pages = {1--41},
  publisher = {{Wiley}},
  address = {{Oxford,  England}},
  abstract = {A general outline of the ideas and trends in the application of artificial automata theory to living organisms and particularly the human central nervous system. The parts or elements of a system are treated as "black boxes" and their properties and functional regularities in combination are discussed. The material is organized under the major headings: Preliminary considerations, Discussion of certain relevant traits of computing machines, Comparisons between computing machines and living organisms, Future logical theory of automata, Principles of digitalization, Formal neural networks, Concept of complication; self-reproduction, and a general panel discussion included between the pages 32-41. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{von93,
  title = {First Draft of a Report on the {{EDVAC}}},
  author = {{von Neumann}, J.},
  year = {1993},
  journal = {IEEE Annals of the History of Computing},
  volume = {15},
  number = {4},
  pages = {27--75},
  doi = {10.1109/85.238389}
}

@article{Wur09,
  title = {Recounting the Impact of {{Hubel}} and {{Wiesel}}: {{Recounting}} the Impact of {{Hubel}} and {{Wiesel}}},
  shorttitle = {Recounting the Impact of {{Hubel}} and {{Wiesel}}},
  author = {Wurtz, Robert H.},
  year = {2009},
  month = jun,
  journal = {The Journal of Physiology},
  volume = {587},
  number = {12},
  pages = {2817--2823},
  issn = {00223751},
  doi = {10.1113/jphysiol.2009.170209},
  urldate = {2023-09-05},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\HYTMVI3E\Wurtz - 2009 - Recounting the impact of Hubel and Wiesel Recount.pdf}
}

@article{YLC+14,
  title = {Sleep Promotes Branch-Specific Formation of Dendritic Spines after Learning},
  author = {Yang, Guang and Lai, Cora Sau Wan and Cichon, Joseph and Ma, Lei and Li, Wei and Gan, Wen-Biao},
  year = {2014},
  journal = {Science},
  volume = {344},
  number = {6188},
  pages = {1173--1178},
  doi = {10.1126/science.1249098},
  abstract = {Many researchers believe sleep helps us consolidate our memories, but no one knows quite how. Yang et al. investigated the precise role of sleep in changing mouse brain structures (see the Perspective by Euston and Steenland). When mice learned motor tasks, small protuberances\textemdash or ``spines''\textemdash formed on some of the dendritic branches of specific brain neurons. These spines represent the physical correlate of a memory. But the neurons grew and retained these spines better when the mice slept after learning the task. Neurons that fired during learning fired again during subsequent slow-wave sleep, allowing the mice to conserve the newly formed spines\textemdash and memories. Science, this issue p. 1173; see also p. 1087 In mice, synaptic connectivity changes after motor learning are influenced by sleep. [Also see Perspective by Euston and Steenland] How sleep helps learning and memory remains unknown. We report in mouse motor cortex that sleep after motor learning promotes the formation of postsynaptic dendritic spines on a subset of branches of individual layer V pyramidal neurons. New spines are formed on different sets of dendritic branches in response to different learning tasks and are protected from being eliminated when multiple tasks are learned. Neurons activated during learning of a motor task are reactivated during subsequent non\textendash rapid eye movement sleep, and disrupting this neuronal reactivation prevents branch-specific spine formation. These findings indicate that sleep has a key role in promoting learning-dependent synapse formation and maintenance on selected dendritic branches, which contribute to memory storage.},
  file = {C:\Users\thorstensuckow\Zotero\storage\VWCPHZHQ\Yang et al. - 2014 - Sleep promotes branch-specific formation of dendri.pdf}
}

@book{zotero-10,
  isbn = {978-0-262-03561-3}
}

@misc{zotero-105,
  title = {Donald {{HEBB}}, {{PhD}} | {{The Neuro}} - {{McGill University}}},
  urldate = {2023-08-16},
  howpublished = {https://www.mcgill.ca/neuro/about/donald-hebb-phd},
  file = {C:\Users\thorstensuckow\Zotero\storage\7ZNSDTIG\donald-hebb-phd.html}
}

@misc{zotero-178,
  title = {Deep-Learning Technique Predicts Clinical Treatment Outcomes | {{MIT News}} | {{Massachusetts Institute}} of {{Technology}}},
  urldate = {2023-09-07},
  howpublished = {https://news.mit.edu/2022/deep-learning-technique-predicts-clinical-treatment-outcomes-0224},
  file = {C:\Users\thorstensuckow\Zotero\storage\JST7VU5V\deep-learning-technique-predicts-clinical-treatment-outcomes-0224.html}
}

@misc{zotero-180,
  title = {[1511.05942] {{Doctor AI}}: {{Predicting Clinical Events}} via {{Recurrent Neural Networks}}},
  urldate = {2023-09-07},
  howpublished = {https://arxiv.org/abs/1511.05942},
  file = {C\:\\Users\\thorstensuckow\\Zotero\\storage\\BFXGJ65J\\[1511.05942] Doctor AI Predicting Clinical Events.pdf;C\:\\Users\\thorstensuckow\\Zotero\\storage\\7U99IZQS\\1511.html}
}

@misc{zotero-190,
  title = {[1505.04597] {{U-Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  urldate = {2023-09-07},
  howpublished = {https://arxiv.org/abs/1505.04597},
  file = {C:\Users\thorstensuckow\Zotero\storage\6BRVC7HC\1505.html}
}

@misc{zotero-197,
  title = {{{KI-System}} Knackt {{Proteincode}} - {{AlphaFold}} Entschl\"usselt {{Faltung}} von {{Proteinen}} Mit Bislang Unerreichter {{Pr\"azision}} - Scinexx.De},
  urldate = {2023-09-07},
  howpublished = {https://www.scinexx.de/news/biowissen/ki-system-knackt-proteincode/},
  file = {C:\Users\thorstensuckow\Zotero\storage\YRQSHNJ5\ki-system-knackt-proteincode.html}
}

@misc{zotero-199,
  title = {Offene {{Daten}} F\"ur {{KI}} | {{BMZ Digital}}.{{Global}}},
  urldate = {2023-09-07},
  howpublished = {https://www.bmz-digital.global/initiativen-im-ueberblick/fair-forward/},
  file = {C:\Users\thorstensuckow\Zotero\storage\IR27Y95C\fair-forward.html}
}

@misc{zotero-224,
  title = {Deep-Learning Algorithm Diagnoses Pneumonia in 10 Seconds},
  urldate = {2023-09-09},
  howpublished = {https://healthexec.com/topics/precision-medicine/deep-learning-algorithm-diagnoses-pneumonia-10-seconds},
  file = {C:\Users\thorstensuckow\Zotero\storage\6GSW8EDB\deep-learning-algorithm-diagnoses-pneumonia-10-seconds.html}
}

@misc{zotero-232,
  title = {{{IMS Disease Analyzer}} - {{Deutsch}}},
  urldate = {2023-09-10},
  howpublished = {https://www.gbe-bund.de/gbe/abrechnung.prc\_abr\_test\_logon?p\_uid=gast\&p\_aid=0\&p\_knoten=FID\&p\_sprache=D\&p\_suchstring=453},
  file = {C:\Users\thorstensuckow\Zotero\storage\8L2CG4PS\abrechnung.html}
}

@misc{zotero-236,
  title = {Deep {{Learning}} to {{Predict Falls}} in {{Older Adults Based}} on {{Daily-Life Trunk Accelerometry}} - {{PubMed}}},
  urldate = {2023-09-10},
  howpublished = {https://pubmed.ncbi.nlm.nih.gov/29786659/},
  file = {C:\Users\thorstensuckow\Zotero\storage\YAJYW67Y\29786659.html}
}

@misc{zotero-238,
  title = {Scalable and Accurate Deep Learning with Electronic Health Records | Npj {{Digital Medicine}}},
  urldate = {2023-09-10},
  howpublished = {https://www.nature.com/articles/s41746-018-0029-1},
  file = {C:\Users\thorstensuckow\Zotero\storage\GQEZZAGM\s41746-018-0029-1.html}
}

@misc{zotero-240,
  title = {Effect of a Machine Learning-Based Severe Sepsis Prediction Algorithm on Patient Survival and Hospital Length of Stay: A Randomised Clinical Trial - {{PMC}}},
  urldate = {2023-09-10},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5687546/},
  file = {C:\Users\thorstensuckow\Zotero\storage\XR4GFETK\PMC5687546.html}
}

@misc{zotero-256,
  title = {Digitalisierung Und {{K\"unstliche Intelligenz}}},
  urldate = {2023-09-12},
  howpublished = {https://www.gesundheitsforschung-bmbf.de/de/digitalisierung-und-kunstliche-intelligenz-9461.php},
  file = {C:\Users\thorstensuckow\Zotero\storage\4VZEDSK3\digitalisierung-und-kunstliche-intelligenz-9461.html}
}

@article{zotero-33,
  title = {Warren {{S}}. {{McCulloch}}: {{What Is}} a {{Number}}, That a {{Man May Know It}}, and a {{Man}}, That {{He May Know}} a {{Number}}?},
  langid = {english},
  file = {C:\Users\thorstensuckow\Zotero\storage\VKGR3KXL\Warren S. McCulloch What Is a Number, that a Man .pdf}
}

@misc{zotero-45,
  title = {{{McCulloch}}, {{W}}. {{S}}. (1967a). {{Lekton}}, Being a Belated Introduction to the Thesis of {{Eilhard}} von {{Domarus}}. {{In L}}. {{Thayer}} ({{Ed}}.). {{Communication}}: {{Theory}} and Research. {{Proceedings}} of the First Symposium (Pp. 348\textendash 350). {{Springfield IL}}: {{Charles C}}. {{Thomas}}.},
  file = {C:\Users\thorstensuckow\Zotero\storage\WVJYP2VL\[Mcc67] Lekton, being a belated introduction to the thesis of Eilhard von Domarus.pdf}
}

@book{zotero-51,
  type = {Book}
}

@article{zotero-77,
  title = {B. {{W}}. {{White}} Und {{F}}. {{Rosenblatt}}, ,,{{Principles}} of {{Neurodynamics}}: {{Perceptrons}} and the {{Theory}} of {{Brain Mechanisms}}``, {{The American Journal}} of {{Psychology}}, {{Bd}}. 76, {{Nr}}. 4. {{University}} of {{Illinois Press}}, {{S}}. 705, {{Dez}}. 1963. Doi: 10.2307/1419730.},
  file = {C:\Users\thorstensuckow\Zotero\storage\NLY3M2FM\B. W. White und F. Rosenblatt, „Principles of Neur.pdf}
}

@misc{zotero-79,
  title = {Professor's Perceptron Paved the Way for {{AI}} \textendash{} 60 Years Too Soon | {{Cornell Chronicle}}},
  urldate = {2023-08-14},
  howpublished = {https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon},
  file = {C:\Users\thorstensuckow\Zotero\storage\NHJPHBUZ\professors-perceptron-paved-way-ai-60-years-too-soon.html}
}

@misc{zotero-93,
  title = {Dr. {{Donald Hebb Biographie}}, {{University}} of {{Alberta}}, {{Canada}}}
}

@inproceedings{ZSQ+17,
  title = {Pyramid {{Scene Parsing Network}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhao, H. and Shi, J. and Qi, X. and Wang, X. and Jia, J.},
  year = {2017},
  month = jul,
  pages = {6230--6239},
  publisher = {{IEEE Computer Society}},
  address = {{Los Alamitos, CA, USA}},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.660},
  abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
  keywords = {automobiles,convolution,feature extraction,image segmentation,neural networks,semantics}
}
