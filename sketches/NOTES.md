# Notizen

- Graphentheorie: ein Algorithmus ist in der Lage, zwischen verschiedenen Punkten den kürzesten Weg zu finden. Eine KI kann unter BBerücksichtigung zusätzlicher MErk-male herausfinden, ob es einen Weg gibt, den bereits andere mit ähnlichen Merkmalen bevorzugen, und braucht die Berechnung unit durchführen (Big Data) - siehe [ERT20], Seite 11, "Hybride Systeme": §neuronale Netze einsetzen, um Heuristiken zur Reduktion riesiger kombinatorischer Suchräume bei der Suche nach Beweisen zu lernen" (informierte Suche?)

- Definition KI Elaine Rich: Artificial Intelligence is the study of how to make computers do things at which, at the moment, people are better

- Schachcomputer: Züge vorausberechnen könnte mittels aller möglicher Kombinationen in endlicher(?) Zeit "gebruteforced" werden, ist ein Computer aufgrund solcher Berechnung intelligent, oder liegt die Intelligenz in dem Algorithmus, damit bei dem, der den Algorithmus entwickelt hat, auch, damit der Computer selbständig lernt? Dann ist KI immer nur ein Abbild von dem, zu dem Mensch gerade wissenschaftlich in der Lage ist, und so hat die Definition von Elaine Rich ihre Berechtigung.

- Welchen Stand hat die Neurowissenschaft/ Philosophie gegenüber Lernen / Bewußtsein?

- probabilistisches Schließen: Welche Rolle hat der Nicht-Determinismus?

- KI übernimmt Arbeiten: Mehr Ressourcen für die KI werden benötigt - welche Auswirkungen hat das auf die Umwelt?

- Sinnhaftigkeit KI: Siehe Spiegel Drohne greift General an. In welchen Dienst stellen wir die KI? Medizin? Militär? Das eine vernichtet und schützt Leben, das andere schützt Lebe. Siehe auch Hawking Zitat reddit: "Please encourage your students to think not only about how to create AI, but also about how to ensure its beneficial use".

- Wie sicher ist KI im Verkehr? Weniger Unfälle durch weniger Emotionen & Unkonzentriertheit? Dann wären die Algorithmen, de ein Auto steuern besser als der Fahrer (ähnlich bildgeb. Verfahren, bei denen Fehlerquote geringer ist als der von Medizinern)

- Medizin vs KI: Menschen können KI bald nicht mehr verstehen, siehe [ERT20, 9.10.1] - Medizin basiert bei manchen Medikamenten auch auf Statisten, Medikamente werden zugelassen, ohne das Nebenwirkungen komplett erstanden sind

- Sicherheit in der KI wird relevant - bspw, wenn Verkehr Fahrzeugführung übernommen wird, Medizin möglich? KI gestützte Systeme - Netzwerkangriff - Krankenhaus kann nicht weiterarbeiten oder nur auf Reserve

- Agent: System, das Informationen verarbeitet und aus Eingabe Ausgabe produziert

- unterschied Reflex Agent / Agent mit Gedächtnis: Wenn Agent seinen Ort und die Zeit kennt, kann er Geschwindigkeit nicht bestimmen, Agent mit Gedächtnis kann das, wenn er in kurzen Zeitabständen Ort speichert und abgleicht - Heisenbergsche Unschärferelation: Entweder Ort oder Geschwindigkeit bestimmen - ggf. Analogie

- Architektur wissensbasierter Systeme: Welches Model eignet sich hier? Gibt es Architekten, die sich speziell um die Planung KI basierter Systeme kümmern? (ggfl. E Wolff?)

- wie trennen wir KI von Intelligenz Menschen: Stichwort wissensbasierte Systeme: Wissen ist vorgegeben, Abrufbar für Computer, ähnlich Lexikon für Menschen. Wie kann ein Computer nun Schlüsse zwischen Aussagen herziehen? Auch: Wenn Computer sich nur auf KB (Knowledge Base) stützt, geschieht dort eine Trennung zu dem Menschen (Bewusstsein, Instinkte, Reflexe und Lernen, während Computer "nur lernen")

- KI Medizin: Ist nur Analyse möglich/ sinnvoll, oder auch beratende KI (Stichwort Expertensysteme); welche Rolle spielt dann bspw. Natural Language Processing

- In welchem Kontext steht bei KI das Verstehen der menschlichen Sprache (Anfragen verarbeiten in natürlicher Sprache statt Formalismen) und das Beantworten in menschlicher Sprache (statt in Formalismen)? Hier ggf. interessant Komplexität und Aufwand für das Umsetzen einer Sprache <-> zu beschreiben (Turing, Automat?, Eingabe, Alphabet)

- Abgrenzung: Reagiert ein System schneller als ein Mensch - ist es dem Menschen dann überlegen, oder sind die Art seiner Reaktionen (ausschließlich) wesentlich für die Einschätzung der Intelligenz einer KI - bsp. KI Rate-show Watson Jeopardy: Schnellere Reaktion - eher verstanden und deshalb schneller erdrückt oder gleich schnell verstanden wie der Mensch und Reaktionszeit maschinell begünstigt?

- KI kann nur mit Daten arbeiten. Wo kommen die Daten her, wie sieht es mit Datenschutz aus. Bsp USA, Daten müssen öffentlich sein wenn finanziell Unterstützt vom Staat (KI Lectures an der LMU, Folge 2) - können wir die Daten garantieren?

- Gibt es in der Wirtschaft Lobbyisten, die gegen KI in der Medizin sind?

- Wer trägt Verantwortung für KI? Programmierer? Datenlieferant? Unternehmen, das KI einsetzt?

- Differenzierung KI Kognitionswissenschaften

- Time Sharing wg geringer Rechenleistung: MMM?

- wie skaliert KI?

- KI und Silver Bullet?

- Die Rolle der Mathematik in der KI, ggf. Kapitel

- Windows normative Diagnosen - Expertensystem zur Fehlerkorrektur (ggf. Parnas, Undesired Events)

- Reintegration, KI? (RN, p 51)

- KI-Technologien ist bereits seit langem in Suchmaschinen, Empfehlungssystemen etc.

- Minsky et al, 2004: KI sollte zurück zu ihren Wurzeln "Maschinen die denken, die lernen und kreativ sein können" (RN, p51)

- Artificial General Intelligence sucht nach einem universellen Algorithms  für das Lernen und Handeln in einer beliebigen Umgebung

- Welches Ziel hat KI? Besser als Mensch zu werden? Turing: Modellieren wir doch KI erstmal als kleines Kind"

- KI: Internet als Datengrundlage: neueste Arbeiten zeigen daß große Datengrundlage effektiver als gute Algorithmen

- FAIR (?), standardisierte Daten; Problem Freitext in Befunden, Herausforderung für KI; elektronische Patientenakte als mögliche Lösung; bzgl. standardisierte Verfahren: Blaue Buch, Beschreibung Modellfindung für Chemie/Biologie
- KI: finde Waldo? 

- ethische Herausforderungen; Ethik: Normative Reflexionswissenschaften; kann KI zu einem GUTEN Leben beitragen; Verantwortung: Differenzierung Ethik/Juristisch

- Russel Norvig Umgebung Bildanalyse deterministisch p. 72, medizinische Diagnose

- Problem bei Klassifikation mit vielen Features: Wie läßt sich bspw Wetter klassifizieren?

- COVID Data Analysis Goup LMU

- narziss kunstinstallation, Computer vor Spiegel: https://christianmioloclair.com/narciss

- Domain specific machine learning: Medizin, Biologie, Physik, Geowwissenschaften, Humane KI

- KI unterscheidung menschliches Handeln, denken, siehe erste Vorlesung Clausthal

- unterschied übersetzung intelligence deutsch / englisch ("artificial intelligence", intelligence ~ Intelligenz)

- Ertel beschreibt in 7.3.7 die Erfahrungen mit LEXMED in der PRaxis und geht u.a. darauf ein, daß sich so ein System in D schwierig vermarkten läßt. Diverse Gründe sind dem Absatz zu entnehmen, darunter u.a. die konservative Lehre und das Mißtrauen der Patienten

- Bayes Netz Beispiel Alarm original in Russel und Norvig, bei Ertel in 7.4.1

- Ertel, Kap. 8, Lernen mit Bayes Netzen

- Ertel, in 7.5: Scores veraltet, zu schwach? Lernende Modelle besser?

- ist ein Lichtschalter schon KI? Sensoren registriere, Aktuatoren reagieren.

- "Beim Perzeptron wird das in den Trainingsdaten vorhandene Wissen extrahiert und in komprimierter Form in den Gewichten w_i gespeichert." [ERT21, p.217] 

- Punkt, um Fehlerminimum auf den Testdaten zu finden: Kreuzvalidierung (cross validation) [ERT21, p.246]

- Zweiklassenproblem zur Vorstellung eines Perzeptors, ggf. auch Lernen von Entscheidungsbäumen

- Perzeptron ist ein feed-forward Netzwerk mit einer Schicht

- Konvergenztheorem von Rosenblatt: Alle Funktionen, die mit Perzeptronen dargestellt werden können, können mit Lernalgorithmus gelernt werden kann.

- Welche Heuristiken gibt es zur optimalen Bestimmung von Knoten in einem neuronalen Netzwerk

- Qualität eines gelernten Modells ist gegeben durch dessen generalisierungsfähigkeit auf neue Daten, hängt stark von den Trainingsdaten ab [ERT21, p.249]. Siehe auch Bilderkernnung, Rotation um bestimmte Gradmaße zur Veränderung der Trainingsdaten: VErrauschen von Daten, um Overfitting zu vermeiden: Translationsinvarianz, Rotationsinvarianz (auch [ERT21, p 249])

- Zitat Turing Lernen Kind / Erwachsen: Imitiation Game oder Computing Machinery and Intelligence?

- Normalisierung von Daten wichtig für maschninelles Lernen, z.b Schrifterkennung: einfarbiger Hintergrund, Schrift dann andersfarbig und stärker saturiert je nach Druckstärke Stift, einzelne Buchstaben auf gewisse Größe, e.g. 64 x 64

- Recherche: müssen für neuronale Netze Daten normalisiert werden?

- KNIME (Konstanz Information Miner) als Tool für Data Mining, MLOSS.org, frei verfügbare Software für maschinelles Lernen

- Zwei der besten Algorithmen zum Lernen mit Lehrer: Xboost und Random Forest [ERT21, p.273]

- grundstein KI neuronale Netze [ERT21, p285]: 1943 McCulloch und Pitts "A logical Calculus of the ideas immanent in nervous activity" [Anderson, Rosenfeld: Neurocomputing: Foundations of Research, MIT Press, 1988]

- In fast allen Gebieten drer KI wird versucht, kognitive Prozesse nachzubilen, etwa in der Logik oder beim pobabilistischen Schließen [ERT21, p. 285]

- Hopfield Netz: Anzahl Neuronen == Anzahl Bildpunkte wenn Autoassoziativspeicher genutzt wird für Mustererkennung? Auch: Phasenübergang geordneter Musterererkennung hin zu chaotische Dynamik: Wie lange arbeiten Hopfield-Netze zuverlässig, d.h. bei wievielen gespeicherten Mustern?

- Boltzmann-Maschine mit stetigen Aktivierungswerten und probabilistischen Aktualisierungsregeln für die Netzwerkdynaik, damit Netz nicht in einem lokalen Minimum hängenbleibt: simulated annealing - auch hier Bezug Physik <-> mathematisches Modell KI Netz, siehe Hopfield Netze und Spins [ERT21, p297]

- Teuvo Kohonen: Pionier auf dem Gebiet Assoziativspeicher

- neuronale Netze: Unterschiedliche Speichermodelle

- Ab wann enbthält ein neuronales Netz keine Information mehr? Bsp.: binäre Hebb-Regel, [ERT21 p. 302] solange Gewichtsmatrix dünn besetzt ist geht Verfahren gut, aber je mehr unterschiedliche Muster gespeichert werden, desto voller wird die Matrix unddie Antworten enthalten keine Informatione mehr: Speichereffizienz, formale Definitionen recherchieren

- Optimale Gewichtsmatrix in einem Netz minimiert mittleren Fehler. Wie findet man das Optimum [ERT21, p.305]

- Intelligenz: Möglichst gut generalisieren, aus Fehlern lernen, keine Überanpassung [ERT21, p.306]

- Batch-Lernverfahren vs. inkrementelles Lernen: Neue Trainingsdaten müssen mit allen anderen Trainingsdaten wiederholt eingespeist werden, inkrementelles erlaubt einzelnes hinzufügen

- Lernrate: Konstante, große Lernrate konvergiert schneller, erhöht aber Risiko für Oszillation  um minima oder flache Täler [ERT21, p.310] (Oszillation: Siehe auch Gradientenabstiegsverfahren http://www.neuronalesnetz.de/backpropagation5.html)

- Backpropagation-Algorithmus geht aus der inkrementellen Delta-Regel hervor [ERT21, p. 313], bekannt durch Rumelhart, Hinton, Williams: Learning Internal Representations by Error Propagation (in: Rumelhart, McClelland: Parallel Distributed Processing, MIT Press, 1986)

- Nettalk: Ein Netz lernt Sprechen [ERT21, p.317] - hier besonders interessant die lange Rechenzeit für das Trainieren des Netzes sowie die Beobachtungen des Forcherteams; Recherche: JNNS (Java Neural Network Simulator)

- Problematisch bei Back-Propagation:  bei vielen Gewichten udn TRainingsdaten konvergiert das Netz zu lokalen Minima der quadratischen Fehlerfunktion, Konvergen oft sehr langsam. Oszillationen können vermieden werden, durch das langsame Verkleinern der Lernrate, oder durch Verwendung eines Impulsterms; mit Hilfe von ReLU kann Konvergenz schneller erreicht werden [ERT21, p. 319-321] 

- KI nicht einzige Herausforderung, auch Winnsensingenieurs müssen sinnvolle Traininsdaten erarbeiten und beisteuern

- Deep Learning: klasse von Algorithmen wie Convolutional Neural Networks oder Deep Belief Networks: Netzwerke mit bis zu 1000 Schichten, Übersicht: deeplearning.stanford.eu

- Mustererkennung ist leicht in niedrigdimensionalen Räumen oder wenn Klassen linear separabel sind (Zweiklassenproblem): Sonst treten Probleme auf, denn Lernen stellt schwieriges nichtlineares Optimierungsprobelm dar [ERT21, p. 322]; Backpropagation und Gradientenabstiegsverfahren sind Lösungen, aber wegen Konvergenzproblemen und Rechenzeiten wurde nach anderen Verfahren gesucht

- Deep Learning Architektur: Unsupervises Learning & Supervised Learning (Backpropagation)

- Das Finden der optimalen Metaparameter  für einen Lernalgorihmus kann durch kreuzvalidierung erfolgen [ERT21, p.326]

- keine Theorie für Deep Learning Netze: Verweis in [ERT21, p. 327] auf https://math.ias.edu/tml/dlsagenda

- Deep Residual Learning erlaubt das effiziente Trainieren von bis zu 1000 Schichten mit minimalem Fehler (ZHANG, REN, SUN: Deep residual learning for image recognition (2016))

- Image Net Datenbank: 14 Mio Fotos mit Objekten aus 21.000 Klassen, alle manuell gelabelt

- semantische Segmentierung von Bildern: Aufteilen eines Bildes in Bereiche mit unterschiedlicher Bedeutung (bspw. autonomes Fahren, Verkehrslage: Auto, Ampel, Bürgersteig...) - relevant für Medizin?

- Recherche: CNN gut bei Objekterkennung, Rolle in der Medizin?

- Tensorflow: frei verfügbares SOftware System für Deep Learning

- Generative Adversarial Networks können Bilder auf Basis von textueller Beschreibung generieren, semantische Segmentierung kann erzeugt werden, Objekte können ausgetauscht werden; Generatornetz <-> Diskrimonatornetz im Wechselspiel

- GPT ist ein Transformernetz: UL bekommt viele Texte als Eingabe für das Training, um Sprachmodell zur Vorhersage eines nächsten Wortes zu lernen (siehe RADFORD, NARASIMHAN, SALIMANS, SUTSKEVER: Improving language understanding by generative pre-training, 2018)

- Gerichtete neuronale Netze mit nur einer Lage von Gewichten sind linear; Gefahr overfitting gering; wenn Klassen nicht separabel sind, kommen Backpropagation in mehrschichtigen Netzen zum Einsatz: Konvergenzprobleme, Overfitting, lokale Minima [ERT21, p.340]

- bzgl. Klassifikationsaufgaben sind auch die SVM (Support Vektor Machinen bzw. Kernel Methoden) interressant, die mittlerweile auch auf Regressoinsprobleme anwendbar sind

- Perzeptron, Delta-Regel, Backpropagation udn Deep Learning: Feed Forward Netze, Klasse neuronale Netze

- Prädikatenlogik erlaubt es, Relationen zu formalisieren

- Deep Learning erstellt bessere Diagnosen als Ärzte [ERT21, p. 343]: ESTEVA, KUPREL, NOVOA: Dermatologist-level classificarion of skin cancer with deep neural networks, Nature 542 (2017); LITJENS, SANCHES, TIMOFEEVA: Deep Learning as a toll for increased accuracy and efficiency of histopathological diagnosis, Nature Scientific Reports 6 (2016) 

- Zweiklassenproblem: Daten werden in pos/neg halbraum eingeteilt, Gewichtsvektor steht hierbei senkrecht auf der Trenngerade, warum? (Beweis) w*x = 0 

- cosinusabstand als mass ähnlichkeit daten und die Bedeutung desselben in den verschiedenen Modelle 

- ix.de/zq3t quellen ki software: auch modelle für verschiedene Wissenschaftsdisziplinen, also domänenspezifisch


# Struktur
- Vorwort: Plötzlich ist KI allgegenwärtig - Deus Ex machina - vs. wird uns die KI vernichten
- "Wir müssen wissen, wir werden wissen" als Einleitung
- Hobbes Zitat "künstliches Tier"
- verwendete KI (spielt Natural Language Processing eine Rolle? Welche VErfahren sollten vorgestellt werden, mit denen dann eine Vorstellung der verwendeten KI als Technologie eingergeht?)
- Russell Norvig Einordnung Zitat Rich (menschliches Handeln)
- Klassifizierung, Prädikatenlogik
- Einleitung: Bild mit vielen Details, aus dem eins hervorsticht: Frage: Wie kann ein Computer wissen, daß es sich bei der
Abbildung um ein Bild handelt, das etwas ganz bestimmtes Zeigt, ohne, daß sich KI bei der Analyse in Details verliert? "Man präsentiert etwas" anstatt "Leute schauen Mann zu" - obwohl beides richtig ist, gibt es ja nur eine Kernaussage; genauso wird das ja auch Kindern beigebracht, wenn ihnen etwas gezeigt wird, es folgt ja immer eine Erklärung => Machine Learning
