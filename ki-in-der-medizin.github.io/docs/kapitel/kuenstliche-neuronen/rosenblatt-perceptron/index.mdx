---
id: index
title: "2.2 Das Rosenblatt-Perzeptron"
description: Neuronenmodell nach Frank Rosenblatt
---

Donald O. Hebb, 1904 - 1985, Kandischer Psychologe der die Effekte von Gehirnschäden auf Intelligenz unetrsuche,
 - war fasziniert vond er Art und Weise wie Menschen lernen und Informationen speichern
 - Hauptforscuhungsgebiet war wie junge und ältere Gehirne Informationen berarbeiten
 - einer der einflusreichsten behavioristen der Psychologie im 20 Jahrhundert
 - geboren in Cheser, Nova Scotia, 22 Julie 1904, starb 1985 ebenda. [Str01, S. 298 ff.]

His view of psychology as a biological science and his
neuropsychological cell-assembly proposal rejuvenated
interest in physiological psychology. [Ray99, S. 1]

The
writings of James, Freud, and Watson stimulated his interest
in psychology, and as a part-time graduate student at McGill
University, Hebb was exposed to Pavlov's program. [Ray99, S. 1]

where he explored the
impact of brain injury and surgery, particularly lesions of
the frontal lobes, on human intelligence and behaviour [Ray99, S. 1]

Hebb's studies of intelligence
led him to the conclusion that experience played a much
greater role in determining intelligence than was typically
assumed (Hebb, 1942).[Ray99, S. 1]

His book, The Organization of Behavior: A
Neuropsychological Theory, wielded a kind of magic in the
years after its appearance (Hebb, 1949). It attracted many
brilliant scientists into psychology, made McGill University
a North American mecca for scientists interested in brain
mechanisms of behaviour, led to many important discoveries, and steered contemporary psychology onto a more
fruitful path.
For Hebb "the problem of understanding behavior is the
problem of understanding the total action of the nervous
system, and vice versa," (1949, p. xiv) and his advocacy of an
interdisciplinary effort to solve this neuropsychological
problem was his most general theme.  [Ray99, S. 1]

and
the computer metaphor of information processing psychology were all fruitful to a point, but then limited and misleading. Hebb's appealingly simple alternative was to explain
human and animal behaviour and thought in terms of the
actual device which produces them — the brain — and in
The Organization of Behavior, Hebb presented just such a neuropsychological theory.
There were three pivotal postulates: [Ray99, S. 1, ff.]


1. Connections between neurons increase in efficacy in
proportion to the degree of correlation between pre- and postsynaptic activity. [...]
In Neuroscience this proposal corresponds to the "Hebb
synapse," the first instances of which were later discovered in
long-term potentiation (Bliss & L0mo, 1973) and kindling
(Goddard, Mclntyre, & Leech, 1969), whereas in Cognitive
Science this postulate, often called the "Hebb rule," provides the
most basic learning algorithm for adjusting connection weights
in artificial neural network models. [Ray99, S. 2]

2.  Groups of neurons which tend to fire together form a cellassembly whose activity can persist after the triggering event and
serves to represent it. [Ray99, S. 2]

3. Thinking is the sequential activation of sets of cellassemblies [Ray99, S. 2]


A precis of Hebb's theory is presented in the Introduction
to The Organization of Behavior: "Any frequently repeated,
particular stimulation will lead to the slow development of
a "cell-assembly," [Ray99, S. 2]

Hebb
fulfilled this promise and laid the foundation for
neoconnectionism which seeks to explain cognitive processes in terms of connections between assemblies of real or
artificial neurons. [Ray99, S. 2]

Hebb's ideas within psychology and behavioural neuroscience will grow to match the stature of
Darwin's ideas within biology.  [Ray99, S. 2]

In the scientific literature references to Hebb, the
Hebbian cell-assembly, the Hebb synapse and the Hebb
rule, increase each year. These forceful ideas of 1949 are now
applied in engineering, robotics, and computer science as
well as neurophysiology, neuroscience and psychology [Ray99, S. 3]

dAMIT GILT hEBB ALS eNTDECKER DER SYNAPTISCHEN pLATIZITÄT:

Daher bezeichnet man Synapsen, die auf diese Weise modifiziert werden können, als Hebb-Synapsen und derartige Veränderungen als Hebb’sches
Lernen. [BCP18, S. 876 Abs. 2]

"Grundlegende Mechanismen der cortikalen Plastizität" [BCP, S. 882]
1. Wenn das präsynaptische Axon aktiv ist und gleichzeitig das postsynaptische Neuron
unter dem Einfluss weiterer Eingänge stark aktiviert wird, wird die vom präsynaptischen Axon gebildete Synapse gestärkt. Dies ist nur eine andere Formulierung der
bereits erwähnten Hypothese von Hebb. Mit anderen Worten: Synchrone neuronale
Aktivität verstärkt die Kopplung.

-> Hebb’sches Lernen Die Effizienz einer Synapse erhöht sich, [BCP Glossar, S. 985 & 986]
wenn prä- und postsynaptische Neuronen gleichzeitig aktiviert
werden.
-> Eine Synapse, die Hebb’sches Lernen zeigt.

2. Wenn das präsynaptische Axon aktiv ist und gleichzeitig das postsynaptische Neuron
unter dem Einfluss weiterer Eingänge nur schwach aktiviert wird, wird die vom prä-
synaptischen Axon gebildete Synapse geschwächt. Mit anderen Worten: Asynchrone
neuronale Aktivität vermindert die Kopplung

 Cell Assembly [BCP18, "Hebb und der Neuronenverband"S. 907-908]
 -  Hebb stellte sich vor, dass all diese Zellen reziprok miteinander verschaltet seien.
Die interne Repräsentation des Objekts würde so lange im Kurzzeitgedächtnis behalten,
wie die Aktivität durch die Verschaltungen in dem Neuronenverband zirkuliert. Weiterhin
stellte er die Hypothese auf, dass es, sofern die Aktivierung des Neuronenverbands nur
lange genug andauere, zu einer Konsolidierung durch einen „Wachstumsprozess“ käme,
durch den die reziproken Verschaltungen effektiver werden; Neuronen, die gemeinsam
feuern, würden auch gemeinsam verschaltet (Abb. 24.11b). Folglich würden, wenn nur
ein Bruchteil der Zellen des Verbands durch einen späteren Reiz aktiviert wird, die nun
verstärkten reziproken Verschaltungen bewirken, dass der gesamte Verband wieder aktiviert wird

Paper [CPL1978, Siehe KSJ+13, S. 1269 Figure. 56-8]

- Hebb Synapse [BLS19, S. 831]
- Arbeitsweise von Hebb Synapsen, Blinzeln, Ton, Luftstoß (Konditionierung),  [BLS19, S. 832]

# 0 Todos
 - Heaviside (Treppenfunktion) mit Bezug auf 2.1.3.1 Aktivierungs- und Eingabefunktion, Gl. 2.1.3
 - Auf Plastizität zurückkommen (angemerkt in 2.1)

Einführung: Hebb 1949, Organization of Behavior, Lernregel, synaptische PLastizität, "fire togteher, wire together".
Auf dieser Grundlage dann Modell Rosenblatt, Synapsen gewichte

Heb ostuliert "cell assemblies" in dem Buch, selbstverstärkende cluster von neuronen die informstionen repräsentieren im Nervensystem, einzelne Zellen können zu mehreren Clustern gehören AR88, S. 44

> Perceptrons are neural nets that change with “experience,” using
an error-correction rule designed to change the weights of each
response unit when it makes erroneous responses to stimuli that
are presented to the network. [ARB02, S.20]

> In
other words, the simple perceptron can only compute a linearly
separable function of the pattern as provided by the associator
units.  [ARB02, S.20]

> The answer was “Yes: if the patterns are
linearly separable, then there is a learning scheme which will eventually yield a satisfactory setting of the weights.” The best-known
perceptron learning rule strengthens an active synapse if the efferent neuron fails to fire when it should have fired, and weakens an
active synapse if the neuron fires when it should not have done so:  [ARB02, S.20]

> Supervised learning adjusts the weights in an attempt
to respond to explicit error signals provided by a “teacher,” which
may be external, or another network in the same “brain.” This
model was introduced in the perceptron model, which is reviewed
in PERCEPTRONS, ADALINES, AND BACKPROPAGATION (of which
more details in the next road map, Grounding Models of Networks).  [ARB02, S. 30]

Evtl. interessant: Perceptrons, Adalines, and Backpropagation Arb02, S. 871

#### Convergence Proof

> It appearedin two later papersin the Reviewsof ModernPhysics . Those
paperscontained an attempt at a formal proof of the convergence. In 1961-
62 AI Novikoff recognizedthe essentialsimilarity between linear threshold
elementsand the algorithm, and linear programming, and produced a very
elementary but neat proof by contradiction that it converged in a finite
numberof trials. Novikoff 's proof madeclearwhat wasgoing on and should
have
, actually, triggered a lot of work. [AR98, S. 100]

### AI Winter
> These talks also mention the controversies that surrounded the rise and
fall of public and scientificinterest in Rosenblatt 's perceptron, as well as the
influence of Marvin Minsky and Seymour Papert on the loss of interest
in neural networks during the 1970s [AR98, S X]

### Notes
> Here was a machinethat could do pattern recognition
in a humanlike way; it could recognize all kinds of things. Almost
everyoneat MIT was very skeptical . [AR98, S.99]

> Bernard Widrow (born December 24, 1929) is a U.S. professor of electrical engineering at Stanford University.[1] He is the co-inventor of the Widrow–Hoff least mean squares filter (LMS) adaptive algorithm with his then doctoral student Ted Hoff.[2] The LMS algorithm led to the ADALINE and MADALINE artificial neural networks and to the backpropagation technique. He made other fundamental contributions to the development of signal processing in the fields of geophysics, adaptive antennas, and adaptive filtering.