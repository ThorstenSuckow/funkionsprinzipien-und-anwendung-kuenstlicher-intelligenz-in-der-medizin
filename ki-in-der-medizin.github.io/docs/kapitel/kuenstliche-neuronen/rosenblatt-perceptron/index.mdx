---
id: index
title: "2.2 Das Rosenblatt-Perzeptron"
description: Neuronenmodell nach Frank Rosenblatt
---

import {R, F, S, EQ, Def, Tbl, initEq} from "../../../../src/components/Refs.js";

#### 0 Todos
 - Heaviside (Treppenfunktion) mit Bezug auf 2.1.3.1 Aktivierungs- und Eingabefunktion, Gl. 2.1.3
 - Normalenvektor steht orthogonal auf der Gerade

## 2.2.1 Cells that fire together, wire together: Synaptische Plastizität

Donald Hebb (1904 - 1985), gebürtiger Kanadier, Sohn eines Ärzte-Elternpaares und 1965 nominiert für den Nobelpreis <R>BM03, S. 1014</R>, geht als junger Mann einer Karriere als Schriftsteller nach. Er studiert Englisch als Hauptfach und macht 1924 seinen Bachelor[^1] <R>Coo05, S.852</R>, doch die Schriften Freuds, mit denen er sich nach seinem Abschluss beschäftigt, wecken in ihm den Wunsch, sich tiefer mit Psychologie zu beschäftigen <R>BM03 S. 1013</R>: An der McGill Universität in Montreal[^2] macht er 1932 seinen Master darin[^3], und leitet dort 16 Jahre später als Professor die Fakultät für Psychologie <R>Coo05, S. 853</R>.

Seine Faszination darüber, wie das Gehirn lernt, Informationen verarbeitet und speichert <R>Str01, S. 298 ff.</R> wird Bestandteil seiner Forschungsarbeit: 1949 veröffentlicht er das Buch "**The organization of Behavior: A Neuropsychological Theory**" <R>Heb49</R>; seine darin formulierten Postulate[^4] <R>Kle99, S.2 f.</R> liefern einen wichtigen Beitrag für die Neurowissenschaften[^5]. Oft zitiert wird seine Idee bzgl. synaptischer Verstärkung, was heute als **Hebbsche Synapse** bekannt ist <R>AR88, S. 4, Abs. 5</R>[^6]:

> _When an axion of cell_ A _is near enough to excite a cell_ B _and repeatedly or persistently takes part in firing it, some
growth process or metabolic change takes place in one or both cells such that_ A_'s efficiency, as one of the cells firing_ B_, is increased._ <R>Heb88, S.50, Hervorhebung i.O.</R>

Derartige Veränderungen synaptischer Verbindungen wird als **Hebbsche Lernregel** <R>BCP18, S.985, "Hebb'sches Lernen"</R> bezeichnet. Das damit verbundene geflügelte Wort _Cells that fire together, wire together_[^7] beschreibt die Hypothese bildhaft. Seine Idee der "**Cell Assembly**" (siehe [^6], Zitat _b_) schließt daran an: Damit sind Verbände von Neuronen gemeint, die miteinander verschaltet sind, und deren Verbindungen durch das Hebbsche Lernen so sehr verstärkt sind, das die Aktivierung einzelner Zellen in diesen Verbänden ausreicht, das alle Zellen aktiviert werden <R>BCP18, S. 907-908, "Hebb und der Neuronenverband"</R>.

Hebbs Theorien gelten durch die Forschung als bestätigt[^8], und mit der von Hebb formulierten synaptische Plastizität wurde auch eine Idee für lernende künstliche neuronale Netze geliefert[^9]. In **Abschnitt 2.1.4** haben wir geschlossen, dass ein MCP-Netz nur durch vorhergehende Analyse der Aufgabe und Anpassung der Topologie von Außen zur Lösung einer Aufgabe imstande ist. Hebbs Erkenntnis führt kurz vor Beginn der 60er Jahre zu einem Modell, das in der Lage ist, sich selbst anzupassen.


[^1]: an der Dalhousie Universität: https://dal.ca (abgerufen 17.08.2023)
[^2]: McGillUniversität, Montreal, Quebec (Kanada): https://www.mcgill.ca/neuro/about/donald-hebb-phd (abgerufen 16.08.2023)
[^3]: zu der Zeit studierte er in Teilzeit an der McGill Universität: "as a part time graduate student" <R>Kle99, S. 1</R>. Seine Master-Arbeit schrieb er aufgrund einer Erkrankung im Bett <R>BM03, S. 1014</R>
[^4]: es sind "three pivotal postulates" <R>Kle99, S.2 f.</R>
[^5]: als hätte sein Buch eine Art Golgräberstimmung in der der Psychologie ausgelöst, schreibt <R>Kle99</R>: "It attracted many
brilliant scientists into psychology, made McGill University a North American mecca for scientists interested in brain mechanisms of behaviour, led to many important discoveries, and steered contemporary psychology onto a more fruitful path." <R>Kle99, S.1</R>
[^6]: sowie als Zitat: _a:_ "[...] any two cells or systems of cells that are repeatedly active at the same time will tend to become “associated,” so that activity in one facilitates activity in the other." <R>Heb88, S. 52, "Mode of perceptual Integration: The Cell-Assembly"</R> sowie _b:_ "A series of such events \[Aktivierung von "cell-assemblies"\] constitutes a “phase sequence”—the thought process. <R>Heb88, S. 48, "Mode of perceptual Integration: The Cell-Assembly</R>; die Aussage findet sich im Original <R>Heb49</R> bereits in "Introduction", S. xi-xix
[^7]: Zumindest in <R>Heb49</R> findet sich kein solches Zitat. <R>KG14, S. 2</R> behauptet: "This mnemonic phrase was first introduced by Carla Shatz \[12\] in an article for the Scientific American aimed at lay public" und meint damit den Satz "In a sense, then, cells that fire together wire together." in <R>Sha92, S. 94</R>. https://en.wikipedia.org/wiki/Hebbian_theory#cite_ref-2 (abgerufen 16.08.2023) hingegen schreibt den Ursprung <R>LS92</R> zu: "neurons wire together if they fire together." <R>LS92, S.211</R>
[^8]:  vgl. <R>BLS19, S.833, Abs. 2 f.</R>, außerdem <R>YLC+14</R> und <R>BL73</R>. <R>BPC18, S. 875, Exkurs 23.5</R> verweist auf <R>Cop78</R>
[^9]: \[Hebb\] laid the foundation for neoconnectionism which seeks to explain cognitive processes in terms of connections between assemblies of real or
artificial neurons. <R>Kle99, S. 2</R>


## 2.2.2 Das Perzeptron - ein linearer Klassifizierer

Bereits 1954 wurden Versuche unternommen, lernfähige neuronale Netze zu modellieren <R>Ros62, S. 24, Abs. 2</R>[^10]. Erst 1958 schafft es ein Modell, eine Sensation auszulösen: Das **Perceptron** (im folgenden "Perzeptron") <R>AR88, S.90</R>[^11]. 1957 beschreibt es sein Schöpfer Frank Rosenblatt (1928 - 1971) in <R>Ros57</R> als Teil eines internen Forschungsprojektes des _Cornell Aeronautical Labors_: Die Forschungseinrichtung gehörte von 1946 bis 1972[^12] zu der Cornell Universität[^13], an der Rosenblatt 1950 seinen A.B und 1956 seinen Ph.D. gemacht hatte, und an der er bis zu seinem Lebensende[^14] als Psychologe und Neurobiologe forschen und lehren wird <R>Ehb71</R>.

#### "The Perceptron - A perceiving and recognizing automaton"
Das Forschungsprojekt "_Perceiving and Recognizing Automaton_" beschreibt einen Apparat, der mittels einer Kamera geometrische Figuren erkennen und zuordnen kann. Die Funktionen simuliert Rosenblatt zunächst auf einem IBM 704 Rechner <R>Ros60</R>, bevor  die Hardware Anfang der 60er Jahre als _Mark 1 Perceptron_ gebaut wird: 400 Cadmiumsulfid-Photozellen auf einem 20x20 großen Raster angeordnet - dem **S-System** (**S** = _Sensory_) - leiten Signale an das **A-System** (**A** = _Association_); dort werden sie registriert und ausgewertet, und schlussendlich über das **R-System** (**R** = _Response_) ausgegeben (vgl. <R>Ros57, S. 4, f.</R>, <R>Ros58 S. 389 ff.</R> sowie <R>Bis06, S. 193 "Frank Rosenblatt" sowie S. 196 Figure 4.8</R>). Dabei lernt die Maschine im ersten Schritt durch die Unterstützung der Ingenieure, wie gegebene Formen zu interpretieren sind: Für aktivierte Photozellen wird die erwartete Ausgabe manuell festgelegt. Die Verbindungen zwischen den **S**-, **A**- und **R**-Units erinnert nicht nur von der Namensgebung her an biologische Neuronen, auch deren Struktur und Verschaltung wird hier als Vorbild genommen[^15] <R>Ros62, s. 4, Abs. 2</R>.

**Abbildung 2.2.1:** Schematische Darstellung der S, A, R Units


[^10]: Rosenblatt verweist hier auf <R>CF54</R>
[^11]: vgl. auch <R>MP69, S. xix</R>: "Interest in connectionist networks revivied dramatically in 1962 with the publication of Frank Rosenblatt's book _Principles of Neurodynamics_, [...]" (Hervorhebung i.O.)
[^12]: Seit 1972 Calspan Corporation <R>BB06</R>, https://calspan.com/ (abgerufen 18.08.2023)
[^13]:  https://www.cornell.edu/, abgerufen 18.08.2023
[^14]: 1971 durch einen Bootsunfall <R>Car71</R>
[^15]: Die **S**-Units konnten sowohl hemmende als auch erregende Signale in das **A**-System einspeisen-. Darüber hinaus war das **R**-System in der Lage, über Rückkoppelungen hemmende Signale an das **A**-System zu senden: Damit sollte verhindert werden, dass weitere **R**-Units aktiviert werden, die sich mit den bereits aktivierten Units gegenseitig ausschließen. <R>Ros57, S. 4, Punkt (3)</R>


## 2.2.3 Das Modell

Rosenblatt definiert das Perzeptron wie folgt[^16]:

> A <u>perceptron</u> is a network of S, A, and R units with a variable interaction matrix _V_ which depends on the
sequence of past activity states of the network. <R>Ros62, S. 83, "DEFINITION 17" (Hervorhebung i.O.)</R>

Für die **A**-Units wird definiert:

> A <u>simple A-unit</u> is a logical decision element, which
generates an output signal if the algebraic sum of its
input signals, $\alpha_i$ , is equal or greater than a threshold
quantity, $\Theta > 0$. The output signal $a^*_i$ is equal to $+1$ if $\alpha_i \geq \Theta$ and $0$ otherwise. If $a^*_i = +1$,
the unit is said to be <u>active</u>. <R>Ros62, S. 81, "DEFINITION 9" (Hervorhebung i.O.)</R>

Ähnlichkeiten zu der in **Kapitel 2.1.3** beschriebenen Aktivierungsfunktion sind durchaus erkennbar. Rosenblatt selber weist darauf hin, dass er sein Modell direkt von dem von McCulloch und Pitts eingeführten Modell ableitet[^17]. Darüber hinaus weist er auch auf Einflüsse von Hebb und von Neumann hin <R>Ros62, S.5</R>.

Das klassische Rosenblatt-Perzeptron verwendet in einem Netz von Eingabe- und Ausgabe-Knoten gewichtete Verbindungen - die Knoten selber sind Schwellenwertelemente, Verbindungen werden stochastisch ermittelt <R>Roj93, S. 51, Abs. 3</R>. Nach Rosenblatts Veröffentlichung wurde sein Modell analysiert und verfeinert <R>Roj93, S. 51, Abs. 3</R> u. a. von Minsky und Papert in <R>MP17</R>, die wie folgt definieren:

> A <u>perceptron</u> is a device capable of computing all predicates which are linear in some given set $\Phi$ of partial predicates. <R>MP17, S. 12 (Herorhebung i.O.)</R>

Prädikate sind hier Verbindungen zu den Eingabesignalen, die einen Wahrheitswert $0$ oder $1$ basierend auf der Eingabe $X$ berechnen. Die Ausgabe der Prädikate werden individuell gewichtet und an die Zelle weitergeleitet, die die Aktivierungsfunktion implementiert (vgl. <R>Roj93, S. 52</R> und <R>MP17, S. 8-12</R>).


**Abbildung 2.2.2:** Schematische Darstellung der Eingabesignale, Prädikate, Gewichte und Schwellenwertzelle [Ro93, S. 53]


Die Eingabefunktion setzt sich dann wie **_2-1-2_** aus der Summe der Produkte der Prädikate $P_i \in \Phi$ (wobei $P_i(X) \in \{0, 1\}$) und den Gewichten $w_i \in \R$ der Verbindungen zusammen (**_2-2-1_**), und die Aktivierungsfunktion (**_2-2-1_**) ist wieder eine Treppenfunktion mit dem reellen Schwellenwert $\Theta$:

<EQ id={"2-2-1"}>

$g:= g(X) = \sum^n_{i=1} P_i(X) w_i$

</EQ>

<EQ id={"2-2-2"}>

$f:= f(x) = f(g(X)) = \begin{cases}
1 &\text{falls} &x >= \Theta \\
0 &\text{falls} &x < \Theta
\end{cases}$

</EQ>

_Minsky_ und _Papert_ weisen in ihrer Definition des Perzeptrons auf eine besondere Voraussetzung hin, die wir uns in dem nächsten Abschnitt genauer anschauen werden.

### 2.2.3.1 Lineare Trennbarkeit
Mit **_2-2-2_** folgt für eine Eingabe, dass sie entweder zu $0$ oder zu $1$ evaluiert. Wir haben es hier also wieder mit einem binären Wertebereich zu tun, den wir auch als zwei unterschiedliche Klassen <R>RN09, S. 812, Abs. 2</R> verstehen können. Eingabedaten, die zu $0$ oder zu $1$ konvergieren, sind also einer der beiden Klassen zuzuordnen. Im Folgenden wollen wir die Zusammenhänge geometrisch darstellen. Der Einfachheit halber beschränken wir uns hierzu auf den zweidimensionalen Raum $\R_+^2$ und betrachten hier die _1. Winkelhalbierende_ im 1. Quadrant des kartesischen Koordinatensystems. Die zugehörige _Gerade_ $L$ <R>Fis19, S. 18</R> ist

<EQ id={"2-2-3"}>

$L = \{(x_1, x_2) \in \R_+^2: x_1 = x_2\}$

</EQ>


**Abbildung 2.2.3:** Winkelhalbierende im kartesischen Koordinatensystem

Für beliebige Punkte $(x_1, x_2) \in \R_+^2$ gilt  offensichtlich

<EQ id={"2-2-4"}>

$x_1 - x_2 \begin{cases}
\gt 0 &\text{falls} &x_1 > x_2 \\
= 0 &\text{falls} &x_1 = x_2 \\
\lt 0 &\text{falls} &x_1 < x_2
\end{cases}$

</EQ>

Die _Gerade_ $L$ repräsentiert eine _Hyperebene_ <R>BHW+12 S. 81, "Definition 2.3"</R> in $\R_+^2$. Punkte, die nicht zu dieser Hyperebene gehören, liegen in dem Fall $\R_+^2$ in zwei unterschiedlichen _Halbräumen_[^18]. Die Halbräume und deren Trennung wird greifbarer durch die geometrische Darstellung in **Abbildung 2.2.4**. Wir können feststellen, dass

 - Punkte, die $x_1 - x_2 > 0$ erfüllen (im folgenden $M_-$) in dem Halbrum _unter_ der durch $L$ beschriebenen Gerade liegen
 - Punkte, für die  $x_1 - x_2 < 0$ gilt (im folgenden $M_+$) _über_ der durch $L$ beschriebenen Gerade liegen
 - Punkte mit $x_1 - x_2 = 0$ _auf_ der Geraden liegen (im folgenden $\subset M_-$[^19])

**Abbildung 2.2.4:** Skizzierung der durch die 1. Winkelhalbierende entstandenen Halbräume. Die Mengen $M_-$ und $M_+$ sind linear separierbar, die Gleichung für die _Trenngerade_ hierzu lautet $x_1 - x_2 = 0$

Formal ausgedrückt bedeutet das, dass $M_+$ und $M_-$ _linear separabel_ sind. Nach <R>Roj93</R> lautet die Definition für _Lineare Trennbarkeit_:

<Def>

**Definition 2.2.1[^20]**: Zwei Mengen _A_ und _B_ von Punkten in einem _n_-dimensionalen Raum sind _linear trennbar_, falls _n_+1 reelle Zahlen $w_1, ... , w_{n+1}$ existieren, so daß für jeden Punkt $x_1, ... , x_n \in A$ gilt

<EQ id="2-2-5">

$\sum^n_{i=1} w_ix_i \geq w_{n+1}$

</EQ>

und für jeden Punkt $x_1, ... , x_n \in B$

<EQ id="2-2-6">

$\sum^n_{i=1} w_ix_i < w_{n+1}$

</EQ>

</Def>

Für unser Beispiel mit den oben eingeführten Mengen $M_- = \{(x_1, x_2) \in \R_+^2: x_1 \geq x_2\}$ und $M_+=\{(x_1, x_2) \in \R_+^2: x_1 < x_2\}$ in $\R_+^2$ wählen wir $w_1 = -1, w_2 = 1, w_3 = 0$.
Dass diese Mengen nach **Definition 2.2.1** linear separabel sind, lässt sich leicht anhand einer Fallunterscheidung nachweisen:

1. Fall $x_1 = x_2$:
Es gilt $w_1x_1 + w_2x_2 = w_1x_1 + w_2x_1 = -x_1 + x_1 = w_3 = 0$. Mit $0 \geq 0$ ist somit **_2-2-5_** erfüllt.

2. Fall $x_1 < x_2$:
Es gilt $w_1x_1 = -x_1$. Addition von $w_1x_1$ auf beiden Seiten von $x_2 > x_1$ liefert $x_2 + (-x_1) = w_2x_2 + w_1x_1 > 0 = w_3$ und erfüllt **_2-2-5_**.

3. Fall $x_1 > x_2$:
Es gilt wieder $w_1x_1 = -x_1$. Addition auf beiden Seiten von $x_1 > x_2$ liefert $w_3 = 0 > x_2 + (-x_1) = w_2x_2 + w_1x_1$ und erfüllt **_2-2-6_**.

Die von <R>Ert21</R> formulierte Definition für ein Perzeptron wollen wir für die weiteren Beobachtungen übernehmen:

<Def>

**Definition 2.2.2**[^21]: Sei $w = (w_1, ..., w_n) \in \R^n$ ein Gewichtsvektor und $x \in \R^n$ ein Eingabevektor. Ein **Perzeptron** stellt eine Funktion $P: \R^n \to \{0, 1\}$ mit

<EQ id="">

$P(x) = \begin{cases}
1 &\text{falls} &wx = \sum^n_{i=1} w_ix_i >0 \\
0 &\text{sonst}
\end{cases}$

</EQ>

dar.

</Def>


### 2.2.3.2 Die Lernregel

Die Eigenschaft linearer Trennbarkeit von Daten ist eine wesentliche Voraussetzung dafür, dass ein Perzeptron _konvergiert_: Die _Lernregel_ des Perzeptrons passt während der Laufzeit die Gewichte $w_1 ... w_n$ solange an, bis sie - eingesetzt in eine lineare Gleichung - die $n$-dimensionalen Daten entsprechend **Definition 2.2.1** _klassifizieren_ kann. Aus diesem Grund wird das Perzeptron auch **linearer Klassifizierer** genannt <R>Ert21, S. 210 - 216</R>.

Das Perzeptron **lernt** diese Gewichte zunächst durch _Traningsdaten_[^22]. Jeder Eintrag dieser Trainingsdaten ist einer erwarteten Ausgabe zugeordnet.

Das Perzeptron lernt durch folgende Schritte (vgl. <R>RM87 S. 65</R> sowie <R>RN09 S. 842</R>):

1. Wähle einen Datensatz und berechne die Ausgabe
2. Wenn die Ausgabe $1$ ist, obwohl sie $0$ sein sollte (Fehler[^23] $=-1$), verringere die Gewichte
3. Wenn die Ausgabe $0$ ist, obwohl sie $1$ sein sollte  (Fehler $=1$), erhöhe die Gewichte
4. Wenn die Ausgabe korrekt ist, passe die Gewichte nicht an

Die Schritte werden so lange durchlaufen, bis für alle Trainingsdaten die Ausgabe korrekt ist, oder eine maximale Anzahl von Trainingsläufen erreicht wurde. Einen Trainingslauf nennt man dabei _Epoche_ <R>Fau94, S. 436, "Training epoch"</R>. Sind die Trainingsdaten linear separabel, _konvergiert_[^24] das Perzeptron nach einer endlichen Zahl von Epochen <R>MP17, S. 164</R>[^24], und ist danach in der Lage zu _generalisieren_ <R>Ert21, S. 202</R>.

Da wir in unserem Beispiel nur Daten betrachtet haben, die durch eine Gerade durch den Ursprung ($(0,0)$) getrennt sind, brauchen wir noch eine Möglichkeit, die $x_2$-Koordinate der Trenngerade anzupassen.

**Abbildung 2.2.5:** Daten, die nicht durch eine Ursprungsgerade separabel sind.

Dies erreicht man mit einer sogenannten **bias unit**. Das Bias-Gewicht[^26] ist ein Wert, der zu der Gleichung aus **Definition 2.2.2** hinzuaddiert wird, und für eine Verschiebung der Ursprungsgeraden[^27]
sorgt <R>Ert21, S.215</R>. Im $R^2$ bedeutet das eine Verschiebung der Trenngerade entlang der $y$-Achse.

Dies erreicht man, indem man den Eingabedaten einen fixen Eingabewert $x_{n+1} = 1$ hinzufügt: Der Eingabevektor $x \in \R^n$ wird _erweitert_: $(x_1, ..., x_n, 1)$ <R>Roj93, S. 58, "3.2.2 Gewichtete Netze mit einem kanonischen Baustein"</R>. $w_{n+1}$ wird dann in als Schwellenwert (vgl. **Definition 2.2.1**) von der Ungleichung subtrahiert, so dasss der neue Schwellenwert $0$ wird. $w_{n+1}$ wird dann von der Lernregel angepasst.

<EQ id="2-2-7">

$\sum^n_{i=1} w_ix_i = w_{n+1} * 1 \iff \sum^n_{i=1} w_ix_i - w_{n+1} = 0$

</EQ>





[^16]: Definitionen aller Zustände, Signale und Funktionen in <R>Ros62, S. 79 - 94</R>
[^17]: vgl. auch "Ein _einfaches Perzeptron_ ist eine McCulloch-Pitts-Zelle, die ihre Eingabe gewichtet berechnet." in <R>Roj93, S. 57, "Definition 3.1", Hervorhebungen i.O.</R>
[^18]: Renze, John; Uznanski, Dan; and Weisstein, Eric W. "Half-Plane." From MathWorld--A Wolfram Web Resource. https://mathworld.wolfram.com/Half-Plane.html (abgerufen 21.08.2023)
[^19]: Wenn die Hyperebene selbst im Halbraum enthalten ist, spricht man von einem _abgeschlossenen Halbraum_. (https://de.wikipedia.org/wiki/Halbraum, abgerufen 22.08.2023)
[^20]: <R>Roj93, S. 61, "Definition 3.2", Hervorhebungen i.O., Nummerierung eigene</R>
[^21]: <R>Ert21, S. 212, "Definition 8.3", Hervorhebungen i.O.</R>
[^22]: "supervised learning": überwachtes lernen; vgl. <R>RN09, S. 811, "18.2 Überwachtes Lernen"</R> sowie <R>Fau94, S. 15</R>
[^23]: Der **Fehler** ist hierbei die Differenz von $\text{erwartete Ausgabe}$ und $\text{tatsächliche Ausgabe}$.
[^24]: "iterative training processes converge if the weight updates reach equilibrium (stop changing)" <R>Fau94, S. 425, "Convergence"</R>
[^25]: Beweise hierzu in <R>Ros62, S. 111 ff.</R>, <R>MP17, S. 167 ff.</R> sowie in <R>Nov62</R>
[^26]: vgl. <R>RN09, S. 839</R>
[^27]: im $\R^n$ durch eine Hyperebene im Ursprung <R>Ert21, S.215</R>

## 2.2.3 Das Perzepton als Python-Programm

## Notes
> The perceptron is a relative newcomer to this
field,having first been described by this writer in 1957 (Ref . 78). Perceptrons
are of interest because their study appears to throw light upon the biophysics of
cognitive systems: they illustrate, in rudimentary form, some of the processes
by which organisms, or other suitably organized entitites, may come to
possess "knowledge" of the physical world in which they exist, and by which
the knowledge that they possess can be represented or reported when occasion
demands. [Ros62, S. 3]



> Simulation of a statistically
connected network to investigate possible learning capabilities was first
carried out. successfully by Farley and Clark in 1954 (Ref. 10). [Ros61, S.24]

> * The term "cell assembly" seems appropriate here, as the sets which are
formed in the terminal state of a cross -coupled perceptron bear a close
resemblence in organization and functional properties to the cell assembly
concept proposed by Hebb, in Ref. 33. [Ros62 S. 450]

> In a value -conserving cross -coupled perceptron, where there is
the possibility of developing pronounced inhibitory interaction between A-seti,
there is a tendency to develop "cell assemblies" (in Hebb's sense), and these
cell -assemblies tend to rival one another for dominance at all times. [Ros62, S. 464]

> In the recognition of a complex structured object, such as a
man (regardless of posture, angle of view, etc.) a program of observations
might note significant parts and the transitions between them. There should,
for example, be a head joined to the shoulders, and by following a path from
one of the hands, the system should successively come to a forearm, shoulder,
and torso. The reader may recognize a similarity between this suggestion
and Hebb's concept of a "phase sequence" (Ref . 33). [Ros62, S. 506]

Bei der MCP-Zelle hatten wir eine Kantengewichtsfunktion eingeführt, die je nach hemmender oder erregender afferenter Verbindung einen positiven oder negativen Wert zurückliefert (s. **Definition 2.1**). Für eine solche Kantengewichtsfunktion müßte nun



Einführung: Hebb 1949, Organization of Behavior, Lernregel, synaptische PLastizität, "fire togteher, wire together".
Auf dieser Grundlage dann Modell Rosenblatt, Synapsen gewichte

Heb ostuliert "cell assemblies" in dem Buch, selbstverstärkende cluster von neuronen die informstionen repräsentieren im Nervensystem, einzelne Zellen können zu mehreren Clustern gehören AR88, S. 44

> Perceptrons are neural nets that change with “experience,” using
an error-correction rule designed to change the weights of each
response unit when it makes erroneous responses to stimuli that
are presented to the network. [ARB02, S.20]

> In
other words, the simple perceptron can only compute a linearly
separable function of the pattern as provided by the associator
units.  [ARB02, S.20]

> The answer was “Yes: if the patterns are
linearly separable, then there is a learning scheme which will eventually yield a satisfactory setting of the weights.” The best-known
perceptron learning rule strengthens an active synapse if the efferent neuron fails to fire when it should have fired, and weakens an
active synapse if the neuron fires when it should not have done so:  [ARB02, S.20]

> Supervised learning adjusts the weights in an attempt
to respond to explicit error signals provided by a “teacher,” which
may be external, or another network in the same “brain.” This
model was introduced in the perceptron model, which is reviewed
in PERCEPTRONS, ADALINES, AND BACKPROPAGATION (of which
more details in the next road map, Grounding Models of Networks).  [ARB02, S. 30]

Evtl. interessant: Perceptrons, Adalines, and Backpropagation Arb02, S. 871

#### Convergence Proof

> It appearedin two later papersin the Reviewsof ModernPhysics . Those
paperscontained an attempt at a formal proof of the convergence. In 1961-
62 AI Novikoff recognizedthe essentialsimilarity between linear threshold
elementsand the algorithm, and linear programming, and produced a very
elementary but neat proof by contradiction that it converged in a finite
numberof trials. Novikoff 's proof madeclearwhat wasgoing on and should
have
, actually, triggered a lot of work. [AR98, S. 100]

### AI Winter
> These talks also mention the controversies that surrounded the rise and
fall of public and scientificinterest in Rosenblatt 's perceptron, as well as the
influence of Marvin Minsky and Seymour Papert on the loss of interest
in neural networks during the 1970s [AR98, S X]

### Notes
> Here was a machinethat could do pattern recognition
in a humanlike way; it could recognize all kinds of things. Almost
everyoneat MIT was very skeptical . [AR98, S.99]

> Bernard Widrow (born December 24, 1929) is a U.S. professor of electrical engineering at Stanford University.[1] He is the co-inventor of the Widrow–Hoff least mean squares filter (LMS) adaptive algorithm with his then doctoral student Ted Hoff.[2] The LMS algorithm led to the ADALINE and MADALINE artificial neural networks and to the backpropagation technique. He made other fundamental contributions to the development of signal processing in the fields of geophysics, adaptive antennas, and adaptive filtering.