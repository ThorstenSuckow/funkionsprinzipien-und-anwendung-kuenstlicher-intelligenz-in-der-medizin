---
id: index
title: "2.2 Das Rosenblatt-Perzeptron"
description: Neuronenmodell nach Frank Rosenblatt
---

import {R, F, S, EQ, Def, Tbl, initEq} from "../../../../src/components/Refs.js";

#### 0 Todos
 - Heaviside (Treppenfunktion) mit Bezug auf 2.1.3.1 Aktivierungs- und Eingabefunktion, Gl. 2.1.3


## 2.2.1 Cells that fire together, wire together: Synaptische Plastizität

Donald Hebb (1904 - 1985), gebürtiger Kanadier, Sohn eines Ärzte-Elternpaares und 1965 nominiert für den Nobelpreis <R>BM03, S. 1014</R>, geht als junger Mann einer Karriere als Schriftsteller nach. Er studiert Englisch als Hauptfach und macht 1924 seinen Bachelor[^1] <R>Coo05, S.852</R>, doch die Schriften Freuds, mit denen er sich nach seinem Abschluss beschäftigt, wecken in ihm den Wunsch, sich tiefer mit Psychologie zu beschäftigen <R>BM03 S. 1013</R>: An der McGill Universität in Montreal[^2] macht er 1932 seinen Master darin[^3], und leitet dort 16 Jahre später als Professor die Fakultät für Psychologie <R>Coo05, S. 853</R>.

Seine Faszination darüber, wie das Gehirn lernt, Informationen verarbeitet und speichert <R>Str01, S. 298 ff.</R> wird Bestandteil seiner Forschungsarbeit: 1949 veröffentlicht er das Buch "**The organization of Behavior: A Neuropsychological Theory**" <R>Heb49</R>; seine darin formulierten Postulate[^4] <R>Kle99, S.2 f.</R> liefern einen wichtigen Beitrag für die Neurowissenschaften[^5]. Oft zitiert wird seine Idee bzgl. synaptischer Verstärkung, was heute als **Hebbsche Synapse** bekannt ist <R>AR88, S. 4, Abs. 5</R>[^6]:

> _When an axion of cell_ A _is near enough to excite a cell_ B _and repeatedly or persistently takes part in firing it, some
growth process or metabolic change takes place in one or both cells such that_ A_'s efficiency, as one of the cells firing_ B_, is increased._ <R>Heb88, S.50, Hervorhebung i.O.</R>

Derartige Veränderungen synaptischer Verbindungen wird als **Hebbsche Lernregel** <R>BCP18, S.985, "Hebb'sches Lernen"</R> bezeichnet. Das damit verbundene geflügelte Wort _Cells that fire together, wire together_[^7] beschreibt die Hypothese bildhaft. Seine Idee der "**Cell Assembly**" (siehe [^6], Zitat _b_) schließt daran an: Damit sind Verbände von Neuronen gemeint, die miteinander verschaltet sind, und deren Verbindungen durch das Hebbsche Lernen so sehr verstärkt sind, das die Aktivierung einzelner Zellen in diesen Verbänden ausreicht, das alle Zellen aktiviert werden <R>BCP18, S. 907-908, "Hebb und der Neuronenverband"</R>.

Hebbs Theorien gelten durch die Forschung als bestätigt[^8], und mit der von Hebb formulierten synaptische Plastizität wurde auch eine Idee für lernende künstliche neuronale Netze geliefert[^9]. In **Abschnitt 2.1.4** haben wir geschlossen, dass ein MCP-Netz nur durch vorhergehende Analyse der Aufgabe und Anpassung der Topologie von Außen zur Lösung einer Aufgabe imstande ist. Hebbs Erkenntnis führt 8 Jahre später zu einem Modell, das in der Lage ist, sich selbst anzupassen.


[^1]: an der Dalhousie Universität: https://dal.ca (abgerufen 17.08.2023)
[^2]: McGillUniversität, Montreal, Quebec (Kanada): https://www.mcgill.ca/neuro/about/donald-hebb-phd (abgerufen 16.08.2023)
[^3]: zu der Zeit studierte er in Teilzeit an der McGill Universität: "as a part time graduate student" <R>Kle99, S. 1</R>. Seine Master-Arbeit schrieb er aufgrund einer Erkrankung im Bett <R>BM03, S. 1014</R>
[^4]: es sind "three pivotal postulates" <R>Kle99, S.2 f.</R>
[^5]: als hätte sein Buch eine Art Golgräberstimmung in der der Psychologie ausgelöst, schreibt <R>Kle99</R>: "It attracted many
brilliant scientists into psychology, made McGill University a North American mecca for scientists interested in brain mechanisms of behaviour, led to many important discoveries, and steered contemporary psychology onto a more fruitful path." <R>Kle99, S.1</R>
[^6]: sowie als Zitat: _a:_ "[...] any two cells or systems of cells that are repeatedly active at the same time will tend to become “associated,” so that activity in one facilitates activity in the other." <R>Heb88, S. 52, "Mode of perceptual Integration: The Cell-Assembly"</R> sowie _b:_ "A series of such events \[Aktivierung von "cell-assemblies"\] constitutes a “phase sequence”—the thought process. <R>Heb88, S. 48, "Mode of perceptual Integration: The Cell-Assembly</R>; die Aussage findet sich im Original <R>Heb49</R> bereits in "Introduction", S. xi-xix
[^7]: Zumindest in <R>Heb49</R> findet sich kein solches Zitat. <R>KG14, S. 2</R> behauptet: "This mnemonic phrase was first introduced by Carla Shatz \[12\] in an article for the Scientific American aimed at lay public" und meint damit den Satz "In a sense, then, cells that fire together wire together." in <R>Sha92, S. 94</R>. https://en.wikipedia.org/wiki/Hebbian_theory#cite_ref-2 (abgerufen 16.08.2023) hingegen schreibt den Ursprung <R>LS92</R> zu: "neurons wire together if they fire together." <R>LS92, S.211</R>
[^8]:  vgl. <R>BLS19, S.833, Abs. 2 f.</R>, außerdem <R>YLC+14</R> und <R>BL73</R>. <R>BPC18, S. 875, Exkurs 23.5</R> verweist auf <R>Cop78</R>
[^9]: \[Hebb\] laid the foundation for neoconnectionism which seeks to explain cognitive processes in terms of connections between assemblies of real or
artificial neurons. <R>Kle99, S. 2</R>


## 2.2.2 Das Perzeptron - ein linearer Klassifizierer

Bei der MCP-Zelle hatten wir eine Kantengewichtsfunktion eingeführt, die je nach hemmender oder erregender afferenter Verbindung einen positiven oder negativen Wert zurückliefert (s. **Definition 2.1**). Für eine solche Kantengewichtsfunktion müßte nun



Einführung: Hebb 1949, Organization of Behavior, Lernregel, synaptische PLastizität, "fire togteher, wire together".
Auf dieser Grundlage dann Modell Rosenblatt, Synapsen gewichte

Heb ostuliert "cell assemblies" in dem Buch, selbstverstärkende cluster von neuronen die informstionen repräsentieren im Nervensystem, einzelne Zellen können zu mehreren Clustern gehören AR88, S. 44

> Perceptrons are neural nets that change with “experience,” using
an error-correction rule designed to change the weights of each
response unit when it makes erroneous responses to stimuli that
are presented to the network. [ARB02, S.20]

> In
other words, the simple perceptron can only compute a linearly
separable function of the pattern as provided by the associator
units.  [ARB02, S.20]

> The answer was “Yes: if the patterns are
linearly separable, then there is a learning scheme which will eventually yield a satisfactory setting of the weights.” The best-known
perceptron learning rule strengthens an active synapse if the efferent neuron fails to fire when it should have fired, and weakens an
active synapse if the neuron fires when it should not have done so:  [ARB02, S.20]

> Supervised learning adjusts the weights in an attempt
to respond to explicit error signals provided by a “teacher,” which
may be external, or another network in the same “brain.” This
model was introduced in the perceptron model, which is reviewed
in PERCEPTRONS, ADALINES, AND BACKPROPAGATION (of which
more details in the next road map, Grounding Models of Networks).  [ARB02, S. 30]

Evtl. interessant: Perceptrons, Adalines, and Backpropagation Arb02, S. 871

#### Convergence Proof

> It appearedin two later papersin the Reviewsof ModernPhysics . Those
paperscontained an attempt at a formal proof of the convergence. In 1961-
62 AI Novikoff recognizedthe essentialsimilarity between linear threshold
elementsand the algorithm, and linear programming, and produced a very
elementary but neat proof by contradiction that it converged in a finite
numberof trials. Novikoff 's proof madeclearwhat wasgoing on and should
have
, actually, triggered a lot of work. [AR98, S. 100]

### AI Winter
> These talks also mention the controversies that surrounded the rise and
fall of public and scientificinterest in Rosenblatt 's perceptron, as well as the
influence of Marvin Minsky and Seymour Papert on the loss of interest
in neural networks during the 1970s [AR98, S X]

### Notes
> Here was a machinethat could do pattern recognition
in a humanlike way; it could recognize all kinds of things. Almost
everyoneat MIT was very skeptical . [AR98, S.99]

> Bernard Widrow (born December 24, 1929) is a U.S. professor of electrical engineering at Stanford University.[1] He is the co-inventor of the Widrow–Hoff least mean squares filter (LMS) adaptive algorithm with his then doctoral student Ted Hoff.[2] The LMS algorithm led to the ADALINE and MADALINE artificial neural networks and to the backpropagation technique. He made other fundamental contributions to the development of signal processing in the fields of geophysics, adaptive antennas, and adaptive filtering.