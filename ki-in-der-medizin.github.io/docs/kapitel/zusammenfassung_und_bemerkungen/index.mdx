---
id: index
title: "6. Zusammenfassung und Bemerkungen"
description: Zusammenfassung und Bemerkungen
---

import {R, F, S, EQ, initEq} from "../../../src/components/Refs.js";

<initEq main={1} />

Ein biologisches neuronales Netz ist ein hochkomplexer Kommunikationsverbund, in dem verschiedene biochemische Prozesse ineinandergreifen. Aus einiger Entfernung betrachtet bilden all diese Prozesse ein Fundament für eine wesentliche Funktion: Der Informationsverarbeitung. Denn charakteristisch für eine einzelne Nervenzelle ist anscheinend:

 1. Der Empfang von Signalen
 2. Die Verrechnung von Signalen
 3. Die Ausgabe eines Signals

Diese drei Punkte haben wir in diesem einführenden Kapitel auf einer feingranularen Ebene kennengelernt: Die postsynaptischen Rezeptoren, an denen Transmitter binden, empfangen die Signale und leiten sie weiter zum Soma, wo über zeitliche oder räumliche Summation entschieden wird, ob ein Aktionspotenzial folgt - oder ob das Signal an dieser Stelle "verstummt". Löst ein Aktionspotenzial aus, erfolgt das Ausgabesignal in Form der Neurotransmitter, die in den synaptischen Spalt diffundieren, und so setzt sich die Kette fort, bis tausende von Neuronen dieses Signal an das Ende der Kommunikationskette weitergeleitet haben. So können wir also die Ausgabeberechnung nicht nur für einzelne Neuronen betrachten - aufschlussreich wird es vor allem, wenn wir das Ausgabesignal in dem Verbund betrachten, in dem es übertragen wurde. Denn ob ein Vesikel ein mEPSC versursacht, mag auf analytischer Ebene interessant sein; es sagt aber nichts daraus aus, ob ein Schmerzimpuls über 10tausende vor ihm verschaltete Synapsen eine muskuläre Reaktion ausgelöst hat.

Es muß verschiedene Schaltungsmuster im Gehirn geben, und diese können nicht alle zufällig ablaufen (auch, wenn dieses einführende Kapitel nicht näher auf die "Stochastik" bei der Signalübertragung eingehen konnte [QUELLE]). Wir wissen bereits, dass ein Neuron mit den Dendriten mehrerer Neurone verbunden sein kann (**Divergenz**); dass ein Neuron die Signale mehrerer vorgeschalteter Neurone empfängt (**Konvergenz**). Darüber hinaus gibt es aber noch komplexere Formen der Verschaltung, wie die **rekurrente Hemmung** und die **laterale Hemmung** <R>Eil19, S. 58, "Verschaltungsmuster" ff.</R>, deren Muster zuerst an Boolesche Algebra denken lassen als an den Graph aus einem neuronalen Netz.

Diese naturgegebene Verschaltung von verrechnenden Einheiten fasziniert die Wissenschaft, die versucht ist, diese hochkomplexe Eleganz zu verstehen und in mathematische Formeln zu gießen, um ein einheitliches Modell zu schaffen, das die Funktionsweise des Gehirns erklärt. So verwundert es nicht, das bereits früh versucht wurde, Erklärungsversuche der Funktionsweise auch in abstrahierte Modelle auf Maschinen zu übertragen: "[...], if you really understand something, you can usually make a machine do it" <R>AR88, S. xiii, Abs. 3</R> korollar zu Hopfield in order to make machines that can do what humasn do, we need to study human cognition <R>Fau94, S. 25, Hopfield Quelle benötigt</R> und Elaine Rich "Artificial Intelligence (AI) : is the study of how to make computers do things which, at the moment, people do better"

Immer wieder gelingt dieser interdisziplinären Wissenschaft<R>Fau94, S. 22, Abs. 1</R> in dieser Richtung kleine und große Erfolge: So zählt das Hodgkin-Huxley-Modell aus **1.4.1** sicher zu den wichtigen Entdeckungen, da gezeigt wurde, wie durch elektirsche Schaltungen die Dynamik einer Nervenzelle modelliert werden konnten <R>Roj96, 27, letzter Absatz</R>, von der Jahrzehnte später noch die Forschungsabteilungen und Fachbereiche zehren. Chat-GPT ist sicherlich ein weiterer Meilenstein. Doch manches gelingt ihr aber bis heute nicht. Spannend ist weiterhin die Frage, wie und ob Neuronen an unserem Bewusstsein beteiligt sind, oder wie "Inutition" entsteht.

"Die Neurowissenschaft ist zwar eine wichtige Quelle der Inspiration, aber keinesfalls eine starre Richtungsvorgabe." [GBC18, S. 17, letzter Absatz]

"Und obwohl die Neurowissenschaft für mehrere _Architekturen_ neuronaler Netze erfolgreich Pate stand, wissen wir doch nicht genug über das biologische Lernen, damit uns die Neurowissenschaft als Richtschnur für die _Lernalgorithmen_ dienen kann, mit denen wir diese Architekturen trainieren."  [GBC18, S. 17, letzter Absatz ff.]

"IBM will das Gehirn nachbauen" https://www.heise.de/news/IBM-will-das-Gehirn-nachbauen-217555.html

Erst
urn 1901 erkannte der spanische Physiologe Santiago Ram6n y Cajal. daB die Vernetzung der Nervenzellen eine Richtung ftir die Informationstibertragung festlegt.
Darnit stand fest. daB die Kopplung der Nervenzellen einem hierarchischen System
entspricht. [...] Der Begriff Neuron (aus dem Griechischen
für Nerv) wurde durch den Berliner Professor Wilhelm Waldeger gepragt. nachdem
Ram6n y Cajal in Berlin seine Praparate vorgestellt hatte [Stevens 1973] [Roj93, S. 26]

Quelle Informationsfluss: [Edw99, S. 173]
Quelle Neuron als Name: [MMA+19]

AR88, S. 43 Much modern work in neural networks has moved far away from its study of the brain and psychology, cause for concern, losing contact with its foundation